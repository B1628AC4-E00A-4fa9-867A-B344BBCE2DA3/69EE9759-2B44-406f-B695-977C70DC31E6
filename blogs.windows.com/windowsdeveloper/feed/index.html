<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Windows Developer Blog</title>
	<atom:link href="https://blogs.windows.com/windowsdeveloper/feed/" rel="self" type="application/rss+xml" />
	<link>https://blogs.windows.com/windowsdeveloper/</link>
	<description></description>
	<lastBuildDate>Wed, 10 Sep 2025 17:00:23 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.6</generator>

<image>
	<url>https://blogs.windows.com/wp-content/uploads/sites/3/2021/06/cropped-browser-icon-logo-32x32.jpg</url>
	<title>Windows Developer Blog</title>
	<link>https://blogs.windows.com/windowsdeveloper/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Free developer registration for individual developers on Microsoft Store</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/09/10/free-developer-registration-for-individual-developers-on-microsoft-store/</link>
		
		<dc:creator><![CDATA[Chetna Das]]></dc:creator>
		<pubDate>Wed, 10 Sep 2025 17:00:23 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57557</guid>

					<description><![CDATA[<p>We’re excited to share that individual developers can now publish apps to the Microsoft Store without paying any onboarding fees — and this new experience is now globally available in nearly 200 markets worldwide. Developers will no longer need a</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/09/10/free-developer-registration-for-individual-developers-on-microsoft-store/">Free developer registration for individual developers on Microsoft Store</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[We’re excited to share that individual developers can now publish apps to the Microsoft Store without paying any onboarding fees — and this new experience is now globally available in nearly 200 markets worldwide. Developers will no longer need a credit card to get started, removing a key point of friction that has affected many creators around the world. By eliminating these one-time fees, Microsoft is creating a more inclusive and accessible platform that empowers more developers to innovate, share and thrive on the Windows ecosystem. Visit <a href="https://storedeveloper.microsoft.com/home">storedeveloper.microsoft.com</a> to get started.

We are committed to helping the developer ecosystem thrive, and we thank you for the feedback you continue to give us. The Microsoft Store is now used by over 250 million monthly active users, and  <a href="https://blogs.windows.com/windowsdeveloper/2021/09/28/microsoft-store-more-apps-more-open/">because of its open policies</a>  — it is the most trusted and scalable PC distribution channel across both consumer and commercial devices.<em> </em>
<p style="padding-left: 40px;"><em>“Eliminating registration fees for individual developers isn't just a technical decision: it's a statement of respect for those who create from the periphery, with limited resources but powerful ideas. Thank you for opening this door: not just for me, but for an entire community that can now dream out loud.” <strong>- Freddy Castillo, Independent developer</strong></em></p>
<p style="padding-left: 40px;"><em>“Over the past year, our daily new installs grew 10x on the Microsoft Store. All we focus on is building great apps — the Store does the heavy lifting by connecting us with users, driving targeted traffic, and providing sales funnel insights and customer support tools. This lets us dedicate our time to creating the best experiences for Windows users worldwide.” <strong>-</strong> <strong>HUXSoft,</strong> <strong>Independent publisher</strong></em></p>
<p style="padding-left: 40px;"><em>“Recent improvements in the Microsoft Store led to a strong increase in our user base: daily active users of our flagship app, backiee, grew from 10,000 to 40,000 year-over-year, driving a sixfold revenue increase. We also reached a major milestone of 7 million downloads, while continuing to deliver unique features for our users.” <strong>- Good2Create, Independent publisher </strong></em></p>
<img class="alignnone wp-image-57559 size-full" src="https://blogs.windows.com/wp-content/uploads/sites/3/2025/09/new-boarding-process.png" alt="Screenshot of new onboarding process." width="602" height="338" />
<p style="text-align: center;"><em>New onboarding process with zero registration fees for Individual developers.</em></p>

<h2>What’s new in onboarding?</h2>
<ul>
 	<li><strong>Free registration:</strong> No payment required for individual developers — just sign in with a personal Microsoft account and follow the guided, modern UI.</li>
 	<li><strong>Lightweight ID verification: </strong>Quickly verify your identity by scanning a valid government-issued ID and your selfie — supported in all markets worldwide.</li>
 	<li><strong>Streamlined setup:</strong> Auto-filled profile details and instant access to Partner Center once verified, meaning you can go from sign-up to submission in minutes.</li>
</ul>
<h2>Why build for Microsoft Store?</h2>
The Windows developer platform is evolving rapidly powered by Copilot+ PCs, AI innovation and a thriving ecosystem. The Microsoft Store sits at the heart of this momentum, offering developers unmatched reach, flexibility and trust.
<ul>
 	<li><strong>Massive reach</strong>: Over 250 million users visit the Store each month.</li>
</ul>
<ul>
 	<li><strong>All app types welcome</strong>: Publish your Win32 (including .NET WPF and WinForms), UWP, PWA, .NET MAUI or Electron apps to the Microsoft Store — no code changes required.</li>
</ul>
<ul>
 	<li><strong>Trusted distribution</strong>: Get discovered through Windows Search, deliver secure installs and reach enterprise customers via Intune.</li>
</ul>
<ul>
 	<li><strong>Commerce flexibility</strong>: Use Microsoft’s commerce platform at <a href="https://learn.microsoft.com/en-us/windows/apps/publish/publish-your-app/why-distribute-through-store">low revenue share rates</a> to maximize your profit, while benefiting from its convenience and security. Alternatively, for non-game apps, developers can also bring their own in-app commerce system and keep 100% of the revenue.</li>
</ul>
<ul>
 	<li><strong>Complimentary hosting, free signing and automatic updates (MSIX):</strong> Package your app as an MSIX and we’ll <a href="https://learn.microsoft.com/en-us/windows/apps/publish/faq/get-started-with-the-microsoft-store#:~:text=Additionally%2C%20when%20you%20publish%20through,infrastructure%20managed%20by%20Microsoft%2C%20including">host</a> your binary on our infrastructure and pay for the distribution — no need to set up your own CDN. We’ll also sign your app for free to boost trust and security, and Windows delivers updates automatically, so your users always have the latest version without any extra work. These built-in services reduce your overhead and let you focus on building great experiences.</li>
</ul>
<img class="alignnone wp-image-57560 size-full" src="https://blogs.windows.com/wp-content/uploads/sites/3/2025/09/Reach.png" alt="Screenshot from the Microsoft Store." width="602" height="338" />
<p style="text-align: center;"><em>Reach millions of users across Windows devices through the Microsoft Store.</em></p>

<h2>Ready to publish?</h2>
Get started by opening your account at <a href="https://storedeveloper.microsoft.com/home">storedeveloper.microsoft.com</a>.]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>PowerToys Command Palette utility</title>
		<link>https://learn.microsoft.com/en-us/windows/powertoys/command-palette/overview</link>
		
		<dc:creator><![CDATA[Steve Clarke, Editor]]></dc:creator>
		<pubDate>Wed, 20 Aug 2025 16:27:32 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Feature Documentation]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57563</guid>

					<description><![CDATA[<p>The post <a href="https://learn.microsoft.com/en-us/windows/powertoys/command-palette/overview">PowerToys Command Palette utility</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>The post <a href="https://learn.microsoft.com/en-us/windows/powertoys/command-palette/overview">PowerToys Command Palette utility</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Available today: gpt-oss-20B Model on Windows with GPU Acceleration – further pushing the boundaries on the edge</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/08/05/available-today-gpt-oss-20b-model-on-windows-with-gpu-acceleration-further-pushing-the-boundaries-on-the-edge/</link>
		
		<dc:creator><![CDATA[Windows Developer Blog Team]]></dc:creator>
		<pubDate>Tue, 05 Aug 2025 23:40:28 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57541</guid>

					<description><![CDATA[<p>With OpenAI’s release of gpt-oss models today, we are thrilled to bring GPU optimized gpt-oss-20B model variants to Windows devices.</p>
<p>This milestone brings powerful, open-source reasoning models to Windows developers, with support for local inferen</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/08/05/available-today-gpt-oss-20b-model-on-windows-with-gpu-acceleration-further-pushing-the-boundaries-on-the-edge/">Available today: gpt-oss-20B Model on Windows with GPU Acceleration – further pushing the boundaries on the edge</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[With OpenAI’s release of gpt-oss models today, we are thrilled to bring GPU optimized gpt-oss-20B model variants to Windows devices.

This milestone brings powerful, open-source reasoning models to Windows developers, with support for local inference. You can try it out in Foundry Local or AI Toolkit for VS Code (AITK) and start using it in your applications today.

Learn more about what’s possible with Open AI’s gpt-oss models on the <a href="https://aka.ms/OAIOSSfoundryblog">Azure blog</a>.

<strong>Want to get started on Windows today?</strong>

Get gpt-oss-20B up and running on your Windows device in just a few minutes using Foundry Local or AI Toolkit!

To get started with Foundry Local:
<ol>
 	<li>Install Foundry Local via WinGet (recommended) using the following command:
<p style="padding-left: 20px;"><code>winget install Microsoft.FoundryLocal</code></p>
<p style="padding-left: 20px;"><em>Note: As an alternative, Foundry Local can also be <a href="https://github.com/microsoft/Foundry-Local/releases">installed from GitHub</a>.</em></p>
</li>
 	<li>Open your Terminal and run the model from the Foundry Local CLI with the following command:
<p style="padding-left: 20px;"><code>foundry model run gpt-oss-20B</code></p>
</li>
 	<li>Start sending Foundry Local your prompts!</li>
</ol>
To get started with <a href="https://learn.microsoft.com/en-us/windows/ai/toolkit/toolkit-getting-started?utm_source=chatgpt.com&amp;tabs=rest">AI Toolkit for VS Code</a>:
<ol>
 	<li>If you don’t have it already, <a href="https://code.visualstudio.com/download">install Visual Studio Code here</a>.</li>
 	<li><a href="https://encoded-592c9deb-987b-4562-aa3c-9fa3d37d83e9.uri/https%3a%2f%2fvscode%3aextension%2fms-windows-ai-studio.windows-ai-studio">Install AI Toolkit extension</a>.</li>
 	<li>Open Model Catalog and download the model gpt-oss-20B.</li>
 	<li>Open the Model Playground, load the model, and start sending it prompts!</li>
</ol>
After exploration via either tool, you can modify prompts, tune inference parameters, and integrate into your app using the Foundry Local SDK.]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Phi Silica task specialization using LoRA in Microsoft Learning Zone: A technical deep dive</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/07/31/phi-silica-task-specialization-using-lora-in-microsoft-learning-zone-a-technical-deep-dive/</link>
		
		<dc:creator><![CDATA[Shay Ben-Elazar]]></dc:creator>
		<pubDate>Thu, 31 Jul 2025 16:00:10 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57522</guid>

					<description><![CDATA[<p>At Build 2025, <a href="https://blogs.windows.com/windowsdeveloper/2025/05/19/advancing-windows-for-ai-development-new-platform-capabilities-and-tools-introduced-at-build-2025/">we announced</a> support for LoRA (low-rank-adaptation) finetuning </p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/07/31/phi-silica-task-specialization-using-lora-in-microsoft-learning-zone-a-technical-deep-dive/">Phi Silica task specialization using LoRA in Microsoft Learning Zone: A technical deep dive</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[At Build 2025, <a href="https://blogs.windows.com/windowsdeveloper/2025/05/19/advancing-windows-for-ai-development-new-platform-capabilities-and-tools-introduced-at-build-2025/">we announced</a> support for LoRA (low-rank-adaptation) finetuning for <a href="https://blogs.windows.com/windowsexperience/2024/12/06/phi-silica-small-but-mighty-on-device-slm/">Phi Silica</a> – our inbox Small Language Model (SLM) that runs locally on Copilot+ PCs. LoRA makes fine-tuning more efficient by updating only a small subset of parameters of the model with custom data. This allows improved performance on desired tasks without affecting model's overall abilities.

This post shares the behind-the-scenes work and design considerations that enabled us to customize generation for a real-world use case: generating high-quality, pedagogically valuable Kahoot! quizzes. Our efforts led to a <b>75% reduction in rejection rates</b> and show a <b>4.6X uplift in subjective quality scores</b><sup>1</sup>.

https://www.youtube.com/watch?v=M5h2Sn0odnM
<h2><b>Microsoft Learning Zone: Generating Kahoot! games on-device</b></h2>
Earlier this year, we introduced <a href="https://www.microsoft.com/en-us/education/blog/2025/06/empowering-educators-with-ai-innovation-and-insights/">Microsoft Learning Zone</a> (under the code name “Project Spark”), Microsoft’s first learning companion app designed specifically for Copilot+ PCs. It empowers educators to effortlessly create interactive and personalized lessons using on-device AI – at no cost.

As part of this initiative, we partnered with Kahoot!, the beloved learning platform, to enable the creation of engaging classroom games powered entirely by Phi Silica.

Microsoft Learning Zone supports a wide range of generation tasks with varying pedagogical requirements – from dynamic introductions and multiple-choice formats to customizable refinement flows. Naturally, training and distributing a custom fine-tuned model for each generation task would be inefficient and impractical. Instead, we leveraged LoRA adapters to specialize a single, base Phi Silica model to diverse task needs with minimal overhead.
<h2><b>Defining quality: verifiable vs. subjective</b></h2>
Kahoot! quizzes consist of multiple-choice questions, and evaluating their quality is subjective – combining structural requirements with human judgement. We defined two axes of quality:
<h3>Verifiable quality</h3>
This includes clearly defined output format constraints – like maximum character lengths for questions and answers – aligned with Kahoot!’s UX across devices. These are enforced as hardcoded <b>guardrails</b> in Microsoft Learning Zone’s real-time generation pipeline. Generated Kahoot! multiple-choice questions are streamed to the user for review only if they successfully pass through these guardrails. Reducing the rejection rate due to guardrail violations directly improves user-perceived latency, as discarded generations increase the delay until a user can review a newly generated question.
<h3>Subjective quality</h3>
Subjective quality addresses attributes like engagement, clarity and educational relevance which are not easy to measure quantitatively but are important for user perception and user satisfaction. In collaboration with the Kahoot! team, we defined a rubric and guidelines for human annotators and then used those insights to scale human evaluation via a novel agentic framework – more on that below.
<h2><b>Dataset curation and distillation</b></h2>
To enable effective LoRA finetuning, we curated a high-quality dataset grounded in real-world Microsoft Learning Zone use, as described below. The challenge was to provide Phi Silica with educationally rich, diverse content it could learn from to adapt the behavior of the model while only finetuning ~1% of the parameters with LoRA adapters.

Instead of relying solely on base model outputs, we adopted a <b>distillation approach</b>, using a leading LLM as a teacher. We applied it to generate synthetic Kahoot!-style Q&A tuples from curated learning materials. This approach allowed us to bootstrap a training dataset with higher initial quality and coverage.

The Microsoft Learning Zone pipeline starts by ingesting curated learning materials and extracting key facts and segments. Each segment is processed independently to ensure reasoning focus and tractability due to model context length constraints. For each extracted segment-key fact pair, we prompted GPT-4o to generate a single Kahoot!-style question centered on the fact. We passed each generated question through Microsoft Learning Zone’s guardrails that validate hard constraints such as character length limits on questions and answers – aligned with Kahoot!’s UI guidelines.

In total, we generated approximately 13,000 synthetic examples, which were then split into 10,000 training and 3,000 testing subsets, accordingly. Curating datasets for AI systems is one of the most overlooked but important pieces of the puzzle to making AI systems work effectively for the scenario at hand.
<h2><b>LoRA finetuning with AI toolkit</b></h2>
By using the recently released <a href="https://learn.microsoft.com/en-us/windows/ai/apis/phi-silica-lora?tabs=csharp0">Phi Silica LoRA finetuning feature in AI toolkit</a>, we were able to train LoRA adapters against the (quantized) Phi Silica model. The adapters produced run locally to customize the output of Phi Silica and match the requirements of our Kahoot! feature.
<h3>System prompt considerations</h3>
A system prompt grounds the model with context, instructions or other information relevant to a specific use case. It can define the following in a model’s response:
<ul>
 	<li>What persona should be used</li>
 	<li>What the model should and shouldn’t answer</li>
 	<li>Format of model’s response</li>
</ul>
Though it may seem intuitive to provide more information in a system prompt, it costs resources and performance by using tokens in context length and adding latency to a model’s response. The original system prompt needed to get the desired output format from our Phi Silica model was lengthy – it required specifying the format of the JSON table <i>and</i> providing detailed descriptions of the types of questions and answers we wanted.

After customization of our Phi Silica model, we were able to use a shorter system prompt since those details (format, persona, etc.) had been encoded in the LoRA adapter during training. This proved useful since the output required was very different from the base model’s default output.

For example, we used a short prompt that pushed the base model’s output in the direction we wanted while restricting it to a couple of sentences. The prompt we used for training was:

<i>“You will be given a fact and some additional context. Respond with a relevant question, one correct answer and some incorrect answers.”</i>

With this prompt, the base model gave answers in plain English with questions and answers that were based on the fact and context provided. However, the style of the questions and answers did not match what we wanted, and we needed the output to have a specific JSON format.

When performing inference with the LoRA adapter, we often got responses that had good questions and answers, but sometimes the JSON format was still not exactly what we provided in the training set. The solution was to reinforce that format in the system prompt we used for inference:

<i>You will be given a fact and some additional context. Respond with a relevant question, one correct answer and some incorrect answers. Reply with a strict JSON string for class <code class="EnlighterJSRAW" data-enlighter-language="json">{question: string, answers: [{answer: string, correct: bool}], gettyImage: string}</code></i><i>, wrapped in <code class="EnlighterJSRAW" data-enlighter-language="generic">```json</code> </i><i>tags.</i>

The combination of the LoRA adapter and this new system prompt gave us the desired output.
<h3>Hyperparameter selection</h3>
In addition to changes to the system prompt, changes to the hyperparameters used during LoRA adapter training may improve output quality.

The default AI toolkit hyperparameters offer a solid starting point but adjusting them can optimize results for specific scenarios. We evaluated various settings on a smaller dataset for faster experiments. Training remained stable near default values, while extreme settings led to failed convergence – evidenced by stagnant loss and poor output – indicating the importance of staying close to defaults.

To identify which adapters perform best, it is important to include some evaluations during experiments. At this stage of parameter exploration, we used a simplified agent-as-a-judge assessment, as described in the following section.

For our Kahoot! use case, we did not find any parameter combination that worked better than our defaults. However, experimenting gave us confidence in our defaults.

Once confident in our system prompt and hyperparameters, we froze them and proceeded with longer training runs. Transitioning from <i>exploration</i> to <i>exploitation</i> in LoRA adapter training, we used the full dataset and increased early stopping patience, allowing extended training if evaluation numbers showed improvement.
<h2><b>Evaluating model quality</b></h2>
<h3><b>Verifiable quality: guardrail pass rate</b></h3>
The customized system with Phi Silica + LoRA adapter showed a <b>75% reduction in rejection rate</b>, measured as statistically significant via guardrails<sup>2</sup>. This directly improved user experience by reducing failed generations and perceived latency.
<h3><b>Subjective quality: agent-as-a-judge evaluation</b></h3>
Human evaluation costs time and resources. To scale subjective assessment, we built a multi-<b>agentic evaluation framework</b> using <a href="https://microsoft.github.io/autogen/0.2/"><b>Autogen</b></a>. Unlike traditional LLM-as-a-judge approaches, this framework simulates a <b>review team</b> of AI agents engaging in deliberative conversation to deliver nuanced, balanced and multi-perspective assessments. To do this we instructed the ‘review team’ with a set of quality measures we want to evaluate for each question. We leverage this framework to accelerate preliminary offline quality assessment. After these initial evaluations, we gradually validate results against multiple layers of human reviewers as part of Microsoft’s responsible product release practices.

We share our <a href="https://github.com/microsoft/AgentAsJudge"><b>base code</b></a> for further research.
<h3>Agent roles</h3>
The evaluation framework consisted of several personas engaged in a discussion, here we detail the agents involved and their tasked roles:
<ul>
 	<li>The <b>Reviewer</b> agent was tasked with evaluating each quality attribute using a chain-of-thought (CoT) approach, asking it to provide an initial justification and then a score for each quality criterion.</li>
 	<li>The <b>Critic</b> agent received the same context as the Reviewer agent along with the Reviewer’s evaluation. It was instructed to challenge the Reviewer’s reasoning - either proposing alternative scores or reinforcing agreement with reasoned justification.</li>
</ul>
This dialogue continues iteratively until a convergence point is reached, at which stage the <b>Meta-Reviewer</b> is invoked.
<ul>
 	<li>The <b>Meta-Reviewer</b> reviews the full conversation between the Reviewer and Critic, weighs their arguments along with the base context, and issues a final verdict. This final score, whether it aligns with or diverges from the previous agents, is treated as the final output of the evaluation framework.</li>
</ul>
<h3>Quality metrics</h3>
<span data-contrast="auto">The review team created a set of metrics they wish to evaluate in the generated questions; here are these metrics exactly as they appear in the system prompt of each of the agent components.</span>
<ol>
 	<li><b>Educational Value</b>: whether the question teaches or tests a nontrivial concept that is highlighted in the given context.</li>
 	<li><b>Clarity and Phrasing</b>: whether the wording is clear, precise, grammatically correct and understandable without confusion (e.g.,no double negatives).</li>
 	<li><b>Correct Answers Quality</b>: whether the correct answers fully answer the question.</li>
 	<li><b>Distractors Quality</b>: whether the distractors (incorrect answers) are reasonable and originate from a similar context, ensuring that someone unfamiliar with the material could be misled. The distractors must be wrong answers to the question.</li>
 	<li><b>Focus</b>: whether the question targets a single, clear idea without mixing unrelated concepts.</li>
 	<li><b>Conciseness</b>: whether the question is concise and to the point avoiding unnecessary complexity or verbosity (especially since it is presented in a Kahoot! activity).</li>
</ol>
<h3>Agent-as-a-judge evaluation results</h3>
Using a scoring framework we crafted with the Kahoot! team, our agents rated each question across key quality metrics, from 1 to 10. Our results show that LoRA outperforms the Phi Silica-base model across all quality attributes, as seen in the graph below.

<i><img class="alignnone wp-image-57535 size-large" src="https://blogs.windows.com/wp-content/uploads/sites/3/2025/07/all_metrics_performance_with_ci1920-1024x717.png" alt="Graph showing average quality scores for Phi Silica vs. Phi Silica + LoRA across six evaluation aspects, with 95% confidence intervals." width="1024" height="717" />Figure 1: Average quality scores for Phi Silica vs. Phi Silica + LoRA across six evaluation aspects, with 95% confidence intervals</i>

The results show that the Phi Silica + LoRA model consistently outperforms the baseline Phi Silica model (without customization) across all six quality aspects of question generation, including clarity, correctness and educational value. Notably, the most significant improvements are seen in the quality of correct answers and quality of distractors (incorrect answers), while Phi Silica + LoRA achieves both higher average scores and narrower confidence intervals. The 95% confidence intervals indicate the statistical reliability of these findings - where non-overlapping intervals between models suggest that LoRA's improvements are not due to chance. Overall, the results highlight a meaningful and statistically robust gain in quality enabled by LoRA fine-tuning.

Overall, we found that our agent-as-a-judge favored the samples generated by the Phi Silica + LoRA model in 22.5% of the cases, compared to 14.5% for those generated by the base model.

<i><img class="alignnone wp-image-57536" src="https://blogs.windows.com/wp-content/uploads/sites/3/2025/07/ab_experiment_agent_as_a_judge1920-1024x683.png" alt="Graph showing Base vs Base + LoRA A/B test overall “win rate” according to our agent-as-a-judge evaluation system, where the base model is Phi Silica." width="500" height="333" />Figure 2: Base vs Base + LoRA A/B test overall “win rate” according to our agent-as-a-judge evaluation system, where the base model is Phi Silica</i>
<h2><b>Comparison with human judgement</b></h2>
To assess the effectiveness of using Phi Silica with a LoRA adapter compared to Phi Silica alone, we conducted an A/B test via a human evaluation study. The study included 2,350 paired samples generated by both models, with one human preference collected for each pair.

Annotators were shown pairs of multiple-choice questions generated by the two models from the same input context. For each pair, annotators selected the better question based on the same criteria defined in our agent-as-a-judge evaluation framework. Each annotator received the original context along with both generated questions and their answer choices and was asked to choose the preferred question in a blind AB test. The results show a trend of favoring the Phi Silica + LoRA model with a powerful effect size measured as 4.6X uplift, which can be seen in Figure 3.

<i><img class="alignnone wp-image-57537" src="https://blogs.windows.com/wp-content/uploads/sites/3/2025/07/human_ab_test_recreated_1000dpi1920-1024x683.png" alt="Graph showing human preference in a blind AB test on question generated by Phi Silica and Phi Silica + LoRA." width="500" height="333" />Figure 3: Human preference in a blind AB test on question generated by Phi Silica and Phi Silica + LoRA</i>

Our framework produces a set of scores across selected quality metrics. To aggregate these into a single score per question, we averaged the metric scores. We then compared the aggregated scores of each question pair to determine a model preference. Finally, we compared these preferences to those of human annotators to assess alignment. The results show that the framework achieves 79.5% accuracy and an F1 score of 77.3 in predicting human preference. We note that human preferences may differ in how they weight various quality metrics. While our evaluation relied on a simple average across all metrics, individuals are likely to prioritize certain aspects of quality over others, leading to potential misalignment.

We share the model’s confusion matrix in Figure 4 below.

<i><img class="alignnone wp-image-57538" src="https://blogs.windows.com/wp-content/uploads/sites/3/2025/07/confusion_matrix_binary_balanced1920-1024x768.png" alt="Normalized binary confusion matrix on a balanced dataset, comparing Phi Silica before and after adding the LoRA adapter." width="550" height="413" />Figure 4: Normalized binary confusion matrix on a balanced dataset, comparing Phi Silica before and after adding the LoRA adapter</i>
<h2><b>Summary</b></h2>
The work done by the Microsoft Education team is a real-life example of how LoRA adapters are a cost-effective, lightweight option for customization of Phi Silica for task specific scenarios like generating Kahoot! quizzes.

Instead of training the larger Phi Silica base model, training was done on the smaller LoRA adapter using a curated dataset grounded in Microsoft Learning Zone’s guardrails (ex. aligning to Kahoot!’s UI guidelines). Customization via trained LoRA adapters shortened prompt requirements that improved efficiency while maintaining desired output structures. The default AI toolkit hyper parameters were validated, leading to extended training with fixed parameters.

Output quality was assessed by agents and verified through human testing for added reliability. Between the two tests, the Kahoot! quizzes generated by the Phi Silica + LoRA model were more favored by both the agents <i>and</i> humans.

<i><img class="alignnone wp-image-57539 size-full" src="https://blogs.windows.com/wp-content/uploads/sites/3/2025/07/Picture1.png" alt="LoRA distillation and evaluation flow overview." width="668" height="108" />Figure 5: LoRA distillation and evaluation flow overview</i>

Ultimately, these efforts led to a <b>75% reduction in rejection rates</b> and a <b>4.6X uplift in subjective quality scores</b> or AI generated Kahoot! quizzes.

Kahoot! game generation through Microsoft Learning Zone will launch to public preview for Educators to experiment with later this summer. This work demonstrates how small models, when carefully adapted, can deliver robust, personalized AI experiences – even within constrained environments like on-device learning tools.

Read more about leveraging LoRA with Phi Silica on Copilot+ PC devices at our <a href="https://blogs.windows.com/windowsdeveloper/2025/05/19/advancing-windows-for-ai-development-new-platform-capabilities-and-tools-introduced-at-build-2025/">2025 Build announcement page</a>.

<strong>Acknowledgements:</strong>

<em>Mousa Arraf - Applied Science Intern</em>

<em>Ella Ben-Tov - Principal Product Manager</em>

<em>Pashmina Cameron - Principal Applied Science Manager</em>

<em>Henry Jackson-Flux - Senior Applied Scientist</em>

<em>Merav Mofaz - Senior Applied Scientists</em>

<strong>Endnotes:</strong>

<sup>1</sup> Metrics jointly defined with Kahoot, data generated by Microsoft as further defined below, analysis done on May 13, 2025.

<sup>2</sup> Measurements performed on data generated by Microsoft, analysis done on May 13, 2025.]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Windowing overview for WinUI and Windows App SDK</title>
		<link>https://learn.microsoft.com/en-us/windows/apps/develop/ui-input/windowing-overview</link>
		
		<dc:creator><![CDATA[Steve Clarke, Editor]]></dc:creator>
		<pubDate>Wed, 23 Jul 2025 16:31:32 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Feature Documentation]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57564</guid>

					<description><![CDATA[<p>The post <a href="https://learn.microsoft.com/en-us/windows/apps/develop/ui-input/windowing-overview">Windowing overview for WinUI and Windows App SDK</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<p>The post <a href="https://learn.microsoft.com/en-us/windows/apps/develop/ui-input/windowing-overview">Windowing overview for WinUI and Windows App SDK</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Leveling up your Microsoft Store on Windows experience</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/06/05/leveling-up-your-microsoft-store-on-windows-experience/</link>
		
		<dc:creator><![CDATA[Giorgio Sardo]]></dc:creator>
		<pubDate>Thu, 05 Jun 2025 16:00:13 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57404</guid>

					<description><![CDATA[<p>The Microsoft Store on Windows is used by over 250 million users each month – and we take the responsibility we have to you, our customers, seriously. We use the feedback you send to ensure we’re focusing on the most important things our customer</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/06/05/leveling-up-your-microsoft-store-on-windows-experience/">Leveling up your Microsoft Store on Windows experience</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[The Microsoft Store on Windows is used by over 250 million users each month – and we take the responsibility we have to you, our customers, seriously. We use the feedback you send to ensure we’re focusing on the most important things our customers care about. Last December, <a href="https://blogs.windows.com/windowsdeveloper/2024/12/03/raising-the-bar-updates-to-the-microsoft-store-on-windows/">we announced a variety of product quality improvements</a>, and in February, we shared how we’re <a href="https://blogs.windows.com/windowsdeveloper/2025/02/18/discover-magical-ai-experiences-through-the-microsoft-store-on-windows-with-the-new-ai-hub/">evolving our Store into an AI marketplace</a>. And we’re excited to keep the momentum going, with many more updates planned for this year.

Today, we’re excited to announce a variety of newly available features that we believe will level up your Store experience.

Let’s jump in!
<h3>Home page, curated for you</h3>
<img class="alignnone wp-image-57512 size-large" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/06/Personalization1920-1024x576.png" alt="Image of a personalized home page experience with recommendations curated for users." width="1024" height="576" />

The Microsoft Store homepage will now be personalized for you. Whether you're a gamer chasing the next big hit, a productivity enthusiast looking for time-hacks or a developer in search of tools, the newly redesigned homepage will elevate the content most meaningful to you. In the coming weeks, you’ll see fresh recommendations based on recent activities, what’s trending in your region and the most recent deals. Personalized recommendations are controlled by your Store settings.
<h3>Find what you’re looking for, faster</h3>
We are making four big improvements to help you to search for and discover new content faster. First, search in Store just got a whole lot smarter. We have rearchitected how search works – it is now more intent-aware, leverages signals like app updates and ratings more diligently for ranking and addresses language-specific nuances. This translates to results that are more relevant to what you are looking for – try it out today!

<img class="alignnone wp-image-57513 size-large" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/06/Game_Ask_Copilot1920-1024x712.png" alt="Product page for DOOM: The Dark Ages with a Copilot agent users can interface with and learn more about specific features." width="1024" height="712" />

Second, for users in the United States, <strong>Copilot</strong> is now available in the bottom right corner to answer questions while you’re browsing product pages. You can open it up to ask questions about the page you’re viewing or select two products for comparisons.

<img class="alignnone wp-image-57514 size-large" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/06/CapCut_DiscoverMore1920-1024x712.png" alt="CapCut product page with a “Discover More” section highlighting similar recommended content." width="1024" height="712" />

Third, when you’re browsing product pages, you’ll now see a new “Discover More” section that includes related content that you may be interested in. And fourth, we have added product page badges to help you easily tell which apps have AI features, and which apps are great for Copilot+ PCs.
<h3>Deeper Windows integration</h3>
<img class="alignnone wp-image-57516 size-large" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/06/WSB_Image1920-1024x576.png" alt="" width="1024" height="576" />

One of the superpowers of Store apps is their ability to integrate into the rest of Windows – so here are two new ways we’re trying to meet you where you are. First, if you're like us and use Windows search to look for most things on your PC, we have exciting news! You'll now be able to launch Windows search, search for an app or game from the Store and install it quickly<sup>1</sup>.

<img class="alignnone wp-image-57515 size-large" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/06/Open-with1920-1024x576.png" alt="Image of Windows file explorer with app suggestions as options to open file extensions." width="1024" height="576" />

Second, we're experimenting with offering app suggestions to open select file extensions, which is particularly helpful if you don’t have an app for that extension, or haven’t selected a default app. If you’re a Windows Insider in the U.S. or China regions, you’ll soon be able to try this out by using the context menu to select an app to “Open With” and browsing our recommendations. If you’ve already selected a default app, that will show up first.
<h3>More fixes under the hood</h3>
<img class="alignnone wp-image-57517 size-large" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/06/Store_Perf1920-1024x576.png" alt="Decorative image of the Windows Store logo." width="1024" height="576" />

The Store is getting faster. After rigorous performance investments, the Store launches two times faster than it did six months ago<sup>2</sup>. We have also significantly improved installation reliability and speed over the last six months. To make sure you see the latest improvements, please ensure you have the latest Windows update.
<h3>Other goodies in Store</h3>
There's a long list of fit and finish improvements for you to go try, including: a new capability that lets you install individual components for games; faster in-apps rating dialogs for when you want to share your feedback with developers; and a new field on product pages to let you know when an app or game was last updated.

And we would be remiss if we did not acknowledge the importance of our Store developers. Since last December, we’ve welcomed new partners like <a href="https://apps.microsoft.com/detail/xpdbvss44r0l9h?hl=">Notion</a>, <a href="https://apps.microsoft.com/detail/XP8JNQFBQH6PVF?hl=">Perplexity</a>, <a href="https://apps.microsoft.com/detail/XP8CBJ40XLBWKX?hl=">Docker</a> and <a href="https://apps.microsoft.com/detail/9NN2H8X92WBT?hl=">Day One</a>. And more are on the way – including <strong>Manus</strong>, an autonomous AI agent (productivity tool) designed to perform and deliver complex tasks for knowledge workers across various domains – so please keep checking for new releases.

Built with care and tested with precision, the Microsoft Store on Windows is here to help you find what you’re looking for. As always, we are listening to your feedback, so please submit via Feedback Hub (WIN + F) under Microsoft Store. We still have a lot more in the pipeline, so visit the “What’s New” section in the Store to stay connected on new releases.

<sup>1</sup>Feature availability varies by market.

<sup>2</sup> Data based on internal testing and subject to factors such as device, location, Windows and Store app versions.]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Enhance your application security with administrator protection</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/05/19/enhance-your-application-security-with-administrator-protection/</link>
		
		<dc:creator><![CDATA[Nilanjana Ganguly]]></dc:creator>
		<pubDate>Mon, 19 May 2025 16:30:15 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[News]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57440</guid>

					<description><![CDATA[<h3>Introduction</h3>
<p>Administrator protection is a new Windows 11 platform security feature that aims to protect the admin users on the device while still allowing them to perform the necessary functions which may require use of admin level permissi</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/05/19/enhance-your-application-security-with-administrator-protection/">Enhance your application security with administrator protection</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<h3>Introduction</h3>
Administrator protection is a new Windows 11 platform security feature that aims to protect the admin users on the device while still allowing them to perform the necessary functions which may require use of admin level permissions. This article provides the guidelines and best practices for applications running on Windows to elevate more securely with administrator protection enabled.

As Windows application developers, this article will guide you through best practices for using elevation in your applications and leveraging the new layer of security provided by administrator protection. This provides guidelines on what changes you need to make in your applications to make them compatible with administrator protection and help avoid breakages.

As IT professionals and technical users you will learn how the new administrator protection feature offers a more secure environment to run your applications, and how to navigate the changes effectively.
<h3>How administrator protection increases security</h3>
Applications are most vulnerable to attacks when operating with elevated privileges. In an elevated context, applications possess extensive abilities to alter configurations and implement systemwide changes, which can affect the overall security of the device. Running applications as admin increases the risk from malware infiltration. If malicious code is executed while elevated, it can capture tokens and then have an opportunity to move laterally within an organization, leading to a widespread compromise. Recent statistics from <a href="https://www.microsoft.com/en-us/security/security-insider/intelligence-reports/microsoft-digital-defense-report-2024">Microsoft Digital Defense Report 2024</a> indicate that token theft incidents, which abuse user privileges, have grown to an estimated 39,000 per day.

The administrator protection feature is designed to enhance security by minimizing the risks associated with elevated privileges. It safeguards users by providing just-in-time administrator privileges, incorporating Windows Hello to enhance both security and user convenience. Running applications with administrator protection enabled is crucial for maintaining a robust security posture.
<h3>Key design highlights</h3>
Administrator protection brings in a paradigm shift in user access control (UAC) architecture for admin users. It enforces the Principle of Least Privilege which brings in transparency for the elevations.
<ul>
 	<li><strong>New security boundary with separate profile: </strong>Administrator protection uses a hidden, system-managed, profile-separated local user account to create the isolated admin token. This helps ensure that user-level malware cannot access and compromise the code running in the elevated context, thus making elevation a security boundary. Application developers please note: This System Managed Administrator Account (SMAA) has a different <a href="https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/manage/understand-security-identifiers">security identifier</a> (SID).</li>
 	<li><strong>Just-in-time admin token</strong>: The admin token which is required to perform system tasks is generated from SMAA. This token is non-persistent and created just in time to perform elevation and is discarded once the task is completed. The whole cycle repeats itself when the user tries to perform another task which requires admin privileges.</li>
 	<li><strong>Removal of auto-elevation</strong>: Auto-elevation refers to allowing certain Windows processes and applications to automatically gain elevated privileges without prompting the user to consent. This can be exploited to perform User Access Control (UAC) bypass attacks where malware may misuse admin privileges to install an unwanted application on the device or for changing device security configuration thus compromising the security. With administrator protection, all auto-elevations in Windows are removed and users need to interactively authorize every admin operation. This helps ensure that the user stays in full control and that admin privileges are not abused.</li>
 	<li><strong>Windows Hello integration:</strong> Administrator protection is recommended to be used in conjunction with <a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/windows-hello">Windows Hello</a>. This further enhances security while providing a convenient authentication experience with face, fingerprint or PIN.​</li>
</ul>
<h3>Running applications elevated</h3>
Without administrator protection, an admin user would receive two access tokens upon logon – a full privilege, “elevated” administrator token with admin group set to “Enabled” and a restricted, “unelevated” access token with admin group set to “DenyOnly”. Now, when an application is launched elevated with this split-token administrator model, both “halves” of the user share a common profile. Despite each token being appropriately restricted in its use, both restricted and admin-level processes could access shared resources such as the user file system and the registry. This could be abused to perform classic UAC bypasses, such as the registry key manipulation and environment variable overloading attacks.

When an application is run elevated with administrator protection enabled, it is elevated using the profile separated SMAA. As separate-but-linked accounts, each with its own profile, file system directories, registry hives are no longer shared. This design mitigates the classic UAC bypass attacks as well as allows for the tokens to be created on-demand and discarded just as quickly, thus limiting exposure of the privileged token to the lifetime of the requesting process.

Understanding this new design is crucial for both app developers and technical users to ensure that it does not regress the functionality of their application when the same application is run elevated. The profile separated elevation implemented by administrator protection utilizes the same security principles as implemented by over-the-shoulder elevation for standard users in Windows. Running your application compatibly with administrator protection also helps ensure that it will run compatibly  with a standard user (user without administrative privileges).

The following behavior may be observed when an application is run elevated with administrator protection enabled:
<ul>
 	<li>Files created by apps when running elevated and saved in library folders (e.g. Documents, Pictures, Videos), will be saved into corresponding library folders of SMAA profile by default. If the user is using an app from an elevated context which needs to access a file created from the unelevated context, they will need to navigate to the unelevated library folders.</li>
</ul>
<ul>
 	<li>When running elevated, default registry hive for the current user will map to SMAA user’s registry hive instead of primary user’s registry hive.</li>
</ul>
<ul>
 	<li>Any application specific configuration like background color or font applied by the regular user will not be automatically applied to the SMAA profile, and vice versa.</li>
</ul>
<ul>
 	<li>Users may see more elevation prompts due to the removal of auto-elevations.</li>
</ul>
<h3>Recommendations</h3>
The following are general guidelines and best practices for installing and running applications with administrator protection:

<strong>Application installation</strong>
<ul>
 	<li>Per-user application installations should be run unelevated. Install all executables, applications and packages unelevated. This includes Win32 installers like setup.exe and package installers like MSIX or AppxBundle.</li>
</ul>
<ul>
 	<li>For installing Store applications, use the UI based installation whenever available. Preferably install from an unelevated context. If necessary, you can install elevated too.</li>
</ul>
<ul>
 	<li>If you need to install using API based Store apps they should be installed unelevated using the catalogID.</li>
</ul>
<ul>
 	<li><em>For app developers</em>: If installer elevation is absolutely necessary, then avoid placing app binaries under user profile folder. Use %ProgramFiles% for 64-bit apps or %ProgramFiles(x86)% for 32-bit installs on a 64-bit system. A better option is to use MSIX packaging since that allows for elevation too and manages the installation location itself.</li>
</ul>
<ul>
 	<li><em>For app developers</em>: Refrain from switching context from within the application during installation and continue to install either elevated or unelevated.</li>
</ul>
<ul>
 	<li><em>For app developers</em>: If the files created in the elevated context need to be accessed by the unelevated user (for example, log files using file explorer) then you need to store that file in the unelevated profile’s file directory. Avoid sharing files between elevated and unelevated context from your application.</li>
</ul>
<strong>Launching and running an application</strong>
<ul>
 	<li>Always run applications as non-elevated unless there is a specific task you intend to perform that needs elevation.</li>
</ul>
<ul>
 	<li>If you are required to run the app in both elevated and non-elevated mode, you will need to duplicate your application settings like theme, background, widgets etc., across the two elevated and non-elevated modes.</li>
</ul>
For example, when you change the theme to dark in the unelevated Notepad, the change will not be reflected automatically in the elevated Notepad. If you need parity you will need to make the change manually.

<img class="alignnone wp-image-57444 size-full" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/05/Notepad.png" alt="Screen showing Notepad screens in unelevated and elevated context side by side." width="624" height="275" />
<ul>
 	<li>Any file you create while running your application non-elevated, which needs to be saved in library folders should be saved in the regular user’s library folders, so that they are easy to retrieve when running the application non-elevated. If the files are stored in the library folders of elevated user profile, you will have to run the application elevated to access those files again.</li>
</ul>
<ul>
 	<li><em>For app developers</em>: Accessing the user registry (HKCU hive) and user registered COM classes should be in the context for the same user.</li>
</ul>
<ul>
 	<li><em>For app developers</em>: Since users will mostly be running app unelevated, apps need to be designed for granular use of elevated privileges, rather than elevating “up-front”.</li>
</ul>
<ul>
 	<li><em>For app developers</em>:  With administrator protection enabled there will not be any silent elevations; your application will request authentication at every elevation. To improve user experience, remove dependency on auto-elevation from your applications.</li>
</ul>
<strong>Using elevation</strong>
<ul>
 	<li>Run your apps with least privilege. Reduce and if possible, eliminate the need to elevate your apps.</li>
</ul>
<ul>
 	<li>Consider using system accounts and services-based design to manage processes that need to run elevated.</li>
</ul>
<h3>General troubleshooting guidelines</h3>
<ul>
 	<li>If you are not seeing the toggle to enable administrator protection in the Account protection section in Windows Security settings, make sure you are on a supported build.
<ul>
 	<li>Supported editions: Windows 11 Home, Windows 11 Professional, Windows 11 Enterprise and Windows 11 Education</li>
 	<li>Supported builds at GA: 24H2+ (Servicing build number will be communicated)</li>
 	<li>Preview builds: Windows Insider Program in Canary #27718+, Dev #26200.5702+, Beta #26120.4733+</li>
 	<li>Not supported in Windows Server editions, Windows 10 or other legacy editions.<strong> </strong></li>
</ul>
</li>
 	<li>If you enable administrator protection by toggling the switch on Windows security settings but you are not prompted with Windows Hello when you try to elevate, try the following:
<ul>
 	<li>Reboot the device.</li>
 	<li>Ensure <a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/windows-hello">Windows Hello</a> is enabled on the device.</li>
</ul>
</li>
</ul>
<ul>
 	<li>If your IT admin has informed you that administrator protection has been enabled on your device, but you are not prompted with Windows Hello when you try to elevate, try the following:
<ul>
 	<li>Ensure that there is sufficient time for your device to have synced with Intune. <a href="https://learn.microsoft.com/en-us/intune/intune-service/remote-actions/device-sync">Sync devices with Microsoft Intune | Microsoft Learn</a></li>
 	<li>Reboot your device.</li>
 	<li>Ensure <a href="https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/windows-hello">Windows Hello</a> is enabled on the device.</li>
</ul>
</li>
</ul>
<ul>
 	<li>To test if administrator protection is enabled on the device:
<ul>
 	<li>Launch command prompt as an administrator</li>
 	<li>Type in the command “whoami”</li>
 	<li>You will see the profile as “ADMIN_”</li>
</ul>
</li>
</ul>
<ul>
 	<li>Your device already has administrator protection enabled on which you have installed an application from an elevated context. If the application is not able to launch even after you have uninstalled administrator protection, you may need to reinstall your application.</li>
</ul>
<h3>Case study: Running Visual Studio with administrator protection</h3>
We recommend running Visual Studio as an unelevated user whenever possible. However, <a href="https://learn.microsoft.com/en-us/visualstudio/ide/user-permissions-and-visual-studio?view=vs-2022">installation and certain development, building, debugging, profiling and deployment scenarios</a> require elevated permissions.

In scenarios where Visual Studio is required to run elevated and the Windows administrator protection feature is enabled there are incompatibilities and Visual Studio is not supported in such a configuration. However, many scenarios continue to work and most of the incompatibilities are minor, but there are some incompatibilities that could block certain scenarios.

When running Visual Studio elevated with administrator protection enabled, you’ll notice some behavioral differences caused by the use of per user locations as illustrated in the examples below. The list below isn’t exhaustive and ultimately you will need to test your scenarios that require elevated Visual Studio with administrator protection enabled.
<ol>
 	<li>Extensions are typically per user and are by default installed into a location that’s specific to the user who installed the extension. As such per user extensions won’t be accessible from Visual Studio running as elevated with administrator protection enabled.</li>
 	<li>Various settings in Visual Studio are stored in per user locations and those settings will be different between Visual Studio running elevated or not. However, several settings are roamed and will be based on the user account that is signed into Visual Studio (see: <a href="https://learn.microsoft.com/en-us/visualstudio/ide/synchronized-settings-in-visual-studio?view=vs-2022">Synchronize settings across multiple computers - Visual Studio (Windows) | Microsoft Learn</a>).</li>
 	<li>In places where the default path includes the signed in user, such as in the new project dialog, Visual Studio will use the admin’s account which might not be accessible when not elevated.</li>
</ol>
<img class="alignnone wp-image-57445 size-full" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/05/Visual-Studio-1.png" alt="Visual Studio user interface" width="299" height="105" />

Whenever Visual Studio 2022 is running as an elevated process the top right of the IDE will contain a badge “Admin” as show in the screenshot below.

<img class="alignnone wp-image-57446 size-full" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/05/Visual-Studio-2.png" alt="Visual Studio user interface, top right of the IDE containing a badge “Admin”" width="372" height="130" />
<h3>Conclusion</h3>
Administrator protection is an upcoming security feature in Windows 11, offering robust security and user privilege management to help the user stay in control of changes to their Windows device. By requiring user authorization for administrative tasks, it helps safeguard the system from unauthorized changes and malware, enhancing overall device security. A seamless integration with <a href="https://blogs.windows.com/windows-insider/2024/11/01/announcing-windows-11-insider-preview-build-22635-4440-beta-channel/">modernized Windows Hello</a> helps provide a secure and convenient way to authorize the use of admin privileges.

Our goal is to enable administrator protection by default in Windows very soon. This feature is available now to Windows Insiders. We encourage you to try out your applications with administrator protection enabled and provide us with your <a href="https://support.microsoft.com/en-us/windows/send-feedback-to-microsoft-with-the-feedback-hub-app-f59187f8-8739-22d6-ba93-f66612949332">feedback</a>.

<strong>Securing the present, innovating for the future</strong>

Security is a shared responsibility. Through collaboration across hardware and software ecosystems, we can build more resilient systems secure by design and by default, from Windows to the cloud, enabling trust at every layer of the digital experience.

The updated <a href="https://learn.microsoft.com/en-us/windows/security/book/">Windows Security book</a> and Windows Server Security book  are available to help you understand how to stay secure with Windows. Learn more about <a href="https://www.microsoft.com/en-us/windows/business">Windows 11</a>, <a href="https://learn.microsoft.com/en-us/windows-server/">Windows Server</a> and <a href="https://www.microsoft.com/en-us/windows/business/devices/copilot-plus-pcs">Copilot+ PCs</a>. To learn more about Microsoft Security Solutions, visit our <a href="https://www.microsoft.com/en-us/security/business">website.</a>

Bookmark the <a href="https://www.microsoft.com/security/blog/">Security blog</a> to keep up with our expert coverage on security matters.

Also, follow us on LinkedIn (<a href="https://www.linkedin.com/showcase/microsoft-security/">Microsoft Security</a>) and X (<a href="https://twitter.com/@MSFTSecurity">@MSFTSecurity</a>) for the latest news and updates on cybersecurity.

<em><strong>Editor’s note – July 31, 2025 –</strong> Supported preview builds were updated</em>

<em><strong>Editor’s note – May 21, 2025 –</strong> Supported operating systems and platforms were updated.</em>

<em><strong>Editor’s note – May 19, 2025 –</strong> The installation guidelines for app developers were updated for clarity and consistency. </em>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>The Windows Subsystem for Linux is now open source</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/05/19/the-windows-subsystem-for-linux-is-now-open-source/</link>
		
		<dc:creator><![CDATA[Pierre Boulay]]></dc:creator>
		<pubDate>Mon, 19 May 2025 16:00:30 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57441</guid>

					<description><![CDATA[<p>Today we’re very excited to announce the open-source release of the Windows Subsystem for Linux. This is the result of a multiyear effort to prepare for this, and a great closure to the first ever issue raised on the Microsoft/WSL repo: <a href="ht
</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/05/19/the-windows-subsystem-for-linux-is-now-open-source/">The Windows Subsystem for Linux is now open source</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[Today we’re very excited to announce the open-source release of the Windows Subsystem for Linux. This is the result of a multiyear effort to prepare for this, and a great closure to the first ever issue raised on the Microsoft/WSL repo: <a href="https://github.com/microsoft/WSL/issues/1">Will this be Open Source? · Issue #1 · microsoft/WSL</a>.

That means that the code that powers WSL is now available on GitHub at <a href="https://github.com/microsoft/WSL/releases/tag/2.5.7">Microsoft/WSL</a> and open sourced to the community! You can download WSL and build it from source, add new fixes and features and participate in WSL’s active development.
<h3>WSL component overview</h3>
WSL is made of a set of distribution components. Some run in Windows, and some run inside the WSL 2 virtual machine. Here’s an overview of WSL’s architecture:

<img class="alignnone wp-image-57503 size-full" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/05/wsl-architecture.png" alt="Windows Subsystem for Linux architecture." width="1000" height="934" />

WSL's code can be broken up into these main areas:
<ul>
 	<li>Command line executables that are the entry points to interact with WSL
<ul>
 	<li>wsl.exe, wslconfig.exe and wslg.exe</li>
</ul>
</li>
 	<li>The WSL service that starts the WSL VM, starts distros, mounts file access shares and more
<ul>
 	<li>wslservice.exe</li>
</ul>
</li>
 	<li>Linux init and daemon processes, binaries that run in Linux to provide WSL functionality
<ul>
 	<li>init for start up, gns for networking, localhost for port forwarding, etc.</li>
</ul>
</li>
 	<li>File sharing Linux files to Windows with WSL's plan9 server implementation
<ul>
 	<li>plan9</li>
</ul>
</li>
</ul>
Head over to <a href="https://wsl.dev/">https://wsl.dev</a> to learn more about each component.

This comes as an addition to the already open sourced WSL components:
<ul>
 	<li><a href="https://github.com/microsoft/wslg">microsoft/wslg: Enabling the Windows Subsystem for Linux to include support for Wayland and X server related scenarios</a></li>
</ul>
<ul>
 	<li><a href="https://github.com/microsoft/WSL2-Linux-Kernel">microsoft/WSL2-Linux-Kernel: The source for the Linux kernel used in Windows Subsystem for Linux 2 (WSL2)</a></li>
</ul>
The following components are still part of the Windows image and are not open sourced at this time:
<ul>
 	<li>Lxcore.sys, the kernel side driver that powers WSL 1</li>
</ul>
<ul>
 	<li>P9rdr.sys and p9np.dll, which runs the “\\wsl.localhost” filesystem redirection (from Windows to Linux)</li>
</ul>
<h3>Why open source now? A bit of history…</h3>
WSL was first announced at BUILD back in 2016 and first shipped with the Windows 10 Anniversary update.

At that time WSL was based on a pico process provider, lxcore.sys, which enabled Windows to natively run ELF executables, and implement Linux syscalls inside the Windows kernel. This eventually became what we today know as “WSL 1”, which WSL still supports.

Over time it became clear that the best way to provide optimal compatibility with native Linux was to rely on the Linux kernel itself. WSL 2 was born, and first announced in 2019.

As the community behind WSL grew, WSL gained more features such as GPU support, graphical applications support (via wslg) and support for systemd.

It eventually became clear that to keep up with the growing community and feature requests, WSL had to move faster, and ship separately from Windows. That’s why in 2021 we separated WSL from the Windows codebase, and moved it to its own codebase. This new WSL first shipped as version 0.47.1 to the Microsoft Store, in July 2021. At the time, only Windows 11 was supported, and the package was marked as preview, only recommended to users that wanted to experience the latest and greatest of WSL.

We continued to develop this new “WSL package” until it was ready for general availability. That happened November of 2022, with WSL 1.0.0, which added support for Windows 10 and was the first “stable” release of this new WSL.

From there we kept on improving WSL, with the objective of fully transitioning all users to this new WSL package, and away from the WSL component that shipped with Windows. Windows 11 24H2 was the first Windows build that moved users from the “built-in” WSL to the “new” WSL package. We kept wsl.exe in the Windows image, so it could download the latest package on demand to make the transition easier.

As we kept on improving WSL, we eventually hit another milestone: WSL 2.0.0 (What are the three hardest problems in computer science? Off by one errors and naming things!).

WSL 2.0.0 introduced major improvements such as mirrored networking, DNS tunneling, session 0 support, proxy support, firewall support and more.

And that’s the milestone we’re still building on today! At the time of writing this article, WSL <a href="https://github.com/microsoft/WSL/releases/tag/2.5.7">2.5.7</a> is the latest available version out of our <a href="https://github.com/microsoft/WSL/releases/tag/2.5.7">nine pages of Github releases</a> since 0.47.1 4 years ago !
<h3>The community behind WSL</h3>
Over the years we’ve been incredibly lucky to have a strong community supporting WSL from day 1. We’ve been blessed with people sharing their knowledge, and spending countless hours to help track down bugs, find the best ways to implement new features and improve WSL.

WSL could never have been what it is today without its community. Even without access to WSL’s source code, people have been able to make major contributions that lead to what WSL is now.

This is why we’re incredibly excited to open-source WSL today. We’ve seen how much the community has contributed to WSL without access to the source code, and we can’t wait to see how WSL will evolve now that the community can make direct code contributions to the project.
<h3>Contributing to WSL</h3>
Are you interested in learning how WSL works? Would you like to see how a specific feature works, or make a change? Head over to <a href="https://github.com/microsoft/WSL">microsoft/WSL</a> to learn more!]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Introducing Windows ML: The future of machine learning development on Windows</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/05/19/introducing-windows-ml-the-future-of-machine-learning-development-on-windows/</link>
		
		<dc:creator><![CDATA[Tucker Burns]]></dc:creator>
		<pubDate>Mon, 19 May 2025 16:00:25 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57438</guid>

					<description><![CDATA[<p>Machine learning is at the forefront of technological innovation, enabling transformative user experiences. With the advances in client silicon and model miniaturization, new scenarios are feasible to run completely locally.</p>
<p>To support developers sh</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/05/19/introducing-windows-ml-the-future-of-machine-learning-development-on-windows/">Introducing Windows ML: The future of machine learning development on Windows</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[Machine learning is at the forefront of technological innovation, enabling transformative user experiences. With the advances in client silicon and model miniaturization, new scenarios are feasible to run completely locally.

To support developers shipping production experiences in the increasingly complex AI landscape, we are thrilled to announce the public preview of <strong>Windows ML</strong> – a cutting-edge runtime optimized for performant on-device model inference and simplified deployment, and the foundation of <a href="https://blogs.windows.com/windowsdeveloper/?p=57397">Windows AI Foundry</a>.

Windows ML is designed to support developers creating AI-infused applications with ease, harnessing the incredible strength of Windows’ diverse hardware ecosystem whether it’s for entry-level laptops, <a href="https://www.microsoft.com/en-us/windows/copilot-plus-pcs">Copilot+ PCs</a> or <a href="https://blogs.windows.com/windowsdeveloper/?p=57439">top-of-the-line AI workstations</a>. It’s built to help developers leverage the client silicon best suited for their specific workload on any given device – whether it’s an NPU for low-power and sustained inference, a GPU for raw horsepower or CPU for the broadest footprint and flexibility.

Windows ML provides a unified framework so developers can confidently target Windows 11 PCs that are available today. It was built from the ground up to optimize model performance and agility and to respond to the speed of innovation in model architectures, operators and optimizations across all layers of the stack. Windows ML is an evolution of DirectML (DML) based on our learnings from the past year, listening to feedback from many developers, our silicon partners and our own teams developing AI experiences for Copilot+ PCs. Windows ML is designed with this feedback in mind, empowering our partners – AMD, Intel, NVIDIA, Qualcomm – to leverage the execution provider contract to optimize model performance, and match the pace of innovation.

Windows ML is powered by ONNX Runtime Engine (ORT), allowing developers to utilize the familiar ORT APIs. With ONNX as a native model format and support for pytorch to intermediate representations for the EPs, Windows ML ensures seamless integration with existing models and workflows. A key design aspect is leveraging and enhancing the existing ORT Execution Provider (EP) contract to optimize workloads for varied client silicon. Built in partnership with our Independent Hardware Vendors (IHVs), these execution providers are designed to optimize model execution on existing and emerging AI processors, enabling each to showcase their fullest capability. We’ve been working closely with AMD, Intel, NVIDIA and Qualcomm to integrate their EPs seamlessly in Windows ML, and are pleased to support the full set of CPUs, GPUs and NPUs from day one.

<img class="alignnone wp-image-57497 size-large" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/05/WindowsML_1920x1280@2x-1024x683.jpg" alt="Windows ML architecture." width="1024" height="683" />

<strong>AMD</strong> fully supports Windows ML for Ryzen AI products, where their AMD GPU and AMD NPU Execution Provider enables maximum leverage of GPU and NPU in their platforms. <a href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.amd.com%2Fen%2Fblogs%2F2025%2Fai-pcs-with-ryzen-ai-300-series-and-windows-ml.html&amp;data=05%7C02%7Cpamelaberry%40microsoft.com%7Cef1914d378bf4c43f29708dd93f13f12%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638829386617749529%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&amp;sdata=dADQxW%2Fa9Q1fl6UcyXmS5P0yxwyvgRkYFa5xNtKXvmI%3D&amp;reserved=0"> Learn more</a>.
<p style="padding-left: 40px;"><em>"Windows ML seamlessly integrates across CPUs, GPUs and NPUs across the AMD’s portfolio including Ryzen AI 300 series, empowering ISVs to deliver groundbreaking AI experiences. This deep partnership between Microsoft and AMD is driving the future of AI on Windows, optimizing performance, efficiency and accelerating innovation."</em> — <strong>John Rayfield, Vice President of AI, AMD</strong></p>
<strong>Intel </strong>integrates the performance and efficiency of OpenVINO across CPUs, GPUs and NPUs with the development and deployment simplicity provided by Windows ML, enabling AI developers to more easily target the XPU that best fits their workload across the broad scale of products powered by Intel Core Ultra Processors.  <a href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Unlocking-AI-Development-with-Windows-ML-Intel-and-Microsoft-s/post/1689650">Learn more</a>.
<p style="padding-left: 40px;"><em>“Intel’s partnership with Microsoft on Windows ML supercharges AI app development by tightly integrating high-performance, high-accuracy workflows into the Windows ecosystem. Whether targeting CPU, GPU or NPU, developers can flexibly deploy across any XPU. With the Intel and OpenVINO integration, the focus shifts from plumbing to progress—unlocking faster, smarter AI-powered apps for Windows users everywhere.”</em>  – <strong>Sudhir Tonse Udupa, Vice President, AI PC Software Engineering, Intel</strong></p>
<strong>NVIDIA’s </strong>new TensorRT EP is the fastest way to execute AI models on NVIDIA RTX GPUs for the more than 100 million RTX AI PCs. When compared to the previous Direct ML implementation, TensorRT for RTX delivers up to 2x faster performance for AI workloads.  <a href="https://developer.nvidia.com/blog">Learn more</a>.
<p style="padding-left: 40px;"><em>“Today, Windows developers must often choose between broad hardware compatibility and full performance for AI workloads. Through Windows ML, developers can easily support a wide spectrum of hardware while achieving full TensorRT acceleration on NVIDIA GeForce RTX and RTX PRO GPUs.” </em>– <strong>Jason Paul, Vice President of Consumer AI, NVIDIA</strong></p>
<strong>Qualcomm Technologies Inc. </strong>and Microsoft collaborated to develop and optimize Windows ML-based AI models and applications for the NPU found in Snapdragon X Series processor using the Qualcomm Neural Network Execution Provider (QNN EP). <a href="https://www.qualcomm.com/news/onq/2025/05/microsoft-qualcomm-collaborate-on-windows-11-copilot-plus-pcs-windows-ai-foundry">Learn more</a>.
<p style="padding-left: 40px;"><em>"The new Windows ML's cutting-edge runtime not only optimizes on-device model inference but also simplifies deployment, making it easier for developers to harness the full potential of advanced AI processors in Snapdragon</em><em> X</em><em> Series platforms<strong>. </strong>Windows ML’s unified framework and support for diverse hardware, including our NPUs, GPUs and CPUs, ensures that developers can create AI applications that deliver exceptional performance and efficiency across a wide range of devices. We look forward to continuing our collaboration with Microsoft to drive innovation and velocity of development to bring the best AI experiences on Windows Copilot+ platforms." </em>– <strong>Upendra Kulkarni, VP, Product Management, Qualcomm Technologies, Inc.</strong></p>
There are a few key aspects to highlight for Windows ML:
<ul>
 	<li><strong>Simplified Deployment:</strong> Leveraging our infrastructure APIs, developers no longer need to create multiple builds of their app to target different silicon as they don’t have to bundle ONNX or execution providers in their application directly. We’ll make them available on the device and provide simple ways of registering them and enabling on-device ahead-of-time (AOT) model compilation.</li>
</ul>
<ul>
 	<li><strong>Advanced Silicon Targeting:</strong> Leverage device policies to optimize for low-power, high performance, or override to specify exactly what silicon to leverage for a specific model. In the future this will enable split processing for optimal performance – leveraging CPU or GPU for some pieces of a model and NPU for others.</li>
</ul>
<ul>
 	<li><strong>Performance:</strong> Windows ML is designed for performance; built on the foundations of ONNX and ONNX Runtime we see up to 20% improvement compared to other model formats. Over time we will add more Windows-specific capabilities for further optimization, like progressive memory mapping, partial model pinning and an optimized scheduler for parallel execution.</li>
</ul>
<ul>
 	<li><strong>Compatibility: </strong>Working with our IHV partners, Windows ML will guarantee conformance and compatibility, so you can rely on continued improvement while guaranteeing accuracy build-over-build for your models.</li>
</ul>
But it’s not just about the runtime, we are also introducing a robust set of tools in <a href="https://code.visualstudio.com/docs/intelligentapps/overview">AI Toolkit for VS Code</a> (AI Toolkit) to support model and app preparation – conversion to ONNX from PyTorch, quantization, optimization, compilation and profiling, to help developers ship production applications with proprietary or open-source models. These tools are specifically designed to simplify the process of preparing and shipping performant models via Windows ML without having to create multiple builds and complex logic.

<img class="alignnone wp-image-57498 size-full" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/05/ModelConversion-Build2025.gif" alt="Animated image of Windows ML user interface" width="800" height="636" />

Windows ML is available in public preview starting today on all Windows 11 machines worldwide, offering developers the opportunity to explore its capabilities and provide feedback. The preview includes two layers of APIs:
<ul>
 	<li><strong>ML Layer:</strong> High-level APIs for runtime initialization, dependency management and helper APIs for establishing generative AI loops.</li>
</ul>
<ul>
 	<li><strong>Runtime Layer:</strong> Low-level ONNX Runtime APIs for fine-grained control of on-device inference.</li>
</ul>
To get started, <a href="https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio&amp;ssr=false#overview">install AI Toolkit</a>, leverage one of our conversion and optimization templates, or start building your own. Explore documentation and code samples available on <a href="https://learn.microsoft.com/en-us/windows/ai/windows-ml/">Microsoft Learn</a>, check out AI Dev Gallery (<a href="https://ms-windows-store/pdp/?productid=9N9PN1MM3BD5">install</a>, <a href="https://learn.microsoft.com/en-us/windows/ai/ai-dev-gallery/">documentation</a>) for demos and more samples to help you get started with Windows ML.

<img class="alignnone wp-image-57499 size-large" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/05/Windows_ML_Partners-1024x342.png" alt="Logos of companies under a banner reading Windows ML." width="1024" height="342" />

While building Windows ML, it was important to us to receive feedback and perspective from app developers, especially those who are at the forefront of delivering AI-powered features and experiences. We shared early previews of Windows ML with a few leading developers who are testing integration with Windows ML and we’re thrilled by their early reactions:

<strong>Adobe </strong>(Volker Rölke - Senior ML Computer Scientist): <em>“Adobe Premiere Pro and After Effects juggles terabytes of footage and heavy ML workloads. A reliable Windows ML API that delivers consistent performance across heterogeneous devices would remove huge obstacles and let us ship more exceptional features faster. Windows ML can help us take a hardware-agnostic approach with far less boiler-plate system checks and low-level decision-making.”</em>

<strong>Bufferzone</strong> (Dr. Ran Dubin, CTO, Bufferzone): <em>“At Bufferzone, we believe that AI powered PCs represent the future of endpoints. Windows ML simplifies integration challenges for ISVs, reduces time to market and fosters a higher adoption rate. As a result, customers will gain significantly more from their PCs which is a tremendous benefit for everyone.”</em>

<strong>Filmora </strong>(Luyan Zhang – AI Product Manager): <em>“The simplicity amazes me.  Following Microsoft's easier approach with ONNX models added to our app. We converted a complex AI feature to Windows ML in just 3 days."</em>

<strong>McAfee</strong> (Carl Woodward, Sr. Principal Engineer): <em>“We’re excited about the efficiencies Windows ML can bring to the development and management of the new scam detection capabilities in McAfee+.</em><em> Windows ML will allow us to focus on high-impact areas like model accuracy and performance, while providing confidence that AI components work well across the entire ecosystem, including new hardware revisions.</em><em>”</em>

<strong>Powder </strong>(Barthélémy Kiss - Co-founder and CEO at Powder): <em>“Powder is an early adopter of Windows ML, and it has enabled us to integrate models 3x faster, transforming speed into a key strategic advantage.  With Windows 11 handling the heavy lifting across silicon providers, now we can focus more on doing what our Powder developers do best </em>–<em> developing more magical AI video experiences in less time, and at a drastically lower operating cost.”</em>

<strong>Reincubate </strong>(Aidan Fitzpatrick – CEO): <em>“We're committed to supporting and making the most of new AI hardware chipsets on day one. And Windows ML should be a powerful tool in helping us to move at the speed of silicon innovation. For us, the holy grail is being able to take a single high precision model and have it just work seamlessly across Windows silicon and we think Windows ML is an important step in the right direction.”</em>

<strong>Topaz Labs</strong><strong> </strong>(Dr. Suraj Raghuraman – Head of AI Engine): <em>“Windows ML will reduce our installer size tremendously, going down from gigabytes to</em><em> megabytes.</em><em> This will allow our users to do more things on their disk, because the</em><em> model storage</em><em> requirement goes down as well. Since Windows ML relies heavily on ONNX runtime,</em><em> it was really easy for us to integrate it.</em><em> We integrated the entire API</em><em> within a couple of days and it has been a seamless experience from an innovation standpoint.</em><em>”</em>

Whether you’re a seasoned AI developer or exploring ML for the first time, Windows ML empowers you to focus on innovation rather than infrastructure management, enabling you to delight your customers with AI-infused applications with reduced app footprints. Windows ML will be generally available later this year. In the meantime, we look forward to your feedback and seeing how you leverage Windows ML to create solutions that redefine possibilities. <a href="https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview">Join the Windows ML journey today</a> and be part of the next wave of AI innovation!

<em><strong>Editor's note – May 19, 2025 –</strong> The section above about Windows ML being powered by ONNX Runtime Engine was updated to provide additional details.</em>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Accelerating AI development with Windows-based AI workstations</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/05/19/accelerating-ai-development-with-windows-based-ai-workstations/</link>
		
		<dc:creator><![CDATA[Steve Clarke, Editor]]></dc:creator>
		<pubDate>Mon, 19 May 2025 16:00:20 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57439</guid>

					<description><![CDATA[<p>Today, we <a href="https://blogs.windows.com/windowsdeveloper/?p=57397">announced</a> powerful capabilities for AI development with Windows AI Foundry, featuring components like <a href="https://blogs.windows.com/windowsdeveloper/?p=57438">Windows ML</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/05/19/accelerating-ai-development-with-windows-based-ai-workstations/">Accelerating AI development with Windows-based AI workstations</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[Today, we <a href="https://blogs.windows.com/windowsdeveloper/?p=57397">announced</a> powerful capabilities for AI development with Windows AI Foundry, featuring components like <a href="https://blogs.windows.com/windowsdeveloper/?p=57438">Windows ML</a> that enable developers to bring their own models and deploy them efficiently across diverse silicon partner ecosystem including – AMD, Intel, NVIDIA and Qualcomm spanning CPU, GPU, NPU (Neural Processing Unit).  In the rapidly evolving field of AI development, having robust hardware is just as important as reliable software and tools for optimizing performance and efficiency.

We heard from developers that they need powerful workstations equipped with advanced CPUs, GPUs and specialized processors like <a href="https://www.microsoft.com/en-us/surface/do-more-with-surface/what-are-npus?msockid=30d957fd7d1e69fb121742557c406892">NPUs</a>  with blazing fast memory and storage for moving massive amounts of data around to handle the intensive computational demands of AI tasks such as data processing, model training, finetuning, inference and deployment. A robust Windows-based workstation combined with a reliable platform like Windows AI Foundry enables AI developers to streamline their workflows, reduce latency and achieve real-time processing capabilities.

Your AI stack, your machine: The case for Windows-based Workstations

For AI developers, a workstation is more than just a powerful machine; it's a critical asset that enhances their ability to develop, test and deploy AI models efficiently and more importantly – locally. It offers a few key benefits:
<ul>
 	<li><strong>Privacy and Security</strong>: Workstations enable local development without the need for internet access, ensuring sensitive data remains on device, minimizing security threats and maximizing privacy.</li>
</ul>
<ul>
 	<li><strong>Cost Efficiency</strong>: By handling fine-tuning and inference locally, developers can reduce recurring cloud compute costs. In the case of Copilot+ PCs, local AI workloads are becoming more energy efficient, lowering overall power consumption.</li>
</ul>
<ul>
 	<li><strong>Speed and Reliability</strong>: Developers can iterate quickly and debug without delays from cloud syncs or network dependencies. The hardware also enables low latency inferencing and high-performant user experiences.</li>
</ul>
Choosing a Windows-based AI workstation is especially important for Windows developers who require speed, flexibility and control in AI workloads. It is also ideal for leveraging <a href="https://learn.microsoft.com/windows/ai/toolkit/toolkit-getting-started?tabs=rest">AI Toolkit</a> for Visual Code and the full capabilities of Windows AI Foundry and its components such as Foundry Local and  <a href="https://blogs.windows.com/windowsdeveloper/?p=57438">Windows ML</a>, which are optimized for local workflows.
<h3>Diverse options of Windows workstations, catering to the demanding needs of AI developer workloads</h3>
AI developers on Windows 11 can choose from a wide array of PC hardware from OEM partners like Dell, HP, Lenovo and more. Not only are these systems optimized for AI model finetuning, inferencing and deployment, but they also provide flexibility in your developer workflow with different form factors and price points.

For developers requiring raw power, a desktop workstation with a powerful CPU and GPU can significantly reduce the time for local model finetuning. Those looking for physical space efficiency may consider a mini workstation, while developers on the move can benefit from a mobile workstation, which enables them to run inference on a proprietary model locally in absence of a stable network connection.

At Build 2025, we featured the latest workstations from a few of our OEM partners:
<ul>
 	<li>The <strong>Dell Pro Max Tower T2</strong> desktop leads in thermal innovation and is the world’s fastest tower for single-threaded application performance,<sup>1</sup> made possible by Dell’s exclusive unlimited turbo duration technology. This ensures top-tier performance under sustained heavy workloads. Turbo duration allows the device to excel in prolonged intensive tasks, keeping AI applications running efficiently over extended periods without compromising thermal integrity. The Tower will be available with Intel’s Core Ultra series processors delivering up to 32GB of DDR5 memory, 1TB self-encrypting storage and NVIDIA’s RTX PRO Blackwell Generation GPUs in July 2025. <a href="https://www.dell.com/en-us/plcp/lp/dell-pro-max-pcs">Learn more about Dell Pro Max PCs</a>.</li>
</ul>
<ul>
 	<li>The <strong>Dell Pro Max 16</strong> laptop packs high-end performance in a portable, lightweight and modern design. The 16-inch QHD+ resolution display provides expansive screen real estate and supports complex and demanding applications on-the-go. The laptop is available on both AMD Ryzen AI PRO 300 series and Intel Core Ultra series processors running NVIDIA’s RTX PRO Blackwell Generation Laptop GPUs in July 2025, providing engineers and designers with performance for AI inferencing, rendering and creative applications. <a href="https://www.dell.com/en-us/plcp/lp/dell-pro-max-pcs">Learn more about Dell Pro Max PCs</a>.</li>
</ul>
<ul>
 	<li>The<strong> HP</strong> <strong>ZBook Ultra G1a 14”</strong> powerful 14-inch mobile workstation laptop defies what's possible on the go. A Copilot+ PC with a compact design and new AMD Ryzen AI Max PRO processor, the ZBook Ultra offers the performance to tackle workflows previously not possible on a laptop. Take desk-bound workflows anywhere with an ultra-thin and light design, combined with a long-lasting battery, Next-Gen AI PC features and AI-enhanced privacy. Featuring up to 16 desktop-class CPU cores, discrete-like integrated graphics and up to 128GB of unified memory architecture, and the ability to assign up to 96GB RAM to the GPU, the ZBook Ultra enables seamless multitasking between applications. <a href="https://bit.ly/HPZMSBUILD">Learn more about the ZBook Ultra G1a</a>.</li>
</ul>
<ul>
 	<li>The <strong>HP</strong> <strong>Z2 Mini G1a</strong> mini workstation is equipped with the AMD Ryzen AI Max PRO processor and scalable 128GB of unified memory architecture with the ability to assign up to 96GB exclusively to the GPU, the Z2 Mini G1a delivers performance for graphics-intensive workflows. With an internal power supply, this PC cleanly fits on a desk, mounted behind a monitor or attached under a desk. It also easily fits in a rack, and its small size allows for a high-density rack mount solution that combines performance, manageability and security. <a href="https://bit.ly/HPZMSBUILD">Learn more about the Z2 Mini G1a</a>.</li>
</ul>
<ul>
 	<li>The<strong> Lenovo ThinkPad P14s Gen 6</strong> mobile workstation laptop is a Copilot+ PC that combines pro-grade performance with true mobility. Featuring the latest AMD Ryzen AI PRO 300 series processors and integrated AMD Radeon graphics, it’s ISV-certified and ideal for workflows such as 2D CAD, light 3D design and content creation. Built for hybrid professionals, students and educators, it balances performance, security and battery life in a compact 14-inch chassis. <a href="https://www.lenovo.com/us/en/workstations/">Learn more about Lenovo workstations</a>.</li>
</ul>
<ul>
 	<li>The <strong>Lenovo</strong> <strong>ThinkPad P16s Gen 4</strong> mobile workstation laptop is a Copilot+ PC that offers enhanced performance, a larger display and greater configuration flexibility. Also powered by AMD Ryzen AI PRO 300 series processors and certified for key ISV applications, it’s built to handle more demanding workloads, such as 3D CAD, BIM modeling and data-heavy tasks. With a 16-inch screen, integrated AI acceleration and robust ThinkShield security, it’s ideal for professionals in engineering, architecture and media. <a href="https://www.lenovo.com/us/en/workstations/">Learn more about Lenovo workstations</a>.</li>
</ul>
<h3>Examples of how workstations benefit the developer workflow</h3>
<strong>Fine-Tuning on the Dell Pro Max Tower T2 with Intel Core Ultra 7 processor and the NVIDIA RTX PRO 6000 Blackwell GPU</strong>

Local fine-tuning on a workstation improves developer productivity by offering a quick, powerful and easy way to run and debug your prototypes locally before you take your final workflows to cloud.

With the Dell Pro Max Tower T2, we fine-tuned the <a href="https://techcommunity.microsoft.com/blog/educatordeveloperblog/welcome-to-the-new-phi-4-models---microsoft-phi-4-mini--phi-4-multimodal/4386037"><strong>Microsoft's Phi-4-mini</strong></a> model using the NVIDIA GPU with <a href="https://aka.ms/win-lora">LoRA (low-rank adaption)</a> in <a href="https://learn.microsoft.com/windows/ai/toolkit/toolkit-getting-started?tabs=rest">AI Toolkit</a> and the <a href="https://huggingface.co/datasets/yahma/alpaca-cleaned">alpaca-cleaned dataset</a> which contained 51,760 prompts. For a batch size of 8 prompts, the system was able to backpropagate 2.16 batches per second, finishing 3 full epochs of fine-tuning in about 2 hours and 15 minutes<strong>. </strong>The same workflow could take up to a day to complete on the cloud due to round trip latency with the additional costs of cloud billing.

<strong>Example 1: Fine-tuning Phi-4-mini model local off the NVIDIA RTX PRO 60000 GPU on a Dell Pro Max Tower T2 (sped up 150x)</strong>

https://www.youtube.com/watch?v=P2vsNBMFAsA

<strong>AI Anywhere: Harnessing GPU Power on the go with the HP ZBook Ultra G1a 14” </strong>

The new HP ZBook Ultra G1a 14” powered by the AMD Ryzen AI Max+ PRO 395 is a Copilot+ PC that leverages the power of a CPU, GPU and NPU to maximize performance and resources in various workflows.

<strong>Example 2: Loading and running 70b DeepSeek R1 locally on HP ZBook Ultra G1a 14”</strong>

In one use case, we utilized AI Tool Kit for VS Code to load and run a large 70b parameter DeepSeek R1 model locally – an impressive feat for a mobile workstation.

https://www.youtube.com/watch?v=gm2iZxnJP8A

<strong>Example 3: Running SDXL and Phi-4 Mini concurrently on the HP ZBook Ultra G1a 14” laptop</strong>

In another use case, we ran Stable Diffusion XL (SDXL) model and Phi-4 Mini CPU ONNX model at the same time. The image generation ran at ~3 to 6 it/sec concurrently while the text generation task concluded within 7-17 tokens/sec. You can see from the task manager that the GPU is utilized to give max processing performance. The NVMe SSD makes it ideal for high performance applications like these.

https://www.youtube.com/watch?v=6vkHpTTMvOc

To run both the image generation and text generation concurrently, you typically need a system with enough processing power, memory and storage. Although a desktop setup can meet these requirements, the HP ZBook Ultra G1a 14” laptop is capable of handling such tasks within a compact form factor for productivity on the go.
<h3>Building for the future of AI on Windows based workstations</h3>
By 2027, it is expected that 60% of PCs shipped will feature on-device AI capabilities<sup>2</sup> pushing the boundaries of AI models running locally. Having the right workstation not only accelerates the development cycle but also ensures that AI models run across a wide breadth of CPUs, GPUs and NPUs. Understanding the breadth of available options and investing in the right workstation can significantly enhance developers’ ability to innovate and succeed in the competitive landscape of AI.

To drive innovation, both software and hardware must align. Not only do we encourage developers to check out <a href="https://aka.ms/WindowsAIFoundry">Windows AI Foundry</a>, but we also encourage developers to leverage the latest hardware in Windows-based AI workstations for the best developer experience.

<em>1 The Dell Pro Max Tower T2 is the World’s fastest for single-threaded application performance – Based on internal analysis of competitors within the entry level workstation space, Lenovo P3 Ultra, HP Z2 G9 Mini, HP Z2 G1a Mini and Lenovo P3 Tiny. February 2025</em>

<em>2 Based on report by Canalys </em><a href="https://www.canalys.com/reports/AI-PC-market-forecasts"><em>“Now and next for AI-capable PCs: Revolutionizing computing: AI PCs and the market outlook.”</em></a><em> (January 2024)</em>]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
