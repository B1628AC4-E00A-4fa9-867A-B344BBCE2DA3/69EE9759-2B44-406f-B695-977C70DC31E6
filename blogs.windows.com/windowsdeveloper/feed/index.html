<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Windows Developer Blog</title>
	<atom:link href="https://blogs.windows.com/windowsdeveloper/feed/" rel="self" type="application/rss+xml" />
	<link>https://blogs.windows.com/windowsdeveloper/</link>
	<description></description>
	<lastBuildDate>Mon, 14 Apr 2025 19:31:11 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.5.5</generator>

<image>
	<url>https://winblogs.thesourcemediaassets.com/sites/3/2021/06/cropped-browser-icon-logo-32x32.jpg</url>
	<title>Windows Developer Blog</title>
	<link>https://blogs.windows.com/windowsdeveloper/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>GitHub Actions now supports Windows on Arm runners for all public repos</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/04/14/github-actions-now-supports-windows-on-arm-runners-for-all-public-repos/</link>
		
		<dc:creator><![CDATA[Marcus Perryman]]></dc:creator>
		<pubDate>Mon, 14 Apr 2025 19:31:11 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57413</guid>

					<description><![CDATA[<p>We are continuously investing in improving the Windows on Arm developer experience by providing and improving the tools needed by developers targeting Arm powered Copilot+ PCs.</p>
<p>Today we are thrilled to announce Windows on Arm runner availability has</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/04/14/github-actions-now-supports-windows-on-arm-runners-for-all-public-repos/">GitHub Actions now supports Windows on Arm runners for all public repos</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[We are continuously investing in improving the Windows on Arm developer experience by providing and improving the tools needed by developers targeting Arm powered Copilot+ PCs.

Today we are thrilled to announce Windows on Arm runner availability has been extended to all public repos, including GitHub Free tier accounts.

This enhancement simplifies continuous integration workflows and enables developers to better support Arm-based Windows applications. With this update all accounts, including open-source projects using free tier accounts, can incorporate Windows on Arm targets into their CI pipelines for public projects, achieving a similar level of build and regression testing as is available for Intel targets. This improvement simplifies the process for maintainers to add Windows Arm targets to CI jobs to ensure compatibility and reliability across a wider range of architectures.
<h3>Key Benefits</h3>
<ol>
 	<li>Expanded Architecture Support: With the continued growth of Windows on Arm devices – especially the Qualcomm powered Copilot+ PCs - developers can easily extend support to this platform without requiring additional infrastructure.</li>
 	<li>Improved Continuous Integration workflows: Adding Arm runners to your pipeline allows for consistent testing and building across both Arm and Intel architectures to pick up regressions as early as possible.</li>
 	<li>Directly benefits open-source projects: The availability of Arm runners for all public repos, including free tier accounts, reinforces GitHub’s commitment to supporting innovation in the open-source community. To learn more about eligibility for GitHub free tier, visit the <a href="https://github.com/pricing">GitHub Pricing page</a>.</li>
</ol>
The new Windows 11 Arm image for GitHub runners ships with tooling and software for a variety of different development environments. <a href="https://github.com/actions/partner-runner-images/blob/main/images/arm-windows-11-image.md">Find a full list of included dependencies</a>.
<h3>Getting Started</h3>
Adding Windows on Arm targets to your CI pipeline via GitHub Actions is easy. Windows on Arm runners can be added to your public repos by adding the “windows-11-arm” runner target in your yml workflow. Below is an example GitHub Actions workflow for a C# project using Visual Studio tools:
<img class="alignnone wp-image-57419 size-full" src="https://pub-d00f534024b04d0e8036586fc78a41fa.r2.dev/sites/3/2025/04/Code.png" alt="Example GitHub Actions workflow for a C# project using Visual Studio tools" width="523" height="439" />
<h3>Moving Forward</h3>
The inclusion of Windows on Arm runners in GitHub Actions marks a valuable step forward for open-source projects and developers aiming to support diverse architectures. By integrating Arm targets into your workflows, you can ensure your applications deliver reliable performance across platforms.

Try it out today and broaden your project's reach and compatibility.

<a href="https://docs.github.com/en/actions/using-github-hosted-runners/using-github-hosted-runners/about-github-hosted-runners?supported-runners-and-hardware-resources=">Find out more about adding GitHub-hosted runners to your repository</a>.]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Part 3 – Babylon.js 8.0: glTF, USDz, and WebXR advancements</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/04/03/part-3-babylon-js-8-0-gltf-usdz-and-webxr-advancements/</link>
		
		<dc:creator><![CDATA[Steve Clarke, Editor]]></dc:creator>
		<pubDate>Thu, 03 Apr 2025 20:02:17 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57383</guid>

					<description><![CDATA[<p>The latest Babylon.js 8.0 release includes several updates to assist developers in creating beautiful and performant 3D experiences.</p>
<h3><strong>Updated glTF Support — KHR_materials_diffuse_transmission</strong></h3>
<p>Babylon.js 8.0 continues the lo</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/04/03/part-3-babylon-js-8-0-gltf-usdz-and-webxr-advancements/">Part 3 – Babylon.js 8.0: glTF, USDz, and WebXR advancements</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[The latest Babylon.js 8.0 release includes several updates to assist developers in creating beautiful and performant 3D experiences.
<h3><strong>Updated glTF Support — KHR_materials_diffuse_transmission</strong></h3>
Babylon.js 8.0 continues the long rich tradition of supporting every extension update to the glTF format. As glTF advances, Babylon.js is in lock-step with support for those advancements. Babylon.js 8.0 brings support for the very beautiful KHR_materials_diffuse_transmission!

Check out a demo: <a href="https://aka.ms/babylon8gltfdemo">https://aka.ms/babylon8gltfdemo</a>

Learn more here: <a href="https://aka.ms/babylon8gltfDoc">https://aka.ms/babylon8gltfDoc</a>

https://youtu.be/c5qumu-k9lQ

Credit to<a href="https://www.linkedin.com/in/ericchadwick/"> Eric Chadwick</a> for the beautiful asset, seriously, Eric’s work is incredible, go support him!
<h3><strong>glTF Exporter Improvements</strong></h3>
Babylon.js introduces a host of new improvements to the glTF serializer, allowing you to export your Babylon scenes as glTF objects with support for the latest extensions and features!

Check out a demo: <a href="https://aka.ms/babylon8gltfSerializerDemo">https://aka.ms/babylon8gltfSerializerDemo</a> (try exporting as glTF and opening in your favorite 3D tool)

Learn more here: <a href="https://aka.ms/babylon8gltfSerializerDoc">https://aka.ms/babylon8gltfSerializerDoc</a>

https://youtu.be/RSbGAeS-FEI
<h3><strong>More glTF Loader Options</strong></h3>
Babylon.js 8.0 unlocks new loader options for glTF objects allowing you to programmatically load your assets with pre-determined specifications, such as loading at a specific LOD (Level of Detail). These new loader options will provide new flexibility and control of how you bring assets into your Babylon.js scenes!

Check out a demo: <a href="https://aka.ms/babylon8glTFLoaderDemo">https://aka.ms/babylon8glTFLoaderDemo</a>

Learn more here: <a href="https://aka.ms/babylon8glTFLoaderDoc">https://aka.ms/babylon8glTFLoaderDoc</a>

https://youtu.be/hrZNyJyf9hA
<h3><strong>IES Light Support</strong></h3>
IES Lighting is a technique that describes the “shape” of a light that emits from a lamp. You can read more about it here: <a href="https://ieslibrary.com/">IES-Library</a>. Babylon.js brings support for IES files, unlocking new ways to express dimension and lighting in your scenes!

Check out a demo: <a href="https://aka.ms/babylon8IESLightDemo">https://aka.ms/babylon8IESLightDemo</a> (click/touch to see different IES light shapes)

Learn more here: <a href="https://aka.ms/babylon8IESLightDoc">https://aka.ms/babylon8IESLightDoc</a>

https://youtu.be/j-InYXIJr2w
<h3><strong>USDZ Export</strong></h3>
Babylon.js 8.0 allows you to export .usdz files, making it easier than ever for you to build AR experiences targeted for iOS devices!

Check out a demo: <a href="https://aka.ms/babylon8usdzDemo">https://aka.ms/babylon8usdzDemo</a>

Learn more here: <a href="https://aka.ms/babylon8usdzDoc">https://aka.ms/babylon8usdzDoc</a>

https://youtu.be/DerHDOT0TYE
<h3><strong>GPU Mesh Picking</strong></h3>
Picking meshes in a scene can be an expensive feature since the CPU has to loop through every piece of geometry to find the triangle that most closely intersects the ray cast from the picking location.

Ready for some wizardry? Babylon.js 8.0 introduces the ability to pick meshes from directly from the GPU! Yup, that’s right. This means that in complex scenes, where picking is required, you can boost performance by offloading this to the GPU.

Check out a demo: <a href="https://aka.ms/babylon8gpuPickDemo">https://aka.ms/babylon8gpuPickDemo</a>

Learn more here: <a href="https://aka.ms/babylon8gpuPickDoc">https://aka.ms/babylon8gpuPickDoc</a>

https://youtu.be/4hBhUFk-k_U
<h3><strong>GPU Bounding Box</strong></h3>
Bounding Box calculation can be expensive, especially if you have a few animated meshes with a large number of vertices. Babylon.js 8.0 introduces the ability to pass Bounding Box calculation to the GPU, freeing up valuable cycles on the CPU and improving performance for your scene.

Check out a demo: <a href="https://aka.ms/babylon8gpuBBoxDemo">https://aka.ms/babylon8gpuBBoxDemo</a>

Learn more here: <a href="https://aka.ms/babylon8gpuBBoxDoc">https://aka.ms/babylon8gpuBBoxDoc</a>

https://youtu.be/JdKSMPf-aWg
<h3><strong>EXR Texture Support</strong></h3>
Babylon.js 8.0 also brings support for <a href="https://en.wikipedia.org/wiki/OpenEXR">EXR files</a>. This feature-rich image format unlocks some new superpowers for you to use in Babylon.js. Negative pixel values, for example, can be used to store complex visualizations as a texture format that can now be read in Babylon.js!

Check out a demo: <a href="https://aka.ms/babylon8exrDemo">https://aka.ms/babylon8exrDemo</a>

https://youtu.be/fUHEycgijb8
<h3><strong>WebXR Depth Sensing</strong></h3>
Babylon.js 8.0 brings support for a new and exciting WebXR feature called “Depth Sensing.” This feature uses depth information captured from devices to give developers the ability to overlay real world visuals “on top” of computer-generated images! You just have to see it to believe it. It feels like magic!

Check out a demo (on recent Android devices and Quest 3): <a href="https://aka.ms/babylon8webxrDSDemo">https://aka.ms/babylon8webxrDSDemo</a>

Learn more here: <a href="https://aka.ms/babylon8webxrDSDoc">https://aka.ms/babylon8webxrDSDoc</a>

https://youtu.be/fqmFjLD0xBE
<h3><strong>A Peek Ahead</strong></h3>
Phew! Babylon.js 8.0 is by far the platform’s biggest release to date. There’s so much goodness packed into it that it feels like you’ve been scrolling forever! Would you believe there’s even more in active development?!!! Here’s a tiny tease about a couple of things on the horizon:

<strong>glTF Interactivity Support</strong><strong> </strong>— If you follow the advancement of the glTF file format, then you know that exciting things are coming with the interactivity workstream. We’re actively working on supporting this incredible new extension that will allow asset behaviors to travel with your assets. That means that interactivity is no longer tied to one specific creation tool or engine!

<strong>OpenPBR Support</strong><strong> </strong>— <a href="https://academysoftwarefoundation.github.io/OpenPBR/">OpenPBR</a> is an open standard for how Physically Based Rendered materials look when rendered. Guess what’s in active development in Babylon? 😉

<strong>Tooling For Everyone</strong><strong> </strong>— We understand that everyone who comes to the Babylon platform comes from different backgrounds and experience levels, from software engineers at the top of their game, to students and artists just beginning their computer graphics journey. We are passionate about ensuring that everyone who is interested in leveraging Babylon.js has the tools and workflows to do so. We’ll have more to share on this later this year.
<h3><strong>Thank You</strong></h3>
With each evolution of Babylon.js comes a revolution in web rendering technology and an overwhelming feeling of gratitude. The Babylon platform simply wouldn’t be possible without the incredible community of developers, the 500+ contributors, and the steadfast advocates that contribute their knowledge, expertise, help and passion to this amazing technology. “Thank you” to each one of you for all that you do to help make Babylon.js one of the most powerful, beautiful, simple and open web rendering platforms in the world.]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Part 2 &#8211; Babylon.js 8.0: Audio, Gaussian Splat and physics updates</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/03/31/part-2-babylon-js-8-0-audio-gaussian-splat-and-physics-updates/</link>
		
		<dc:creator><![CDATA[Steve Clarke, Editor]]></dc:creator>
		<pubDate>Mon, 31 Mar 2025 19:59:20 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57382</guid>

					<description><![CDATA[<p>Our mission is to create one of the most powerful, beautiful and simple web rendering engines in the world. The latest Babylon.js 8.0 engine packs a ton of new improvements to help you create stunning experiences.</p>
<h3><strong>Overhauled Audio Engine<
</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/03/31/part-2-babylon-js-8-0-audio-gaussian-splat-and-physics-updates/">Part 2 &#8211; Babylon.js 8.0: Audio, Gaussian Splat and physics updates</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[Our mission is to create one of the most powerful, beautiful and simple web rendering engines in the world. The latest Babylon.js 8.0 engine packs a ton of new improvements to help you create stunning experiences.
<h3><strong>Overhauled Audio Engine</strong></h3>
Babylon’s audio engine is long overdue for a tune-up and Babylon.js 8.0 swings for the fences, bringing an entirely fresh audio engine to your ears. This new engine was designed to be <strong>powerful —</strong><strong> </strong>taking advantage of the full suite of web-audio features, <strong>modern</strong><strong> </strong>— class names and architecture you’ve come to expect, and <strong>simple-to-use</strong> — allowing anyone to leverage these features, no matter what your experience level. For those of you who care passionately about the marriage of audio and visuals to tell a compelling story, Babylon.js 8.0 was built for you!

Check out a demo: <a href="https://aka.ms/babylon8AudioEnginev2Demo">https://aka.ms/babylon8AudioEnginev2Demo</a>

Learn more here: <a href="https://aka.ms/babylon8AudioEnginev2Doc">https://aka.ms/babylon8AudioEnginev2Doc</a>

https://youtu.be/vdjBS_EfeDg
<h3><strong>Gaussian Splat Updates</strong></h3>
Babylon.js 8.0 builds on the exciting foundation of Gaussian Splat support with some exciting new updates such as SPZ and compressed PLY formats, spherical harmonics, as well as runtime optimizations for memory footprint and CPU/GPU usage.

Check out a demo for SPZ: <a href="https://aka.ms/babylon8gsplatImrovementsDemo">https://aka.ms/babylon8gsplatImrovementsDemo</a>

Learn more here: <a href="https://aka.ms/babylon8gsplatImprovementsDoc">https://aka.ms/babylon8gsplatImprovementsDoc</a>

https://youtu.be/8iMUkvsYzLU

<em>Footage from: </em><a href="https://www.basilicasanpietro.va/en/"><em>St. Peter’s Basilica</em></a><em>. A collaboration with Microsoft and Iconem for the Fabbrica di San Pietro.</em>
<h3><strong>Havok Character Controller</strong></h3>
With Babylon.js 8.0, we’ve continued our amazing partnership with the very talented team at Havok, this time bringing their fully featured character controller into Babylon.js. This brings a state-of-the-art character controller to your toolbox allowing you to start making your very own character-centered game with just a few lines of code!

Check out a demo: <a href="https://aka.ms/babylon8havokCCDemo">https://aka.ms/babylon8havokCCDemo</a>

Learn more here: <a href="https://aka.ms/babylon8havokCCDoc">https://aka.ms/babylon8havokCCDoc</a>

https://youtu.be/Bu_hAq6kGBk
<h3><strong>Smart Filters</strong></h3>
Babylon.js 8.0 builds on our rich library of node-based creation tools with the introduction of Smart Filters and the Smart Filter Editor. This new tool allows you to create video filters, texture treatments, post-processes, you name it. If it’s a 2D visual effect, this tool is for you! Under the hood it leverages shaders in the same way you’d expect any of our GPU-based tools to do, but it focuses on helping you create elaborate 2D visual treatments for web experiences. We can’t wait to see what you make with this exciting new tool!

Try it out yourself (on desktop): <a href="https://aka.ms/babylon8sfe">https://aka.ms/babylon8sfe</a>

Check out a demo (on desktop): <a href="https://aka.ms/babylon8sfeDemo">https://aka.ms/babylon8sfeDemo</a>

Learn more here: <a href="https://aka.ms/babylon8sfeDoc">https://aka.ms/babylon8sfeDoc</a>

https://youtu.be/WREWPLE6NAM
<h3><strong>Environment Improvements </strong></h3>
Babylon.js 8.0 continues to improve visual realism, leveling up the environment lighting to look closer and closer to real-time ray traced results! Another shout out to Michael Bond at Adobe for this additional great contribution.

Check out a demo: <a href="https://aka.ms/babylon8EnvImprovementsDemo">https://aka.ms/babylon8EnvImprovementsDemo</a>

Learn more here: <a href="https://aka.ms/babylon8EnvImprovementsDoc">https://aka.ms/babylon8EnvImprovementsDoc</a>

https://youtu.be/AYpnP0a9ZcE
<h3><strong>Node Geometry Editor Updates</strong></h3>
Last year, Babylon.js introduced the ability to procedurally generate geometry without writing any code through the Node Geometry Editor.

With Babylon.js 8.0, Node Geometry takes a big step up with a massive list of new features including a lattice deformer, point list, clean geometry, interceptor, an aggregator and the ability to subdivide.

Check out a demo (on desktop): <a href="https://aka.ms/babylon8ngeDemo">https://aka.ms/babylon8ngeDemo</a>

Learn more here: <a href="https://aka.ms/babylon8ngeDoc">https://aka.ms/babylon8ngeDoc</a>

https://youtu.be/ZjQ0Nu5pXdk
<h3><strong>Node Material Editor Debug Node</strong></h3>
Babylon’s Node Material Editor makes it incredibly simple to create complex visual shaders without writing any code. This artist-friendly tool bridges the gap between the complexity of building GPU shaders and the way artists think and work.

Babylon.js 8.0 introduces some exciting UI improvements as well as the new incredibly useful “visual debug node.” This node allows you to see the visual output at any point in your Node Tree. You no longer have to move the output around and hook it up to different places in your graph. It’s as simple as adding debug nodes throughout your tree to see how your shader changes throughout the computation!

Check out a demo (on desktop): <a href="https://aka.ms/babylon8nmedebugnodedemo">https://aka.ms/babylon8nmedebugnodedemo</a>

Learn more here: <a href="https://aka.ms/babylon8nmedebugnodedoc">https://aka.ms/babylon8nmedebugnodedoc</a>

https://youtu.be/vBwgLWeIERs
<h3><strong>Improved Booleans</strong></h3>
No, not those Booleans. True and False are just fine the way they are. We’re talking about Geometric Booleans!

Babylon.js 8.0 introduces support for the popular <a href="https://github.com/elalish/manifold">Manifold.js</a> library, allowing you to create new shapes with more consistent Geometric Booleans. Or more simply put, “Finally some Booleans that look the way I expect them to look!”

Check out a demo: <a href="https://aka.ms/babylon8booleanDemo">https://aka.ms/babylon8booleanDemo</a>

Learn more here: <a href="https://aka.ms/babylon8booleanDoc">https://aka.ms/babylon8booleanDoc</a>

https://youtu.be/SE5-rF8ryhQ

This list is extensive, but there's more to come! Check out the next post for updates on glTF, USDz, WebXR and more.]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Announcing Babylon.js 8.0</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/03/27/announcing-babylon-js-8-0/</link>
		
		<dc:creator><![CDATA[Steve Clarke, Editor]]></dc:creator>
		<pubDate>Thu, 27 Mar 2025 20:03:55 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57381</guid>

					<description><![CDATA[<p>Our mission is to build one of the most powerful, beautiful, simple and open web rendering engines in the world. Today, web graphics and rendering hit the accelerator with the release of Babylon.js 8.0.</p>
<p>https://youtu.be/kKaomUggipQ</p>
<p>Babylon.js 8.0 r</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/03/27/announcing-babylon-js-8-0/">Announcing Babylon.js 8.0</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[Our mission is to build one of the most powerful, beautiful, simple and open web rendering engines in the world. Today, web graphics and rendering hit the accelerator with the release of Babylon.js 8.0.

https://youtu.be/kKaomUggipQ

Babylon.js 8.0 represents a year of new features, optimizations and performance improvements aimed at helping you create more compelling, interactive web experiences faster than ever.
<h3><strong>IBL Shadows</strong></h3>
Image-Based Lighting (<a href="https://en.wikipedia.org/wiki/Image-based_lighting">IBL</a>) is a computer graphics lighting technique that approximates environment lighting based on a source image. Originating in the visual effects world as a way to blend computer-generated effects with real photography, IBL has become a ubiquitous tool for computer graphics wizards around the world.

Babylon.js first introduced support for IBL over 8 years ago and it has quickly become one of the most commonly used features of the engine.

Today, we are absolutely thrilled to announce that our good friends at Adobe leveled up IBL in Babylon by adding shadows to the mix! Yup, that’s right, now both light and shadows for the scene environment can be approximated from a source image. Special shout out to Michael Bond at Adobe for his incredible work on this!

Check out a demo: <a href="https://aka.ms/babylon8IBLShadows">https://aka.ms/babylon8IBLShadows</a>

Learn more: <a href="https://aka.ms/babylon8IBLShadowsDoc">https://aka.ms/babylon8IBLShadowsDoc</a>

https://youtu.be/xVLyy7N4Us8
<h3><strong>Area Lights</strong></h3>
We are thrilled to announce that Babylon.js 8.0 brings a frequently requested feature…Area Lights! This amazing new addition to the lighting palette allows you to specify a 2D shape that emits light from it, much like a large diffuse light that you might find on a movie set! We can’t wait to see how you use this new light type to bring a new dimension to your scene!

Check out a demo: <a href="https://aka.ms/babylon8AreaLightsDemo">https://aka.ms/babylon8AreaLightsDemo</a>

Learn more here: <a href="https://aka.ms/babylon8AreaLightsDoc">https://aka.ms/babylon8AreaLightsDoc</a>

https://youtu.be/U1FdTyTlJsU
<h3><strong>Node Render</strong><strong> Graph - Alpha</strong></h3>
One of the most powerful new features in Babylon.js 8.0 is something we call the “Node Render Graph.”

Up to now, the specific rendering pipeline for Babylon has been a black box. You tell Babylon what to render and it goes off and does it for you. There have been observables that allow you to manipulate the result after completion of a render, but the render process itself has been opaque. Well…not anymore!

With Babylon.js 8.0 you now have full control of the entire render pipeline. This means that you can fully customize and control every part of the process of how your frames are rendered on the GPU. And if that wasn’t enough, you also now have a fancy new Node Graph tool to allow you to customize your own render pipeline, without writing complex render process code! Please note that the Node Render Graph is in Alpha for you to test and discover but should not be used in production yet as it is subject to change.

Try out the editor (on desktop): <a href="https://nrge.babylonjs.com/">Babylon.js Node Render Graph Editor</a>

Check out a demo (on desktop): <a href="https://aka.ms/babylon8RenderGraphDemo">https://aka.ms/babylon8RenderGraphDemo</a>

Learn more here: <a href="https://aka.ms/babylon8RenderGraphDoc">https://aka.ms/babylon8RenderGraphDoc</a>

https://youtu.be/emoIox2sru0
<h3><strong>All New Lightweight Viewer</strong></h3>
Babylon.js is a powerful tool used by tens of thousands of people and organizations across the globe to bring complex visual ideas to life on the web. Babylon.js 8.0 unlocks a new super-powered tool for the other end of the spectrum…those scenarios where you want to display a simple 3D object on a web page with zero complexity, and stunning visuals in a tiny package.

Introducing the all new Babylon.js Lightweight Viewer. This new viewer is designed to harness the same rendering beauty and power of the full engine but comes in a smaller bundle footprint and uses dynamic imports and capabilities (audio or animation for example) depending on the model that is loaded. It can be added to any web page with just a few lines of HTML and is also fully extensible!

Of course, this new Lightweight Viewer wouldn’t be complete without a super easy-to-use configurator along with it! The Viewer Configurator is a simple tool that allows you to customize the Viewer to your exact specifications and give you the simple .html properties to set so it looks the same in your website!

Play with the Configurator (on desktop): <a href="https://aka.ms/babylon8ViewerConfig">https://aka.ms/babylon8ViewerConfig</a>

Check it out: <a href="https://aka.ms/babylon8viewerHome">https://aka.ms/babylon8viewerHome</a>

Learn more here: <a href="https://aka.ms/babylon8viewerDoc">https://aka.ms/babylon8viewerDoc</a>

https://youtu.be/HmkkGE96Xb4
<h3><strong>WGSL Core Engine</strong><strong> Shaders</strong></h3>
Babylon.js has had support for WebGPU since its inception. The core engine shaders in Babylon.js, however, have been written in GLSL (WebGL shading language) from the beginning. Because WebGPU has its own shading language (WGSL) this posed a very interesting challenge. How do you get GLSL shaders to render in WebGPU? Fortunately, there is a conversion library that’s been available. So, anyone wanting to target WebGPU with Babylon can leverage this library to convert the Babylon shaders into something WebGPU can use. The downside of this is that this conversion library is over 3MB, requiring users to double their download size for a standardBabylon.js project.

With Babylon 8.0, this problem is a thing of the past. All of the core engine shaders for Babylon.js are now available in both GLSL and WGSL. This means direct support for WebGPU right out of the box with no conversion layer, essentially making Babylon.js 2x smaller when targeting WebGPU than in the past!

Check out a demo: <a href="https://aka.ms/babylon8WGSLDemo">https://aka.ms/babylon8WGSLDemo</a> (try switching between WebGL2 and WebGPU)

Learn more here: <a href="https://aka.ms/babylon8WGSLDoc">https://aka.ms/babylon8WGSLDoc</a>

https://youtu.be/pKyGxT5xyUs
<h3><strong>NME -> WGSL</strong><strong> Support</strong></h3>
Well, why stop at core engine shaders? Why not unlock the ability to create custom WGSL shaders using Babylon’s <a href="https://doc.babylonjs.com/toolsAndResources/nme/">Node Material Editor</a> as well!

OK! Check!

Ability Unlocked! Let’s go Babylon 8.0!!!!!

Check out a demo (on desktop): <a href="https://aka.ms/babylon8nmeWGSL">https://aka.ms/babylon8nmeWGSL</a>

Learn more here: <a href="https://aka.ms/babylon8nmeWGSLDoc">https://aka.ms/babylon8nmeWGSLDoc</a>

https://youtu.be/IAKi7krpUgA

Those are just some of the main features of Babylon.js 8.0, there is much more! Tune in for next posts and learn more about Audio, Gaussian Splat and physics advancements, ….]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>AI Toolkit for Visual Studio Code now supports NVIDIA NIM microservices for RTX AI PCs</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/03/25/ai-toolkit-for-visual-studio-code-now-supports-nvidia-nim-microservices-for-rtx-ai-pcs/</link>
		
		<dc:creator><![CDATA[Anna Soracco]]></dc:creator>
		<pubDate>Tue, 25 Mar 2025 13:03:15 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57390</guid>

					<description><![CDATA[<p><strong><em>Editor’s note – March 26, 2025 –</em></strong><em> The screenshot was updated to reflect relevant models.</em></p>
<p><a href="https://aka.ms/aitoolkit">AI Toolkit</a> now supports <a href="https://developer.nvidia.com/nim">NVIDIA NIM mic</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/03/25/ai-toolkit-for-visual-studio-code-now-supports-nvidia-nim-microservices-for-rtx-ai-pcs/">AI Toolkit for Visual Studio Code now supports NVIDIA NIM microservices for RTX AI PCs</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<strong><em>Editor’s note – March 26, 2025 –</em></strong><em> The screenshot was updated to reflect relevant models.</em>

<a href="https://aka.ms/aitoolkit">AI Toolkit</a> now supports <a href="https://developer.nvidia.com/nim">NVIDIA NIM microservice</a>-based foundation models for inference testing in the model playground! Explore these NIMs in AI Toolkit's model catalog today!

[caption id="attachment_57393" align="alignnone" width="1024"]<img class="wp-image-57393 size-large" src="https://pub-d00f534024b04d0e8036586fc78a41fa.r2.dev/sites/3/2025/03/blog_post_update1920-1024x578.png" alt="Editor’s note – March 25, 2025 – The screenshot was updated to reflect relevant models" width="1024" height="578" /> AI Toolkit now supports NVIDIA NIM for a unified development environment[/caption]

Read more about it on the Tech Community Blog at <a href="https://aka.ms/aitk/nimsblog">aka.ms/aitk/nimsblog</a>.]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Available today: DeepSeek R1 7B &#038; 14B distilled models for Copilot+ PCs via Azure AI Foundry – further expanding AI on the edge</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/03/03/available-today-deepseek-r1-7b-14b-distilled-models-for-copilot-pcs-via-azure-ai-foundry-further-expanding-ai-on-the-edge/</link>
		
		<dc:creator><![CDATA[Vivek Pradeep]]></dc:creator>
		<pubDate>Mon, 03 Mar 2025 19:53:09 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57375</guid>

					<description><![CDATA[<p>At Microsoft, we believe the future of AI is happening now — spanning from the cloud to the edge. Our vision is bold: to build Windows as the ultimate platform for AI innovation, where intelligence isn’t just in the cloud but seamlessly woven t</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/03/03/available-today-deepseek-r1-7b-14b-distilled-models-for-copilot-pcs-via-azure-ai-foundry-further-expanding-ai-on-the-edge/">Available today: DeepSeek R1 7B &#038; 14B distilled models for Copilot+ PCs via Azure AI Foundry – further expanding AI on the edge</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[At Microsoft, we believe the future of AI is happening now — spanning from the cloud to the edge. Our vision is bold: to build Windows as the ultimate platform for AI innovation, where intelligence isn’t just in the cloud but seamlessly woven throughout the system, silicon and hardware at the edge. Building on our <a href="https://blogs.windows.com/windowsdeveloper/2025/01/29/running-distilled-deepseek-r1-models-locally-on-copilot-pcs-powered-by-windows-copilot-runtime/">recent announcement</a> of bringing NPU-optimized versions of DeepSeek-R1 1.5B distilled model directly to Copilot+ PCs, we’re taking the next step forward with the availability of DeepSeek R1 7B &amp; 14B distilled models for Copilot+ PCs via Azure AI Foundry. This milestone reinforces our commitment to delivering cutting-edge AI capabilities that are fast, efficient and built for real-world applications — helping developers, businesses and creators push the boundaries of what’s possible.

https://www.youtube.com/watch?v=GotHKdBQPw4

Availability starts with Copilot+ PCs powered by Qualcomm Snapdragon X, followed by Intel Core Ultra 200V and AMD Ryzen.

The ability to run 7B and 14B parameter reasoning models on <a href="https://support.microsoft.com/en-us/windows/all-about-neural-processing-units-npus-e77a5637-7705-4915-96c8-0c6a975f9db4">Neural Processing Units (NPUs)</a> is a significant milestone in the democratization and accessibility of artificial intelligence. This progression allows researchers, developers and enthusiasts to leverage the substantial power and functionalities of large-scale machine learning models directly from their Copilot+ PCs. These Copilot+ PCs include an NPU capable of over 40 trillion operations per second (TOPS).
<h3><strong>NPUs are purpose-built to run AI models locally on-device with exceptional efficiency </strong></h3>
NPUs like those built into Copilot+ PCs are purpose-built to run AI models with exceptional efficiency, balancing speed and power consumption. They ensure sustained AI compute with minimal impact on battery life, thermal performance and resource usage. This leaves CPUs and GPUs free to perform other tasks, allowing reasoning models to operate longer and deliver superior results — all while keeping your PC running smoothly.

Efficient inferencing has heightened significance due to a new scaling law for language models, which indicates that chain of thought reasoning during inference can improve response quality across various tasks. The longer a model can “think,” the better its quality will be. Instead of increasing parameters or training data, this approach taps into additional computational power for better outcomes. DeepSeek distilled models exemplify how even small pretrained models can shine with enhanced reasoning capabilities and when coupled with the NPUs on <a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/">Copilot+ PCs</a>, they unlock exciting new opportunities for innovation.

Reasoning emerges in models of a certain minimum scale, and models at that scale must think using a large number of tokens to excel at complex multi-step reasoning. Although the NPU hardware aids in reducing inference costs, it is equally important to maintain a manageable memory footprint for these models on consumer PCs, say with 16GB RAM.
<h3><strong>Pushing the boundaries of what’s possible on Windows</strong></h3>
Our research investments have enabled us to push the boundaries of what’s possible on Windows even further at the system level and at a model level leading to innovations like Phi Silica. <a href="https://blogs.windows.com/windowsexperience/2024/12/06/phi-silica-small-but-mighty-on-device-slm/">With our work on Phi Silica</a> we were able to create a scalable platform for low-bit inference on NPUs, enabling powerful performance with minimal memory and bandwidth tax. Combined with the data privacy offered by local compute, this puts advanced scenarios like Retrieval Augmented Generation (RAG) and model fine-tuning at the fingertips of application developers.

We reused techniques such as <a href="https://arxiv.org/abs/2404.00456">QuaRot</a>, sliding window for fast first token responses and many other optimizations to enable the DeepSeek 1.5B release. We used Aqua, an internal automatic quantization tool, to quantize all the DeepSeek model variants to int4 weights with QuaRot, while retaining most of the accuracy. Using the same toolchain we used to optimize Phi Silica we quickly integrated all the optimizations into an efficient <a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html">ONNX QdQ</a> model with low precision weights.

Like the 1.5B model, the 7B and 14B variants use 4-bit block wise quantization for the embeddings and language model head and run these memory-access heavy operations on the CPU. The compute-heavy transformer block containing the context processing and token iteration uses int4 per-channel quantization for the weights alongside int16 activations. We already see about 8 tok/sec on the 14B model (the 1.5B model, being very small, demonstrated close to 40 tok/sec) — and further optimizations are coming in as we leverage more <a href="https://arxiv.org/abs/2402.17764">advanced techniques</a>. With all this in place, these nimble language models think longer and harder.

This durable path to innovation has made it possible for us to more quickly optimize larger variants of DeepSeek models (7B and 14B) and will continue to enable us to bring more new models to run on Windows efficiently.
<h3><strong>Get started today</strong></h3>
Developers can access all distilled variants (1.5B, 7B and 14B) of DeepSeek models and run them on Copilot+ PCs by simply downloading the <a href="https://learn.microsoft.com/en-us/windows/ai/toolkit/toolkit-getting-started?utm_source=chatgpt.com&amp;tabs=rest">AI Toolkit VS Code extension</a>. The DeepSeek model optimized in the ONNX QDQ format is available in AI Toolkit’s model catalog, pulled directly from Azure AI Foundry. You can download it locally by clicking the “Download” button. Once downloaded, experimenting with the model is as simple as opening the Playground, loading the “deepseek_r1_1_5” model and sending it prompts.
<h3><strong>Run models across Copilot+ PCs and Azure</strong></h3>
Copilot+ PCs offer local compute capabilities that are an extension of capabilities enabled by Azure, giving developers even more flexibility to train, fine-tune small language models on-device and leverage the cloud for larger intensive workloads. In addition to the ONNX model optimized for Copilot+ PC, you can also try the cloud-hosted source model in Azure Foundry by clicking on the “Try in Playground” button under “DeepSeek R1.” AI Toolkit is part of your developer workflow as you experiment with models and get them ready for deployment. With this playground, you can effortlessly test the DeepSeek models available in Azure AI Foundry for local deployment too. Through this, developers now have access to the most complete set of DeepSeek models available through the Azure AI Foundry from cloud to client.

Copilot+ PCs pair efficient compute with the near infinite compute Microsoft has to offer via its Azure services. With reasoning able to span the cloud and the edge, running in sustained loops on the PC and invoking the much larger brains in the cloud as needed — we are on to a new paradigm of continuous compute creating value for our customers. The future of AI compute just got brighter! We can’t wait to see the new innovations from our developer community taking advantage of these rich capabilities. Please keep the feedback coming!]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Discover magical AI experiences through the Microsoft Store on Windows, with the new AI Hub</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/02/18/discover-magical-ai-experiences-through-the-microsoft-store-on-windows-with-the-new-ai-hub/</link>
		
		<dc:creator><![CDATA[Steve Clarke, Editor]]></dc:creator>
		<pubDate>Tue, 18 Feb 2025 17:00:01 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57340</guid>

					<description><![CDATA[<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/02/18/discover-magical-ai-experiences-through-the-microsoft-store-on-windows-with-the-new-ai-hub/">Discover magical AI experiences through the Microsoft Store on Windows, with the new AI Hub</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[The Microsoft Store on Windows is the best place to discover AI experiences for your PC from Microsoft and our rich developer ecosystem. In 2023, we launched the AI Hub in Store to help our users find great AI-powered apps. In the last two years, we’ve seen hundreds of new AI-capable apps join the Microsoft Store community and we’ve partnered with many to showcase their content in the AI Hub. Developers like <a href="https://apps.microsoft.com/detail/XPDDXX9QW8N9D7?hl=en-US&gl=US&ocid=pdpshare">Grammarly</a> have noted that “through this partnership, we saw a meaningful lift in key metrics, including a 245% increase in page views, a 13% growth in installs and a 24% rise in new active users. The AI Hub spotlight helped us connect with more users seeking AI assistance for communication and productivity, reinforcing the value of Grammarly for Windows.”

<strong>Today, we’re excited to announce the next iteration of AI Hub<sup>1</sup>.</strong> We have reimagined it to be the destination for users to learn about the AI experiences possible on their devices. For users on Copilot+ PCs, we’ll showcase the Windows and developer ecosystem experiences that leverage the power of their PC. For users not yet on Copilot+ PCs, we’ll showcase the best apps and experiences compatible with their devices. And we are proud to do this with a brand-new product design – you’ll find beautiful new visuals and snappy flows that help make your browsing experience inspiring.
<h3>AI experiences powered by Microsoft</h3>
https://youtu.be/A63BCh-qs-E

When you’re browsing the new AI Hub on your Copilot+ PC, you’ll find rich informational content on the AI experiences powered by your PC’s <a href="https://support.microsoft.com/en-us/windows/all-about-neural-processing-units-npus-e77a5637-7705-4915-96c8-0c6a975f9db4">Neural Processing Unit</a> (NPU). The NPU plays a key role in handling tasks related to AI and machine learning. It is designed to speed up complex processes such as facial recognition, voice assistance and data analysis, delivering advanced computation with exceptional battery efficiency. The NPU’s ability to offload these tasks from the CPU and GPU allows for faster, more efficient operation of the entire system.

As you become more familiar with the capabilities of your Copilot+ PC, AI Hub is here to help! We’ve created a new Welcome Experience to help you familiarize yourself with exclusive Windows features, ready for you to use – to start, these include <a href="https://support.microsoft.com/en-us/windows/windows-studio-effects-273c1fa8-2b3f-41b1-a587-7cc7a24b62d8">Windows Studio Effects</a>, <a href="https://support.microsoft.com/en-us/windows/use-copilot-pc-features-in-paint-53857513-e36c-472d-8d4a-adbcd14b2e54">Paint Cocreator</a> and <a href="https://support.microsoft.com/en-us/windows/microsoft-photos-restyle-image-and-image-creator-responsible-ai-faq-6c352e99-d954-49c9-84cd-b7cacd018868">Photos Image Creator</a>. And when you’re ready to dive into a specific workflow, we’ll help you get started.

Your AI Hub will stay current to include details on the latest Copilot+ PC features and NPU-powered apps available. We're excited about helping customers connect to upcoming Windows features when they're generally available, including Recall, Click To Do and Improved Windows Search<sup>2</sup>.

https://youtu.be/3ptccBogS18

For users browsing AI Hub on Windows 11 PCs, you’ll find informational content available on the AI-powered apps that work on your device. Microsoft apps like <a href="https://apps.microsoft.com/detail/9nht9rb2f4hd"><strong>Copilot</strong></a>, <a href="https://apps.microsoft.com/detail/9pjgrcldlx5v"><strong>Designer</strong></a>, <a href="https://apps.microsoft.com/detail/9p1j8s7ccwwt?hl"><strong>Clipchamp</strong></a> and <a href="https://apps.microsoft.com/detail/9n0dgwh9pszf"><strong>Reading Coach</strong></a> all have AI functionalities ready for you to dive into.
<h3>Our rich AI developer ecosystem</h3>
<img class="alignnone wp-image-57371 size-large" src="https://pub-d00f534024b04d0e8036586fc78a41fa.r2.dev/sites/3/2025/02/Rich-Apps-1024x576.png" alt="Decorative image with app icons" width="1024" height="576" />

No matter what PC you’re using, we have a rich AI developer ecosystem available inside the Microsoft Store on Windows – and the AI Hub is a wonderful destination for you to browse content. Some examples include:
<ul>
 	<li>For Copilot+ PC users who are passionate about music creation, <a href="https://apps.microsoft.com/detail/9p9rb7zf49xk?ocid=storeaward24"><strong>djay Pro</strong></a> features "NeuralMix," which leverages the extra processing power of the NPU to isolate vocals or instruments when re-mixing songs. On Copilot+ PCs, NeuralMix can use an AI model that is twice as large and more complex than on other devices, resulting in better sound quality and cleaner music stem separation. djay Pro was selected as a <a href="https://blogs.windows.com/windowsdeveloper/2024/11/01/announcing-the-microsoft-store-awards-2024-winners/">2024 Microsoft Store Awards winner</a> in the category of AI apps!</li>
</ul>
<ul>
 	<li>For users on Copilot+ PCs who require a stand-out video presence, from sales professionals to streamers, <a href="https://apps.microsoft.com/detail/9pgm3qb3pdrd?launch=true&mode=full&hl"><strong>Camo Studio</strong></a> uses the NPU to power real-time visual effects that ensure you look your best. The NPU powers several machine learning workloads (such as real-time background segmentation and relighting) where latency is critical – and with incredible efficiency that ensures minimal impact to battery life.</li>
</ul>
<ul>
 	<li><a href="https://apps.microsoft.com/detail/9mv4lscdrflt?ocid=storeaward24"><strong>Gamma</strong></a><strong> </strong>stands out for its ability to offer users AI-powered presentation creation, a user-friendly interface and versatile formats. Its intuitive design, real-time collaboration features and interactive elements make it a favorite among businesses. And Gamma was also selected as a <a href="https://blogs.windows.com/windowsdeveloper/2024/11/01/announcing-the-microsoft-store-awards-2024-winners/">2024 Microsoft Store Awards winner</a> in the category of Business apps!</li>
</ul>
<ul>
 	<li>And for those who love to converse with assistants, <a href="https://apps.microsoft.com/detail/9nt1r1c2hh7j?hl"><strong>ChatGPT</strong></a> now is available as an app for all Windows devices exclusively through the Microsoft Store. Easily ask questions, get writing assistance and generate ideas – all in a seamless, user-friendly experience.</li>
</ul>
Visit AI Hub regularly to see the latest AI-enabled products from our amazing partners.
<h3>A preview of what’s ahead</h3>
Today, this experience is available for Copilot+ PCs in United States, Canada, United Kingdom and Australia. In the coming months, we will expand it to additional markets and for the broader Windows population.

To stay current on release plans, please watch the “What’s New” page in Store to see our latest release information. And, <a href="https://blogs.windows.com/windowsdeveloper/2024/12/03/raising-the-bar-updates-to-the-microsoft-store-on-windows/">as we shared in December</a>, we are keen on your feedback – please submit via Feedback Hub (WIN + F) under Microsoft Store.

AI is changing the way we do everything on our PCs – from tackling complex research projects, to finding new recipes, editing photos and everything in between. We believe AI innovation isn’t just about creating new products; it’s about enabling us to reimagine how we solve everyday problems.

Happy browsing!

<em><sup>1 </sup>AI Hub is available in select markets within Microsoft Store on Windows. </em>

<em><sup>2 </sup>Windows features will be added to the AI Hub as they become available to retail users by market. Features in the Windows Insider Program will not be included. </em>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Running Distilled DeepSeek R1 models locally on Copilot+ PCs, powered by Windows Copilot Runtime</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/01/29/running-distilled-deepseek-r1-models-locally-on-copilot-pcs-powered-by-windows-copilot-runtime/</link>
		
		<dc:creator><![CDATA[Vivek Pradeep]]></dc:creator>
		<pubDate>Wed, 29 Jan 2025 22:11:14 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57331</guid>

					<description><![CDATA[<p><em><strong>Update: Feb. 3, 2025:</strong> Today, we are pleased to announce that the distilled DeepSeek R1 models optimized using ONNX are now available to use on your Snapdragon powered Copilot+ PCs. With further optimizations in place, the model i</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/01/29/running-distilled-deepseek-r1-models-locally-on-copilot-pcs-powered-by-windows-copilot-runtime/">Running Distilled DeepSeek R1 models locally on Copilot+ PCs, powered by Windows Copilot Runtime</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<em><strong>Update: Feb. 3, 2025:</strong> Today, we are pleased to announce that the distilled DeepSeek R1 models optimized using ONNX are now available to use on your Snapdragon powered Copilot+ PCs. With further optimizations in place, the model is capable of a time to first token of less than 70 ms for short prompts (&lt;64 tokens) and a throughput rate of up to ~40 tokens/s. The time to first token scales with the length of the input prompt. The throughput rate varies based on the complexity of the task specified in the prompt; responses exhibit a throughput range of ~25-40 tokens/s. Longer responses are especially likely to enjoy higher throughput rates. <a href="https://learn.microsoft.com/en-us/windows/ai/toolkit/toolkit-getting-started?utm_source=chatgpt.com&amp;tabs=rest">Get started today by downloading the AI Toolkit extension in VS Code</a>.</em>

AI is moving closer to the edge, and Copilot+ PCs are leading the way. With the <a href="https://azure.microsoft.com/en-us/blog/?p=38333">availability of cloud hosted DeepSeek R1 available on Azure AI Foundry</a>, we’re bringing NPU-optimized versions of DeepSeek-R1 directly to Copilot+ PCs, starting with Qualcomm Snapdragon X first, followed by Intel Core Ultra 200V and others. The first release, DeepSeek-R1-Distill-Qwen-1.5B (<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B">Source</a>), will be available in AIToolkit for VSCode, with the 7B (<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B">Source</a>) and 14B (<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B">Source</a>) variants arriving soon. These optimized models let developers build and deploy AI-powered applications that run efficiently on-device, taking full advantage of the powerful NPUs in Copilot+ PCs.

The <a href="https://support.microsoft.com/en-us/windows/all-about-neural-processing-units-npus-e77a5637-7705-4915-96c8-0c6a975f9db4">Neural Processing Unit (NPU)</a> on Copilot+ PCs offers a highly efficient engine for model inferencing, unlocking a paradigm where generative AI can execute not just when invoked, but enable semi-continuously running services. This empowers developers to tap into powerful reasoning engines to build proactive and sustained experiences.

<a href="https://blogs.windows.com/windowsexperience/2024/12/06/phi-silica-small-but-mighty-on-device-slm/">With our work on Phi Silica</a>, we were able to harness highly efficient inferencing – delivering very competitive time to first token and throughput rates, while minimally impacting battery life and consumption of PC resources. Running models on the NPU is about speed and efficiency. For example, as mentioned in previous posts, the Phi Silica token iterator on the NPU exhibits a 56% improvement in power consumption compared to operating on the CPU. Such efficiency enables new experiences that demand such state-of-the art models to be in the main loop of the program without draining your battery or overly heating your device. The optimized DeepSeek models for the NPU take advantage of several of the key learnings and techniques from that effort, including how we separate out the various parts of the model to drive the best tradeoffs between performance and efficiency, low bit rate quantization and mapping transformers to the NPU. Additionally, we take advantage of Windows Copilot Runtime (WCR) to scale across the diverse Windows ecosystem with <a href="https://onnxruntime.ai/">ONNX</a> QDQ format.

<strong>Get ready to play!</strong>

First things first…let’s give it a whirl.

To see DeepSeek in action on your Copilot+ PC, simply download the AI Toolkit VS Code extension. The DeepSeek model optimized in the ONNX QDQ format is available in AI Toolkit’s model catalog, pulled directly from Azure AI Foundry. You can download it locally by clicking the “Download” button. Once downloaded, experimenting with the model is as simple as opening the Playground, loading the “ deepseek_r1_1_5” model, and sending it prompts.

In addition to the ONNX model optimized for Copilot+ PC, you can also try the cloud-hosted source model in Azure Foundry by clicking on the “Try in Playground” button under “ DeepSeek R1”.

AI Toolkit is part of your developer workflow as you experiment with models and get them ready for deployment. With this playground, you can effortlessly test the DeepSeek models available in Azure AI Foundry for local deployment.

https://youtu.be/CFzH0sekxYI

<strong>Silicon Optimizations </strong>

The distilled Qwen 1.5B consists of a tokenizer, embedding layer, a context processing model, token iteration model, a language model head and a de-tokenizer. We use 4-bit block wise quantization for the embeddings and language model head and run these memory-access heavy operations on the CPU. We focus the bulk of our NPU optimization efforts on the compute-heavy transformer block containing the context processing and token iteration, wherein we employ int4 per-channel quantization for the weights alongside int16 activations.

Details of the various precisions involved are in the table below, for additional clarity on the mix.
<table style="margin-left: auto !important; margin-right: auto !important; display: block;">
<tbody>
<tr>
<td width="207"><strong>Model</strong></td>
<td width="207"><strong>Precision</strong></td>
<td width="207"><strong>Host</strong></td>
</tr>
<tr>
<td width="207">Embeddings</td>
<td width="207">w: int4 a: fp32</td>
<td width="207">CPU</td>
</tr>
<tr>
<td width="207">Context processing</td>
<td width="207">w: int4 a: int16</td>
<td width="207">NPU</td>
</tr>
<tr>
<td width="207">Token iteration</td>
<td width="207">w: int4 a: int16</td>
<td width="207">NPU</td>
</tr>
<tr>
<td width="207">Language model head</td>
<td width="207">w: int4 a: fp32</td>
<td width="207">CPU</td>
</tr>
</tbody>
</table>
While the Qwen 1.5B release from DeepSeek does have an int4 variant, it does not directly map to the NPU due to presence of dynamic input shapes and behavior – all of which needed optimizations to make compatible and extract the best efficiency. Additionally, we use the <a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html">ONNX QDQ</a> format to enable scaling across a variety of NPUs we have in the Windows ecosystem. We work out an optimal operator layout between the CPU and NPU for maximum power-efficiency and speed.

To achieve the dual goals of low memory footprint and fast inference, much like Phi Silica, we make two key changes: First, we leverage a sliding window design that unlocks super-fast time to first token and long context support despite not having dynamic tensor support in the hardware stack. Second, we use the 4-bit <a href="https://www.microsoft.com/en-us/research/publication/quarot-outlier-free-4-bit-inference-in-rotated-llms/">QuaRot</a> quantization scheme to truly take advantage of low bit processing. QuaRot employs Hadamard rotations to remove outliers in weights and activations, making the model easier to quantize. QuaRot significantly improves quantization accuracy, compared to existing methods, such as GPTQ, particularly for low granularity settings such as per-channel quantization. The combination of low-bit quantization and hardware optimizations such the sliding window design help deliver the behavior of a larger model within the memory footprint of a compact model. With these optimizations in place, the model is capable of a time to first token of 130 ms and a throughput rate of 16 tokens/s for short prompts (&lt;64 tokens).

We include examples of the original and quantized model responses below to show the minor differences between the two variants, with the latter being both fast and power-efficient:

<img class="alignnone wp-image-57336 size-large" src="https://pub-d00f534024b04d0e8036586fc78a41fa.r2.dev/sites/3/2025/01/originalleft_deepseekr1_1.5-1024x592.png" alt="Sample response from the original model" width="1024" height="592" />

<img class="alignnone wp-image-57338 size-large" src="https://pub-d00f534024b04d0e8036586fc78a41fa.r2.dev/sites/3/2025/01/quantizedright_deepseekr1_1.5-2-1024x587.png" alt="Sample response from the NPU-optimized model" width="1024" height="587" />

<em>Figure 1: Qualitative comparison. Sample responses from the original model (top) vs NPU-optimized model (bottom) for the same prompt, including the model’s reasoning capability. The model follows a similar reasoning pattern, and reaches the same answer, demonstrating that the optimized model retains the reasoning ability of the original model. </em>

With the speed and power characteristics of the NPU-optimized version of the DeepSeek R1 models users will be able to interact with these ground-breaking models entirely locally. We are excited what this capability enables for the future of the PC experience and looking forward towards innovations from our developer community.]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Raising the bar: Updates to the Microsoft Store on Windows</title>
		<link>https://blogs.windows.com/windowsdeveloper/2024/12/03/raising-the-bar-updates-to-the-microsoft-store-on-windows/</link>
		
		<dc:creator><![CDATA[Steve Clarke, Editor]]></dc:creator>
		<pubDate>Tue, 03 Dec 2024 17:00:17 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57174</guid>

					<description><![CDATA[<p>Your feedback matters – each week, we get thousands of feedback submissions that we triage, categorize and prioritize. We love the enthusiasm our customers and developers show daily for the Store and all its apps and games – and as product makers</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2024/12/03/raising-the-bar-updates-to-the-microsoft-store-on-windows/">Raising the bar: Updates to the Microsoft Store on Windows</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[Your feedback matters – each week, we get thousands of feedback submissions that we triage, categorize and prioritize. We love the enthusiasm our customers and developers show daily for the Store and all its apps and games – and as product makers, we take that energy forward every day into our craft. Our ultimate goal is to build the best product for <em>you</em>.

Today, we’re excited to recap some important quality updates to the Microsoft Store on Windows that we’ve rolled out over the last few months.
<h3>Upping the ante on performance</h3>
<img class="alignnone wp-image-57179 size-large" src="https://blogs.windows.com/wp-content/uploads/prod/sites/3/2024/11/performance-1024x683.png" alt="Decorative image of anthropomorphic Store logo running" width="1024" height="683" />

We have some meaningful improvements to share from our recent work on performance. Overall Store launch time has been reduced by 25%, and we've reduced the number of download hanging issues by 50%<sup>1</sup>. To make sure you see the latest improvements, ensure you have the latest Windows update. We are continually striving for improvements in this space because we know how influential it is to your Store experience.

<img class="alignnone wp-image-57180 size-large" src="https://blogs.windows.com/wp-content/uploads/prod/sites/3/2024/11/New-download-indicator-1024x576.png" alt="Install button showing progress bar, percentage and number of MB downloaded from the total" width="1024" height="576" />

<strong>New download and install progress indicator</strong> – We’ve rebuilt the way we’re communicating download and install progress to you when you’re getting content from a Product Page. This is especially important when you’re downloading larger apps. You also can cancel an operation right from a Product Page if you’ve accidentally clicked download.
<h3>Improved browse and Product Page experiences</h3>
We’ve revamped a few places where we know customers go to find and acquire content.

<img class="alignnone wp-image-57182 size-large" src="https://blogs.windows.com/wp-content/uploads/prod/sites/3/2024/11/App-categories-1024x683.png" alt="A row of buttons on the gaming page are highlighted " width="1024" height="683" /><strong>Apps and games categories </strong>– We’ve introduced new categorizations of apps and games to help you narrow down what you’re looking for. From the Gaming page, you can now navigate easily and quickly through different game categories such as strategy, role-playing, puzzle and simulation. For Apps, we are experimenting with different categories so please keep an eye out for the experience and give us feedback!

<img class="alignnone wp-image-57181 size-large" src="https://blogs.windows.com/wp-content/uploads/prod/sites/3/2024/11/New-PDP-1024x576.png" alt="Product Page interface with large product screenshots and text overlaying trailer background " width="1024" height="576" />

<strong>Immersive Product Pages </strong>– We’re introducing a new feature for Product Pages that adds an immersive experience to the top of the page. For apps and games that provide a trailer, you’ll be able to enjoy it as you navigate to their various pages. Meanwhile, products without trailers will use beautiful hero images provided by the developers. There’s also an option in the Store settings page to toggle video autoplay on or off.
<h3>New Library, Updates and downloads pages</h3>
We know this one’s a crowd favorite – we’ve introduced two new pages, Library, and Updates and downloads, to help you find and manage all your content.

<img class="alignnone wp-image-57183 size-large" src="https://blogs.windows.com/wp-content/uploads/prod/sites/3/2024/11/Library-1024x683.png" alt="Search button on top right corner of Library page highlighted" width="1024" height="683" />

<strong>Redesigned Library page</strong> – We made two big changes here. First, we changed the default filters to show all the products you own, not just products that are installed (you can still toggle this filter if you wish). Second, we’ve added a search bar that will help you find the name or the publisher of products you’re looking for.

<img class="alignnone wp-image-57184 size-large" src="https://blogs.windows.com/wp-content/uploads/prod/sites/3/2024/11/Downloads-1024x576.png" alt="Downloads page showing an app with developer notes in progress of updating" width="1024" height="576" />

<strong>New Updates and downloads page</strong> – We are also moving the list of updates and downloads from the top section of the Library to its own dedicated page. This new page displays your list of active downloads or pending updates, and it shows the list of recent installs or updates. Version notes will also be displayed for a pending update or active download.
<h3>But wait…there’s more!</h3>
There’s a long list of other improvements we’ve quietly rolled out, such as Win32 apps support for our Store Web Installer on web, design updates, search recommendations, and easier ways to update Win32 apps from the Downloads and Product pages (this last feature is accessible via Windows Insiders starting Dec. 4). Lastly, our catalog continues to grow – we recently welcomed <a href="https://www.microsoft.com/store/productId/9NT1R1C2HH7J">ChatGPT</a>, <a href="https://apps.microsoft.com/detail/9p0kn9s5rp86">Fantastical</a>, <a href="https://apps.microsoft.com/store/detail/XPDM5VSMTKQLBJ">Battle.net</a>, <a href="https://apps.microsoft.com/store/detail/XP9CKPHR0JP7F7">World of Warcraft</a>, <a href="https://apps.microsoft.com/store/detail/XPFMDW72VHTTX9">Arc</a> and more!  We also recently announced our <a href="https://blogs.windows.com/windowsdeveloper/2024/11/01/announcing-the-microsoft-store-awards-2024-winners/">2024 Store Awards</a>, and encourage you to check out the winning apps and games to find new AI experiences, gaming content, productivity tools and more!

We hope these improvements spark a bit more joy as you’re using the Store on Windows. Please submit your product feedback in Feedback Hub (WIN + F) under Microsoft Store. And we are always publishing new updates, so leverage the “What’s New” page to keep track of the latest.

We have a lot more fun in store for you in 2025 – stay tuned!

<sup>1</sup> Data based on internal testing and subject to factors such as device, location, Windows and Store app versions.]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>DirectML unlocks new silicon for AI experiences across Windows Copilot+ PCs</title>
		<link>https://blogs.windows.com/windowsdeveloper/2024/11/12/directml-unlocks-new-silicon-for-ai-experiences-across-windows-copilot-pcs/</link>
		
		<dc:creator><![CDATA[]]></dc:creator>
		<pubDate>Tue, 12 Nov 2024 18:00:21 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57163</guid>

					<description><![CDATA[<p>October was an exciting month of amazing showcases and announcements for applications leveraging the power of the NPU on the new Copilot+ PCs to deploy their AI innovations on Windows. Windows’ diverse hardware ecosystem empowers developers with a </p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2024/11/12/directml-unlocks-new-silicon-for-ai-experiences-across-windows-copilot-pcs/">DirectML unlocks new silicon for AI experiences across Windows Copilot+ PCs</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[October was an exciting month of amazing showcases and announcements for applications leveraging the power of the NPU on the new Copilot+ PCs to deploy their AI innovations on Windows. Windows’ diverse hardware ecosystem empowers developers with a range of options at their fingertips to develop and deploy AI. From client to web apps, DirectML, foundational to <a href="https://developer.microsoft.com/en-us/windows/ai/">Windows Copilot Runtime</a>, aims to uniquely simplify how developers can scale their AI innovations across Windows. Through a single, cross-hardware DirectX API, DirectML works across different hardware architectures, local device accelerators and machine learning frameworks, supporting an open platform for AI on Windows.

Let’s see some of the amazing AI innovations leveraging DirectML on Copilot+ PC NPUs.
<h2>Adobe® Premiere® Pro is the first Adobe app to leverage NPU on Copilot+ PCs, powered by Intel® Core™ Ultra and DirectML</h2>
Adobe is at the forefront of machine learning (ML) for creatives by providing a variety of hardware-accelerated features throughout their product suite. Now, Adobe Premiere Pro has enabled the machine learning efficiency of the latest Intel Core Ultra processors, opening up even more platform possibilities for Adobe customers.

Through collaboration with Intel and Microsoft, Adobe enabled Audio Category Tagger to optionally utilize Intel’s latest generation NPU using DirectML as a beta capability in Premiere Pro. This feature automatically tags audio, classifying clips as sound effects, music, dialog or ambience, which makes it easy for creatives to just get started editing their audio files right away. Each clip is marked with an Essential Sound Badge, and clicking on that badge opens the Essential Sound Panel to give access to parameters needed for that sound category.

[caption id="attachment_57164" align="aligncenter" width="1024"]<img class="wp-image-57164 size-large" src="https://blogs.windows.com/wp-content/uploads/prod/sites/3/2024/11/Adobe-Premier-Pro-Audio-Category-Tagger-1024x331.png" alt="A screen capture of Premier Pro showing audio clips marked by icons indicating audio categories A, B, C, and D. A is Music, B is SFX, C is Dialogue, and D is Ambience." width="1024" height="331" /> Audio Category Tagger labeling A. Music B. SFX C. Dialogue D. Ambience[/caption]

We are excited to partner with an AI innovator like Adobe to deliver on the promise of DirectML. Adobe’s extensive portfolio of applications and AI-driven scenarios offers a unique proving ground for DirectML and we’ll continue our partnership across GPU- and NPU-accelerated applications.
<h2>Capture One to bring NPU-enhanced Match Look and AI Crop features to Copilot+ PCs</h2>
<strong>Capture One</strong> is an imaging software commercial studio photographers use in the fashion and e-commerce industries. It helps professionals quickly edit many images while maintaining complete control over the creative expression and artistry behind every shot. At Snapdragon Summit, Capture One showcased two advanced AI-powered features on Copilot+ PCs powered by the Qualcomm NPU.

[caption id="attachment_57165" align="aligncenter" width="1024"]<img class="wp-image-57165 size-large" src="https://blogs.windows.com/wp-content/uploads/prod/sites/3/2024/11/Capture-One-Match-Look-1024x575.jpg" alt="A screenshot of the Capture One application, where the Match Look feature is being applied to an image of a woman holding pears to her ears." width="1024" height="575" /> The Match Look feature being applied to an image in Capture One[/caption]

<strong>Match Look</strong> is a brand new tool that lets you instantly match the style or edits of a reference image — whether from a previous project, a film still or a mood board. It uses AI selection to dynamically adjust multiple editing sliders, giving you a strong starting point for editing and color grading.

[caption id="attachment_57167" align="aligncenter" width="1024"]<img class="wp-image-57167 size-large" src="https://blogs.windows.com/wp-content/uploads/prod/sites/3/2024/11/Capture-One-Ai-Crop-1024x575.jpg" alt="A screenshot of the Capture One application where there is a crop reference set on an image of a purse in order to use the AI Crop feature. AI Crop is displayed on the right hand side of the screenshot where it's consistently applying the crop to other images." width="1024" height="575" /> The AI Crop feature in use in Capture One[/caption]

<strong>AI Crop </strong>allows you to make instant and consistent crops. Avoid errors and reduce manual effort. Set a crop reference on a single photo and automatically create consistent crops for all other images, even while shooting.

Capture One continues to utilize AI innovation in a way that maintains the creative artistry their photographers required and DirectML is excited to partner with them to bring their vision to life.
<h2>Affinity Photo 2 accelerates Object and Subject Selection capabilities with NPU</h2>
Affinity Photo 2 is a popular image-editing app that is used by millions of digital creators worldwide. Human creativity is the heart of Affinity products, so it’s no surprise that the first AI enhanced tools they announced are focused on optimizing artist workflows — eliminating tedious tasks and friction between creator and canvas. Affinity Photo 2 will scale these features to their creator base by using DirectML on Copilot+ PCs. Like stated during the Snapdragon Summit, they have already achieved the optimized workflow of these AI experiences with DirectML, powered by the Qualcomm® Hexagon NPU in the Snapdragon<sup>®</sup> X Elite Compute Platform.

<strong>Object Selection</strong> is a newly announced feature that eliminates the grunt work of manually creating layer masks, by using ML to quickly isolate and segment objects within an image. Automating this process gives artists a shortcut to the ‘real’ creative work — applying visual adjustments and effects to targeted areas within an image. Using the Qualcomm® Hexagon NPU in the Snapdragon<sup>®</sup> X Elite Compute Platform, Object Selection tasks will increase overall efficiency and productivity for creators.

[caption id="attachment_57168" align="aligncenter" width="1024"]<img class="wp-image-57168 size-large" src="https://blogs.windows.com/wp-content/uploads/prod/sites/3/2024/11/Affinity-Object-Selection-1920-1024x576.png" alt="A screenshot of Affinity Photo 2 with Object Selection being applied to select a car in an image of a car with smoke all around it." width="1024" height="576" /> Object Selection being applied within Affinity Photo 2[/caption]

Affinity is at the forefront of identifying ML innovations that deliver to their promise of incredible speed, power and precision image editing software and DirectML looks forward to scaling this promise across Windows’ diverse hardware ecosystem.
<h2>What’s next?</h2>
The DirectML team is always excited to see the creative and amazing ways developers are innovating with AI to enhance the experience for their customers across Windows. Stay tuned for more DirectML features coming soon, as we continue to invest in features to support an open platform for AI on Windows! To learn more about DirectML and to get started today, visit <a href="https://learn.microsoft.com/en-us/windows/ai/directml/dml">aka.ms/directml</a> or check us out at various sessions at <a href="https://ignite.microsoft.com/en-US/home">Microsoft Ignite</a>.]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
