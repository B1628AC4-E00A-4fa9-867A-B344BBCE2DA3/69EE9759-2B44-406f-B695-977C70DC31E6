<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Windows Developer Blog</title>
	<atom:link href="https://blogs.windows.com/windowsdeveloper/feed/" rel="self" type="application/rss+xml" />
	<link>https://blogs.windows.com/windowsdeveloper/</link>
	<description></description>
	<lastBuildDate>Wed, 14 May 2025 17:24:51 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.5.5</generator>

<image>
	<url>https://winblogs.thesourcemediaassets.com/sites/3/2021/06/cropped-browser-icon-logo-32x32.jpg</url>
	<title>Windows Developer Blog</title>
	<link>https://blogs.windows.com/windowsdeveloper/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Microsoft App Assure helps Nord Security build for Windows on Arm</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/05/14/microsoft-app-assure-helps-nord-security-build-for-windows-on-arm/</link>
		
		<dc:creator><![CDATA[Steve Clarke, Editor]]></dc:creator>
		<pubDate>Wed, 14 May 2025 17:24:51 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57405</guid>

					<description><![CDATA[<p>With the recent release of <a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/">Copilot+ PCs</a>, developers are increasingly focused on optimizing their apps for these devices. Copilot+ PCs are the fastest, most intelligent</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/05/14/microsoft-app-assure-helps-nord-security-build-for-windows-on-arm/">Microsoft App Assure helps Nord Security build for Windows on Arm</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[With the recent release of <a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/">Copilot+ PCs</a>, developers are increasingly focused on optimizing their apps for these devices. Copilot+ PCs are the fastest, most intelligent Windows PCs ever built. With powerful new system architecture designed to deliver best-in-class performance, all–day battery life and the ability to run the most advanced AI models on device, Copilot+ PCs help users to be more productive, creative and to communicate more effectively. The first wave of Copilot+ PCs was powered by Snapdragon X Elite and X Plus silicon. AMD and Intel soon released Copilot+ PCs powered by Ryzen AI 300 and Intel Core Ultra 200v processors, respectively, demonstrating that we are all-in on Copilot+ PCs.

Users expect all their apps to run seamlessly on all Windows devices and we are excited to support developers in meeting these expectations through our App Assure Program. Developers can leverage Microsoft’s App Assure program for guidance and technical assistance to get the most out of Copilot+ PCs, especially optimizing apps to run on Arm-based Snapdragon X Series devices. Even the most technically adept companies can benefit from the expert advice offered by App Assure. Many developers have already taken advantage of the App Assure program to release Arm-native versions of their apps. Today, I’d like to talk about a successful engagement with a well-known application developer: NordVPN.

App Assure delivers on Microsoft’s compatibility promise: your apps will run on Windows on Arm, and Microsoft will be here to help remediate any issues. This no-cost program has been successful in helping many market-leading developers build Arm-optimized apps for Windows. One great example of this is the acceleration of NordVPN's release of the Arm-optimized version of their well-known VPN solution.
<h3>Embracing Windows on Arm with Microsoft App Assure</h3>
Windows users expect their favorite apps to work great on Arm-based PCs. To meet this expectation, App Assure engages with the most popular apps, such as NordVPN and many others, helping them optimize the platform.

NordVPN was enthusiastic and receptive when we reached out to them to provide technical support and advice. Adding an Arm-optimized version would extend their market presence and widen their audience. They planned to launch their product soon after Snapdragon X Series-based Copilot+ PCs became available.

"Working with Microsoft's App Assure team was a game-changer for us. Their technical guidance made a significant difference in the project. As engineers, we value direct, effective information and support, and that's exactly what we got. Being able to work so closely with Microsoft almost made it feel like their engineers were part of our extended team. Their engagement sped up our development process and is helping us get to market faster,” said Gytis Murauskas, Head of Engineering, Windows, at Nord Security.

<img class="alignnone wp-image-57408 size-large" src="https://pub-d00f534024b04d0e8036586fc78a41fa.r2.dev/sites/3/2025/04/AppAssure-1024x227.jpg" alt="Quote from the blog post along with photo of a man" width="1024" height="227" />Developers often face challenges when starting the effort of adding a new architecture to their application. Deciding the best approach among the many options can be the difference between an easy port and a difficult one. This is exactly why we provide the <a href="https://blogs.windows.com/windowsdeveloper/2024/03/13/announcing-worldwide-availability-of-arm-advisory-service-for-developers/">Arm Advisory Service</a> for developers, giving them an opportunity to consult directly with Microsoft engineers, ensuring a smooth development process and informed decision-making.

“The biggest decisions and scariest things were at the beginning of kicking off the project. On this whole path of migrating applications to Windows, there are quite a lot of possibilities. You can migrate to Native Farm. You can do some emulation, etcetera, etcetera. There were at least four possible ways to do that, and it was not an easy choice to decide which one made more sense for us,” said Murauskas. Microsoft App Assure engineers offer guidance, best practices and deep insights enabling effective decision making. Murauskas emphasized the value of collaborating with Microsoft, stating, "The input from the App Assure team was instrumental in making informed decisions. This collaboration enabled us to implement solutions with confidence, knowing we were on the right path."

As the engagement continued, our App Assure engineers were available to consult directly whenever challenges arose, ensuring a smooth development process and informed decision-making. Ultimately, this ongoing support and expertise helped NordVPN establish a solid foundation and accelerated their time to market.

We are excited that NordVPN continues to invest in the ARM ecosystem by bringing support for their Threat Protection feature. To try NordVPN along with Threat Protection today, <a href="https://nordvpn.com/download/windows/?msockid=22d4ebb8dda06aea1d24fec6d9a068d8">download NordVPN for Windows PC or Laptop</a>
<h3>App Assure helps developers unlock potential</h3>
With Copilot+ PCs, we have completely reimagined the entirety of the PC – from silicon to the operating system, from the application layer to the cloud – with AI at the center, marking the most significant change to the Windows platform in decades.

The challenges, dependencies and development journey may look very different for every organization offering productivity tools, security, frameworks or gaming, and at Microsoft, we’re passionate to help you in your journey to enable support for our customers.

We are excited to offer App Assure to developers building Arm-optimized applications for Windows, to help all organizations see just how easy it is to build for this platform. Read more about App Assure helping <a href="https://blogs.windows.com/windowsdeveloper/2024/05/16/microsoft-app-assure-helps-opera-build-arm-optimized-browser/">Opera build an Arm-optimized browser</a> to see another example of this work.

If you’d like to know more about how to add Arm support to your Windows app, check out our technical documentation at <a href="https://aka.ms/win/arm/howto">aka.ms/win/arm/howto</a>. Once you’re ready to begin your porting journey, Microsoft’s Arm Advisory Service can provide detailed insights into platform features, best practices and code examples. For example, App Assure engineers can help you:
<ul>
 	<li>Understand the nuances of emulated code translation and how to seamlessly interoperate between native and x64 code.</li>
 	<li>Configure build systems most efficiently for multi-architecture delivery.</li>
 	<li>Obtain Arm-based hardware or get started with Azure Virtual Machines and then prepare those environments for development, continuous integration or test runners.</li>
</ul>
If this sounds like something you’re interested in, <a href="https://aka.ms/AppAssureServicesForm">let us know by completing this form</a>.
<h3>ISV testimonials</h3>
We are proud to showcase a few Independent Software Vendors (ISVs) that have successfully leveraged our Arm Advisory Service to optimize their applications for the Windows on Arm platform. These organizations have harnessed the power of Arm technology to deliver outstanding software solutions, demonstrating the immense potential and ease of transition with the right support.

<img class="alignnone wp-image-57412 size-medium" src="https://pub-d00f534024b04d0e8036586fc78a41fa.r2.dev/sites/3/2025/04/VPN-logomark-logotype-colored-transparent-300x106.png" alt="Proton VPN logo" width="300" height="106" />“At <a href="https://protonvpn.com/">Proton VPN</a>, we prioritize delivering a seamless and secure experience for our users. Working with Microsoft’s App Assure team has been an outstanding experience—their expertise and proactive approach ensured that Proton VPN runs flawlessly on ARM PCs, helping us bring fast, private and reliable VPN access to more devices and people than ever before. Microsoft’s App Assure team made it effortless for us to optimize Proton VPN for PCs with ARM architecture. Their support ensured that users get the best performance and security, reinforcing our commitment to privacy and freedom online." - Antonio Cesarano, Lead Product Manager, Proton VPN.

<img class="alignnone wp-image-57411 size-full" src="https://pub-d00f534024b04d0e8036586fc78a41fa.r2.dev/sites/3/2025/04/FSecure.png" alt="Proton VPN logo" width="183" height="148" />"The App Assure team played a crucial role in driving discussions with <a href="https://www.f-secure.com/en">F-Secure's</a> partners to accelerate ARM64 support, ensuring our product's compatibility with Copilot+ PCs—an effort that proved easier and faster than anyone expected." - Katja Bashlovka, Product Manager, Consumer Engagement.

Of the many prominent ISVs that the App Assure team has helped to develop Arm-optimized apps, a few more recent successes are highlighted below:

<img class="alignnone wp-image-57424 size-full" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/04/isv2.png" alt="Collection of logos" width="970" height="301" />]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>PyTorch Arm native builds now available for Windows</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/04/23/pytorch-arm-native-builds-now-available-for-windows/</link>
		
		<dc:creator><![CDATA[Sanket Kalaskar]]></dc:creator>
		<pubDate>Wed, 23 Apr 2025 19:58:50 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57425</guid>

					<description><![CDATA[<p>We are excited to announce the availability of Arm native builds of PyTorch for Windows! Until now, developers and researchers had to compile PyTorch locally to support Windows Arm64. With PyTorch 2.7 release, developers can now access Arm native bui</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/04/23/pytorch-arm-native-builds-now-available-for-windows/">PyTorch Arm native builds now available for Windows</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[We are excited to announce the availability of Arm native builds of PyTorch for Windows! Until now, developers and researchers had to compile PyTorch locally to support Windows Arm64. With PyTorch 2.7 release, developers can now access Arm native builds of PyTorch for Windows available for Python 3.12. This unlocks the potential to leverage the full performance of Arm64 architecture on Windows devices, like <a href="https://www.microsoft.com/en-us/windows/copilot-plus-pcs?r=1">Copilot+ PCs</a>, for machine learning experimentation, providing a robust platform for developers and researchers to innovate and refine their models.

Now you can use Arm native builds of PyTorch to develop, train and test short-scale machine learning models locally on Arm powered Copilot+ PCs. This includes areas such as image classification, natural language processing, and generative AI like Stable Diffusion which will be exemplified below.
<h2>Getting Started</h2>
<h3>Prerequisites:</h3>
<ul>
 	<li>To address missing dependencies, we recommend installing MSVC and Rust
<ul>
 	<li><a href="https://aka.ms/vs/17/release/vs_BuildTools.exe">Visual Studio Build Tools</a> or any <a href="https://visualstudio.microsoft.com/downloads/">Visual Studio</a>
<ul>
 	<li>In Visual Studio Installer, please select Desktop development with C++</li>
</ul>
</li>
</ul>
</li>
</ul>
[caption id="attachment_57427" align="alignnone" width="645"]<img class="wp-image-57427 size-full" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/04/DesktopDev.png" alt="Desktop development with C++ user interface" width="645" height="160" /> Figure 1: Visual Studio Installer Project Selection[/caption]
<ul>
 	<li>In Visual Studio Installer, make sure VS 2022 C++ ARM64/ARM64EC build tools (latest) selected</li>
</ul>
[caption id="attachment_57428" align="alignnone" width="648"]<img class="wp-image-57428 size-full" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/04/Latest-tools.png" alt="Visual Studio Installer MSVC Selection" width="648" height="33" /> Figure 2: Visual Studio Installer MSVC Selection[/caption]
<ul>
 	<li><a href="https://www.rust-lang.org/tools/install">Install Rust</a></li>
 	<li>Select ARM64 installer for <a href="https://www.python.org/downloads/release/python-3129/">Python Release Python 3.12.9 | Python.org</a>.</li>
</ul>
<h3>PyTorch:</h3>
<ul>
 	<li>To install PyTorch Stable release (2.7.0):</li>
</ul>
<pre class="EnlighterJSRAW" style="padding-left: 40px;" data-enlighter-language="generic">pip install --extra-index-url https://download.pytorch.org/whl torch</pre>
<ul>
 	<li>To access Preview (Nightly) build:</li>
</ul>
<pre class="EnlighterJSRAW" data-enlighter-language="generic">pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cpu</pre>
<h3>LibTorch:</h3>
<ul>
 	<li>The getting started and installation guide for LibTorch can be accessed on <a href="https://pytorch.org/cppdocs/installing.html">PyTorch's website</a></li>
 	<li>To access stable LibTorch stable release, select litroch and follow <a href="https://pytorch.org/get-started/locally/">instructions</a></li>
 	<li>To access Nightly builds: <a href="https://download.pytorch.org/libtorch/nightly/cpu/libtorch-win-arm64-shared-with-deps-latest.zip">Release</a>, <a href="https://download.pytorch.org/libtorch/nightly/cpu/libtorch-win-arm64-shared-with-deps-debug-latest.zip">Debug</a></li>
</ul>
<h3>Recommendations:</h3>
It is advisable to create a Virtual Environment (venv) within your projects.  A virtual environment separates dependencies between projects, providing isolation.

For further information, please refer to the documentation available on <a href="https://code.visualstudio.com/docs/python/environments#_creating-environments">VS Code</a> and <a href="https://docs.python.org/3/library/venv.html">Python</a>.
<h2>Example:</h2>
In this next example, we have used Arm native PyTorch binaries with <a href="https://huggingface.co/stabilityai/sd-turbo"><em>stabilityai/sd-turbo</em></a> model for stable diffusion. Some of the terminologies used in this example:

<strong>Prompt:</strong> A descriptive phrase or sentence that will be used by the Stable Diffusion model to generate images.

<strong>Steps: </strong>A slider that allows the user to select the number of inferences steps the model should take to generate the images. More steps can lead to higher quality images but will take longer to process.

<strong>Seed:</strong> A numerical input field where the user can enter a specific seed value to ensure reproducibility of the generated images. If left empty, a random seed will be used, resulting in different images each time for the same prompt. The default value is 42.

Below is the code, which can also be accessed on the GitHub repository, <a href="https://github.com/Windows-on-ARM-Experiments/pytorch-examples/tree/main/stable-diffusion">pytorch-examples/stable-diffusion</a>:
<pre class="EnlighterJSRAW" data-enlighter-language="generic">import gradio as gr

import torch

from diffusers import StableDiffusionPipeline




device = &quot;cpu&quot;

pipe = StableDiffusionPipeline.from_pretrained(&quot;stabilityai/sd-turbo&quot;, torch_dtype=torch.float32)

pipe = pipe.to(device)




def generate_image(prompt, steps, seed):

generator = None

if seed is not None:

generator = torch.Generator(device=device).manual_seed(seed)




# guidance is 0.0 as recommended by sd-turbo

# width and height are set to 512 as recommended by sd-turbo

result = pipe(prompt, num_inference_steps=steps,        num_images_per_prompt=2, guidance_scale=0.0, generator=generator, width=512, height=512)

return result.images[0], result.images[1]




with gr.Blocks() as demo:

gr.Markdown(&quot;# SD Turbo Demo - WoA&quot;)

with gr.Row():

prompt = gr.Textbox(label=&quot;Prompt&quot;, placeholder=&quot;Enter your prompt here&quot;)

steps = gr.Slider(minimum=1, maximum=5, step=1, value=1, label=&quot;Steps&quot;)

seed = gr.Number(label=&quot;Seed (leave empty for random seed)&quot;, value=42, precision=0)

with gr.Row():

image1 = gr.Image(type=&quot;pil&quot;, label=&quot;Generated Image 1&quot;)

image2 = gr.Image(type=&quot;pil&quot;, label=&quot;Generated Image 2&quot;)

gr.Button(&quot;Generate&quot;).click(

fn=generate_image,

inputs=[prompt, steps, seed],

outputs=[image1, image2]

)




demo.launch(share=False)</pre>
<h2>Output:</h2>
[caption id="attachment_57429" align="alignnone" width="786"]<img class="wp-image-57429 size-full" src="https://winblogs.thesourcemediaassets.com/sites/3/2025/04/Cats.png" alt="Two images of cats" width="786" height="443" /> Figure 3: Image Output[/caption]

<strong><em>Disclaimer:</em></strong><em> The model used in this example is </em><a href="https://huggingface.co/stabilityai/sd-turbo"><em>stabilityai/sd-turbo</em></a><em>. As this is a basic generative AI application, we do not assume responsibility for the results generated. You are solely responsible for generating images on your computer.</em>

<strong>Note</strong>:

Some packages you may use alongside PyTorch don’t have Arm native support for Windows.
<strong>pip</strong> can automatically install dependencies from source code (tar.gz) and compile them into “.whl” files using MSVC and Rust on your system.

Please check the prerequisites section for details.
<ul>
 	<li><strong>NumPy 2.2.3 </strong>can be installed via compilation.</li>
</ul>
<pre class="EnlighterJSRAW" data-enlighter-language="generic">pip install numpy==2.2.3</pre>
<ul>
 	<li><strong>safetensors 0.5.3 </strong>can be installed via compilation.
<pre class="EnlighterJSRAW" data-enlighter-language="generic">pip install safetensors==0.5.3</pre>
</li>
</ul>
<h2>Conclusion</h2>
Developers can now start using stable Arm native PyTorch builds for Windows to create applications that leverage AI and utilize the full potential of ARM architecture. Give the native binaries a try today! <a href="https://pytorch.org/">Download here</a>.]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>GitHub Actions now supports Windows on Arm runners for all public repos</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/04/14/github-actions-now-supports-windows-on-arm-runners-for-all-public-repos/</link>
		
		<dc:creator><![CDATA[Marcus Perryman]]></dc:creator>
		<pubDate>Mon, 14 Apr 2025 19:31:11 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57413</guid>

					<description><![CDATA[<p>We are continuously investing in improving the Windows on Arm developer experience by providing and improving the tools needed by developers targeting Arm powered Copilot+ PCs.</p>
<p>Today we are thrilled to announce Windows on Arm runner availability has</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/04/14/github-actions-now-supports-windows-on-arm-runners-for-all-public-repos/">GitHub Actions now supports Windows on Arm runners for all public repos</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[We are continuously investing in improving the Windows on Arm developer experience by providing and improving the tools needed by developers targeting Arm powered Copilot+ PCs.

Today we are thrilled to announce Windows on Arm runner availability has been extended to all public repos, including GitHub Free tier accounts.

This enhancement simplifies continuous integration workflows and enables developers to better support Arm-based Windows applications. With this update all accounts, including open-source projects using free tier accounts, can incorporate Windows on Arm targets into their CI pipelines for public projects, achieving a similar level of build and regression testing as is available for Intel targets. This improvement simplifies the process for maintainers to add Windows Arm targets to CI jobs to ensure compatibility and reliability across a wider range of architectures.
<h3>Key Benefits</h3>
<ol>
 	<li>Expanded Architecture Support: With the continued growth of Windows on Arm devices – especially the Qualcomm powered Copilot+ PCs - developers can easily extend support to this platform without requiring additional infrastructure.</li>
 	<li>Improved Continuous Integration workflows: Adding Arm runners to your pipeline allows for consistent testing and building across both Arm and Intel architectures to pick up regressions as early as possible.</li>
 	<li>Directly benefits open-source projects: The availability of Arm runners for all public repos, including free tier accounts, reinforces GitHub’s commitment to supporting innovation in the open-source community. To learn more about eligibility for GitHub free tier, visit the <a href="https://github.com/pricing">GitHub Pricing page</a>.</li>
</ol>
The new Windows 11 Arm image for GitHub runners ships with tooling and software for a variety of different development environments. <a href="https://github.com/actions/partner-runner-images/blob/main/images/arm-windows-11-image.md">Find a full list of included dependencies</a>.
<h3>Getting Started</h3>
Adding Windows on Arm targets to your CI pipeline via GitHub Actions is easy. Windows on Arm runners can be added to your public repos by adding the “windows-11-arm” runner target in your yml workflow. Below is an example GitHub Actions workflow for a C# project using Visual Studio tools:
<img class="alignnone wp-image-57419 size-full" src="https://pub-d00f534024b04d0e8036586fc78a41fa.r2.dev/sites/3/2025/04/Code.png" alt="Example GitHub Actions workflow for a C# project using Visual Studio tools" width="523" height="439" />
<h3>Moving Forward</h3>
The inclusion of Windows on Arm runners in GitHub Actions marks a valuable step forward for open-source projects and developers aiming to support diverse architectures. By integrating Arm targets into your workflows, you can ensure your applications deliver reliable performance across platforms.

Try it out today and broaden your project's reach and compatibility.

<a href="https://docs.github.com/en/actions/using-github-hosted-runners/using-github-hosted-runners/about-github-hosted-runners?supported-runners-and-hardware-resources=">Find out more about adding GitHub-hosted runners to your repository</a>.]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Part 3 – Babylon.js 8.0: glTF, USDz, and WebXR advancements</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/04/03/part-3-babylon-js-8-0-gltf-usdz-and-webxr-advancements/</link>
		
		<dc:creator><![CDATA[Steve Clarke, Editor]]></dc:creator>
		<pubDate>Thu, 03 Apr 2025 20:02:17 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57383</guid>

					<description><![CDATA[<p>The latest Babylon.js 8.0 release includes several updates to assist developers in creating beautiful and performant 3D experiences.</p>
<h3><strong>Updated glTF Support — KHR_materials_diffuse_transmission</strong></h3>
<p>Babylon.js 8.0 continues the lo</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/04/03/part-3-babylon-js-8-0-gltf-usdz-and-webxr-advancements/">Part 3 – Babylon.js 8.0: glTF, USDz, and WebXR advancements</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[The latest Babylon.js 8.0 release includes several updates to assist developers in creating beautiful and performant 3D experiences.
<h3><strong>Updated glTF Support — KHR_materials_diffuse_transmission</strong></h3>
Babylon.js 8.0 continues the long rich tradition of supporting every extension update to the glTF format. As glTF advances, Babylon.js is in lock-step with support for those advancements. Babylon.js 8.0 brings support for the very beautiful KHR_materials_diffuse_transmission!

Check out a demo: <a href="https://aka.ms/babylon8gltfdemo">https://aka.ms/babylon8gltfdemo</a>

Learn more here: <a href="https://aka.ms/babylon8gltfDoc">https://aka.ms/babylon8gltfDoc</a>

https://youtu.be/c5qumu-k9lQ

Credit to<a href="https://www.linkedin.com/in/ericchadwick/"> Eric Chadwick</a> for the beautiful asset, seriously, Eric’s work is incredible, go support him!
<h3><strong>glTF Exporter Improvements</strong></h3>
Babylon.js introduces a host of new improvements to the glTF serializer, allowing you to export your Babylon scenes as glTF objects with support for the latest extensions and features!

Check out a demo: <a href="https://aka.ms/babylon8gltfSerializerDemo">https://aka.ms/babylon8gltfSerializerDemo</a> (try exporting as glTF and opening in your favorite 3D tool)

Learn more here: <a href="https://aka.ms/babylon8gltfSerializerDoc">https://aka.ms/babylon8gltfSerializerDoc</a>

https://youtu.be/RSbGAeS-FEI
<h3><strong>More glTF Loader Options</strong></h3>
Babylon.js 8.0 unlocks new loader options for glTF objects allowing you to programmatically load your assets with pre-determined specifications, such as loading at a specific LOD (Level of Detail). These new loader options will provide new flexibility and control of how you bring assets into your Babylon.js scenes!

Check out a demo: <a href="https://aka.ms/babylon8glTFLoaderDemo">https://aka.ms/babylon8glTFLoaderDemo</a>

Learn more here: <a href="https://aka.ms/babylon8glTFLoaderDoc">https://aka.ms/babylon8glTFLoaderDoc</a>

https://youtu.be/hrZNyJyf9hA
<h3><strong>IES Light Support</strong></h3>
IES Lighting is a technique that describes the “shape” of a light that emits from a lamp. You can read more about it here: <a href="https://ieslibrary.com/">IES-Library</a>. Babylon.js brings support for IES files, unlocking new ways to express dimension and lighting in your scenes!

Check out a demo: <a href="https://aka.ms/babylon8IESLightDemo">https://aka.ms/babylon8IESLightDemo</a> (click/touch to see different IES light shapes)

Learn more here: <a href="https://aka.ms/babylon8IESLightDoc">https://aka.ms/babylon8IESLightDoc</a>

https://youtu.be/j-InYXIJr2w
<h3><strong>USDZ Export</strong></h3>
Babylon.js 8.0 allows you to export .usdz files, making it easier than ever for you to build AR experiences targeted for iOS devices!

Check out a demo: <a href="https://aka.ms/babylon8usdzDemo">https://aka.ms/babylon8usdzDemo</a>

Learn more here: <a href="https://aka.ms/babylon8usdzDoc">https://aka.ms/babylon8usdzDoc</a>

https://youtu.be/DerHDOT0TYE
<h3><strong>GPU Mesh Picking</strong></h3>
Picking meshes in a scene can be an expensive feature since the CPU has to loop through every piece of geometry to find the triangle that most closely intersects the ray cast from the picking location.

Ready for some wizardry? Babylon.js 8.0 introduces the ability to pick meshes from directly from the GPU! Yup, that’s right. This means that in complex scenes, where picking is required, you can boost performance by offloading this to the GPU.

Check out a demo: <a href="https://aka.ms/babylon8gpuPickDemo">https://aka.ms/babylon8gpuPickDemo</a>

Learn more here: <a href="https://aka.ms/babylon8gpuPickDoc">https://aka.ms/babylon8gpuPickDoc</a>

https://youtu.be/4hBhUFk-k_U
<h3><strong>GPU Bounding Box</strong></h3>
Bounding Box calculation can be expensive, especially if you have a few animated meshes with a large number of vertices. Babylon.js 8.0 introduces the ability to pass Bounding Box calculation to the GPU, freeing up valuable cycles on the CPU and improving performance for your scene.

Check out a demo: <a href="https://aka.ms/babylon8gpuBBoxDemo">https://aka.ms/babylon8gpuBBoxDemo</a>

Learn more here: <a href="https://aka.ms/babylon8gpuBBoxDoc">https://aka.ms/babylon8gpuBBoxDoc</a>

https://youtu.be/JdKSMPf-aWg
<h3><strong>EXR Texture Support</strong></h3>
Babylon.js 8.0 also brings support for <a href="https://en.wikipedia.org/wiki/OpenEXR">EXR files</a>. This feature-rich image format unlocks some new superpowers for you to use in Babylon.js. Negative pixel values, for example, can be used to store complex visualizations as a texture format that can now be read in Babylon.js!

Check out a demo: <a href="https://aka.ms/babylon8exrDemo">https://aka.ms/babylon8exrDemo</a>

https://youtu.be/fUHEycgijb8
<h3><strong>WebXR Depth Sensing</strong></h3>
Babylon.js 8.0 brings support for a new and exciting WebXR feature called “Depth Sensing.” This feature uses depth information captured from devices to give developers the ability to overlay real world visuals “on top” of computer-generated images! You just have to see it to believe it. It feels like magic!

Check out a demo (on recent Android devices and Quest 3): <a href="https://aka.ms/babylon8webxrDSDemo">https://aka.ms/babylon8webxrDSDemo</a>

Learn more here: <a href="https://aka.ms/babylon8webxrDSDoc">https://aka.ms/babylon8webxrDSDoc</a>

https://youtu.be/fqmFjLD0xBE
<h3><strong>A Peek Ahead</strong></h3>
Phew! Babylon.js 8.0 is by far the platform’s biggest release to date. There’s so much goodness packed into it that it feels like you’ve been scrolling forever! Would you believe there’s even more in active development?!!! Here’s a tiny tease about a couple of things on the horizon:

<strong>glTF Interactivity Support</strong><strong> </strong>— If you follow the advancement of the glTF file format, then you know that exciting things are coming with the interactivity workstream. We’re actively working on supporting this incredible new extension that will allow asset behaviors to travel with your assets. That means that interactivity is no longer tied to one specific creation tool or engine!

<strong>OpenPBR Support</strong><strong> </strong>— <a href="https://academysoftwarefoundation.github.io/OpenPBR/">OpenPBR</a> is an open standard for how Physically Based Rendered materials look when rendered. Guess what’s in active development in Babylon? 😉

<strong>Tooling For Everyone</strong><strong> </strong>— We understand that everyone who comes to the Babylon platform comes from different backgrounds and experience levels, from software engineers at the top of their game, to students and artists just beginning their computer graphics journey. We are passionate about ensuring that everyone who is interested in leveraging Babylon.js has the tools and workflows to do so. We’ll have more to share on this later this year.
<h3><strong>Thank You</strong></h3>
With each evolution of Babylon.js comes a revolution in web rendering technology and an overwhelming feeling of gratitude. The Babylon platform simply wouldn’t be possible without the incredible community of developers, the 500+ contributors, and the steadfast advocates that contribute their knowledge, expertise, help and passion to this amazing technology. “Thank you” to each one of you for all that you do to help make Babylon.js one of the most powerful, beautiful, simple and open web rendering platforms in the world.

<em>See <a href="https://blogs.windows.com/windowsdeveloper/2025/03/27/announcing-babylon-js-8-0/">Part 1</a> and <a href="https://blogs.windows.com/windowsdeveloper/2025/03/31/part-2-babylon-js-8-0-audio-gaussian-splat-and-physics-updates/">Part 2</a> of this series of posts about what's in Babylon.js 8.0.</em>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Part 2 &#8211; Babylon.js 8.0: Audio, Gaussian Splat and physics updates</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/03/31/part-2-babylon-js-8-0-audio-gaussian-splat-and-physics-updates/</link>
		
		<dc:creator><![CDATA[Steve Clarke, Editor]]></dc:creator>
		<pubDate>Mon, 31 Mar 2025 19:59:20 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57382</guid>

					<description><![CDATA[<p>Our mission is to create one of the most powerful, beautiful and simple web rendering engines in the world. The latest Babylon.js 8.0 engine packs a ton of new improvements to help you create stunning experiences.</p>
<h3><strong>Overhauled Audio Engine<
</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/03/31/part-2-babylon-js-8-0-audio-gaussian-splat-and-physics-updates/">Part 2 &#8211; Babylon.js 8.0: Audio, Gaussian Splat and physics updates</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[Our mission is to create one of the most powerful, beautiful and simple web rendering engines in the world. The latest Babylon.js 8.0 engine packs a ton of new improvements to help you create stunning experiences.
<h3><strong>Overhauled Audio Engine</strong></h3>
Babylon’s audio engine is long overdue for a tune-up and Babylon.js 8.0 swings for the fences, bringing an entirely fresh audio engine to your ears. This new engine was designed to be <strong>powerful —</strong><strong> </strong>taking advantage of the full suite of web-audio features, <strong>modern</strong><strong> </strong>— class names and architecture you’ve come to expect, and <strong>simple-to-use</strong> — allowing anyone to leverage these features, no matter what your experience level. For those of you who care passionately about the marriage of audio and visuals to tell a compelling story, Babylon.js 8.0 was built for you!

Check out a demo: <a href="https://aka.ms/babylon8AudioEnginev2Demo">https://aka.ms/babylon8AudioEnginev2Demo</a>

Learn more here: <a href="https://aka.ms/babylon8AudioEnginev2Doc">https://aka.ms/babylon8AudioEnginev2Doc</a>

https://youtu.be/vdjBS_EfeDg
<h3><strong>Gaussian Splat Updates</strong></h3>
Babylon.js 8.0 builds on the exciting foundation of Gaussian Splat support with some exciting new updates such as SPZ and compressed PLY formats, spherical harmonics, as well as runtime optimizations for memory footprint and CPU/GPU usage.

Check out a demo for SPZ: <a href="https://aka.ms/babylon8gsplatImrovementsDemo">https://aka.ms/babylon8gsplatImrovementsDemo</a>

Learn more here: <a href="https://aka.ms/babylon8gsplatImprovementsDoc">https://aka.ms/babylon8gsplatImprovementsDoc</a>

https://youtu.be/8iMUkvsYzLU

<em>Footage from: </em><a href="https://www.basilicasanpietro.va/en/"><em>St. Peter’s Basilica</em></a><em>. A collaboration with Microsoft and Iconem for the Fabbrica di San Pietro.</em>
<h3><strong>Havok Character Controller</strong></h3>
With Babylon.js 8.0, we’ve continued our amazing partnership with the very talented team at Havok, this time bringing their fully featured character controller into Babylon.js. This brings a state-of-the-art character controller to your toolbox allowing you to start making your very own character-centered game with just a few lines of code!

Check out a demo: <a href="https://aka.ms/babylon8havokCCDemo">https://aka.ms/babylon8havokCCDemo</a>

Learn more here: <a href="https://aka.ms/babylon8havokCCDoc">https://aka.ms/babylon8havokCCDoc</a>

https://youtu.be/Bu_hAq6kGBk
<h3><strong>Smart Filters</strong></h3>
Babylon.js 8.0 builds on our rich library of node-based creation tools with the introduction of Smart Filters and the Smart Filter Editor. This new tool allows you to create video filters, texture treatments, post-processes, you name it. If it’s a 2D visual effect, this tool is for you! Under the hood it leverages shaders in the same way you’d expect any of our GPU-based tools to do, but it focuses on helping you create elaborate 2D visual treatments for web experiences. We can’t wait to see what you make with this exciting new tool!

Try it out yourself (on desktop): <a href="https://aka.ms/babylon8sfe">https://aka.ms/babylon8sfe</a>

Check out a demo (on desktop): <a href="https://aka.ms/babylon8sfeDemo">https://aka.ms/babylon8sfeDemo</a>

Learn more here: <a href="https://aka.ms/babylon8sfeDoc">https://aka.ms/babylon8sfeDoc</a>

https://youtu.be/WREWPLE6NAM
<h3><strong>Environment Improvements </strong></h3>
Babylon.js 8.0 continues to improve visual realism, leveling up the environment lighting to look closer and closer to real-time ray traced results! Another shout out to Michael Bond at Adobe for this additional great contribution.

Check out a demo: <a href="https://aka.ms/babylon8EnvImprovementsDemo">https://aka.ms/babylon8EnvImprovementsDemo</a>

Learn more here: <a href="https://aka.ms/babylon8EnvImprovementsDoc">https://aka.ms/babylon8EnvImprovementsDoc</a>

https://youtu.be/AYpnP0a9ZcE
<h3><strong>Node Geometry Editor Updates</strong></h3>
Last year, Babylon.js introduced the ability to procedurally generate geometry without writing any code through the Node Geometry Editor.

With Babylon.js 8.0, Node Geometry takes a big step up with a massive list of new features including a lattice deformer, point list, clean geometry, interceptor, an aggregator and the ability to subdivide.

Check out a demo (on desktop): <a href="https://aka.ms/babylon8ngeDemo">https://aka.ms/babylon8ngeDemo</a>

Learn more here: <a href="https://aka.ms/babylon8ngeDoc">https://aka.ms/babylon8ngeDoc</a>

https://youtu.be/ZjQ0Nu5pXdk
<h3><strong>Node Material Editor Debug Node</strong></h3>
Babylon’s Node Material Editor makes it incredibly simple to create complex visual shaders without writing any code. This artist-friendly tool bridges the gap between the complexity of building GPU shaders and the way artists think and work.

Babylon.js 8.0 introduces some exciting UI improvements as well as the new incredibly useful “visual debug node.” This node allows you to see the visual output at any point in your Node Tree. You no longer have to move the output around and hook it up to different places in your graph. It’s as simple as adding debug nodes throughout your tree to see how your shader changes throughout the computation!

Check out a demo (on desktop): <a href="https://aka.ms/babylon8nmedebugnodedemo">https://aka.ms/babylon8nmedebugnodedemo</a>

Learn more here: <a href="https://aka.ms/babylon8nmedebugnodedoc">https://aka.ms/babylon8nmedebugnodedoc</a>

https://youtu.be/vBwgLWeIERs
<h3><strong>Improved Booleans</strong></h3>
No, not those Booleans. True and False are just fine the way they are. We’re talking about Geometric Booleans!

Babylon.js 8.0 introduces support for the popular <a href="https://github.com/elalish/manifold">Manifold.js</a> library, allowing you to create new shapes with more consistent Geometric Booleans. Or more simply put, “Finally some Booleans that look the way I expect them to look!”

Check out a demo: <a href="https://aka.ms/babylon8booleanDemo">https://aka.ms/babylon8booleanDemo</a>

Learn more here: <a href="https://aka.ms/babylon8booleanDoc">https://aka.ms/babylon8booleanDoc</a>

https://youtu.be/SE5-rF8ryhQ

This list is extensive, but there's more to come! Check out the next post for updates on glTF, USDz, WebXR and more.

<em>See <a href="https://blogs.windows.com/windowsdeveloper/2025/03/27/announcing-babylon-js-8-0/">Part 1</a> and <a href="https://blogs.windows.com/windowsdeveloper/2025/04/03/part-3-babylon-js-8-0-gltf-usdz-and-webxr-advancements/">Part 3</a> of this series of posts about what's in Babylon.js 8.0.</em>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Announcing Babylon.js 8.0</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/03/27/announcing-babylon-js-8-0/</link>
		
		<dc:creator><![CDATA[Steve Clarke, Editor]]></dc:creator>
		<pubDate>Thu, 27 Mar 2025 20:03:55 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57381</guid>

					<description><![CDATA[<p>Our mission is to build one of the most powerful, beautiful, simple and open web rendering engines in the world. Today, web graphics and rendering hit the accelerator with the release of Babylon.js 8.0.</p>
<p>https://youtu.be/kKaomUggipQ</p>
<p>Babylon.js 8.0 r</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/03/27/announcing-babylon-js-8-0/">Announcing Babylon.js 8.0</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[Our mission is to build one of the most powerful, beautiful, simple and open web rendering engines in the world. Today, web graphics and rendering hit the accelerator with the release of Babylon.js 8.0.

https://youtu.be/kKaomUggipQ

Babylon.js 8.0 represents a year of new features, optimizations and performance improvements aimed at helping you create more compelling, interactive web experiences faster than ever.
<h3><strong>IBL Shadows</strong></h3>
Image-Based Lighting (<a href="https://en.wikipedia.org/wiki/Image-based_lighting">IBL</a>) is a computer graphics lighting technique that approximates environment lighting based on a source image. Originating in the visual effects world as a way to blend computer-generated effects with real photography, IBL has become a ubiquitous tool for computer graphics wizards around the world.

Babylon.js first introduced support for IBL over 8 years ago and it has quickly become one of the most commonly used features of the engine.

Today, we are absolutely thrilled to announce that our good friends at Adobe leveled up IBL in Babylon by adding shadows to the mix! Yup, that’s right, now both light and shadows for the scene environment can be approximated from a source image. Special shout out to Michael Bond at Adobe for his incredible work on this!

Check out a demo: <a href="https://aka.ms/babylon8IBLShadows">https://aka.ms/babylon8IBLShadows</a>

Learn more: <a href="https://aka.ms/babylon8IBLShadowsDoc">https://aka.ms/babylon8IBLShadowsDoc</a>

https://youtu.be/xVLyy7N4Us8
<h3><strong>Area Lights</strong></h3>
We are thrilled to announce that Babylon.js 8.0 brings a frequently requested feature…Area Lights! This amazing new addition to the lighting palette allows you to specify a 2D shape that emits light from it, much like a large diffuse light that you might find on a movie set! We can’t wait to see how you use this new light type to bring a new dimension to your scene!

Check out a demo: <a href="https://aka.ms/babylon8AreaLightsDemo">https://aka.ms/babylon8AreaLightsDemo</a>

Learn more here: <a href="https://aka.ms/babylon8AreaLightsDoc">https://aka.ms/babylon8AreaLightsDoc</a>

https://youtu.be/U1FdTyTlJsU
<h3><strong>Node Render</strong><strong> Graph - Alpha</strong></h3>
One of the most powerful new features in Babylon.js 8.0 is something we call the “Node Render Graph.”

Up to now, the specific rendering pipeline for Babylon has been a black box. You tell Babylon what to render and it goes off and does it for you. There have been observables that allow you to manipulate the result after completion of a render, but the render process itself has been opaque. Well…not anymore!

With Babylon.js 8.0 you now have full control of the entire render pipeline. This means that you can fully customize and control every part of the process of how your frames are rendered on the GPU. And if that wasn’t enough, you also now have a fancy new Node Graph tool to allow you to customize your own render pipeline, without writing complex render process code! Please note that the Node Render Graph is in Alpha for you to test and discover but should not be used in production yet as it is subject to change.

Try out the editor (on desktop): <a href="https://nrge.babylonjs.com/">Babylon.js Node Render Graph Editor</a>

Check out a demo (on desktop): <a href="https://aka.ms/babylon8RenderGraphDemo">https://aka.ms/babylon8RenderGraphDemo</a>

Learn more here: <a href="https://aka.ms/babylon8RenderGraphDoc">https://aka.ms/babylon8RenderGraphDoc</a>

https://youtu.be/emoIox2sru0
<h3><strong>All New Lightweight Viewer</strong></h3>
Babylon.js is a powerful tool used by tens of thousands of people and organizations across the globe to bring complex visual ideas to life on the web. Babylon.js 8.0 unlocks a new super-powered tool for the other end of the spectrum…those scenarios where you want to display a simple 3D object on a web page with zero complexity, and stunning visuals in a tiny package.

Introducing the all new Babylon.js Lightweight Viewer. This new viewer is designed to harness the same rendering beauty and power of the full engine but comes in a smaller bundle footprint and uses dynamic imports and capabilities (audio or animation for example) depending on the model that is loaded. It can be added to any web page with just a few lines of HTML and is also fully extensible!

Of course, this new Lightweight Viewer wouldn’t be complete without a super easy-to-use configurator along with it! The Viewer Configurator is a simple tool that allows you to customize the Viewer to your exact specifications and give you the simple .html properties to set so it looks the same in your website!

Play with the Configurator (on desktop): <a href="https://aka.ms/babylon8ViewerConfig">https://aka.ms/babylon8ViewerConfig</a>

Check it out: <a href="https://aka.ms/babylon8viewerHome">https://aka.ms/babylon8viewerHome</a>

Learn more here: <a href="https://aka.ms/babylon8viewerDoc">https://aka.ms/babylon8viewerDoc</a>

https://youtu.be/HmkkGE96Xb4
<h3><strong>WGSL Core Engine</strong><strong> Shaders</strong></h3>
Babylon.js has had support for WebGPU since its inception. The core engine shaders in Babylon.js, however, have been written in GLSL (WebGL shading language) from the beginning. Because WebGPU has its own shading language (WGSL) this posed a very interesting challenge. How do you get GLSL shaders to render in WebGPU? Fortunately, there is a conversion library that’s been available. So, anyone wanting to target WebGPU with Babylon can leverage this library to convert the Babylon shaders into something WebGPU can use. The downside of this is that this conversion library is over 3MB, requiring users to double their download size for a standardBabylon.js project.

With Babylon 8.0, this problem is a thing of the past. All of the core engine shaders for Babylon.js are now available in both GLSL and WGSL. This means direct support for WebGPU right out of the box with no conversion layer, essentially making Babylon.js 2x smaller when targeting WebGPU than in the past!

Check out a demo: <a href="https://aka.ms/babylon8WGSLDemo">https://aka.ms/babylon8WGSLDemo</a> (try switching between WebGL2 and WebGPU)

Learn more here: <a href="https://aka.ms/babylon8WGSLDoc">https://aka.ms/babylon8WGSLDoc</a>

https://youtu.be/pKyGxT5xyUs
<h3><strong>NME -> WGSL</strong><strong> Support</strong></h3>
Well, why stop at core engine shaders? Why not unlock the ability to create custom WGSL shaders using Babylon’s <a href="https://doc.babylonjs.com/toolsAndResources/nme/">Node Material Editor</a> as well!

OK! Check!

Ability Unlocked! Let’s go Babylon 8.0!!!!!

Check out a demo (on desktop): <a href="https://aka.ms/babylon8nmeWGSL">https://aka.ms/babylon8nmeWGSL</a>

Learn more here: <a href="https://aka.ms/babylon8nmeWGSLDoc">https://aka.ms/babylon8nmeWGSLDoc</a>

https://youtu.be/IAKi7krpUgA

Those are just some of the main features of Babylon.js 8.0, there is much more! Tune in for next posts and learn more about Audio, Gaussian Splat and physics advancements, ….

<em>See <a href="https://blogs.windows.com/windowsdeveloper/2025/03/31/part-2-babylon-js-8-0-audio-gaussian-splat-and-physics-updates/">Part 2</a> and <a href="https://blogs.windows.com/windowsdeveloper/2025/04/03/part-3-babylon-js-8-0-gltf-usdz-and-webxr-advancements/">Part 3</a> of this series of posts about what's in Babylon.js 8.0.</em>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>AI Toolkit for Visual Studio Code now supports NVIDIA NIM microservices for RTX AI PCs</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/03/25/ai-toolkit-for-visual-studio-code-now-supports-nvidia-nim-microservices-for-rtx-ai-pcs/</link>
		
		<dc:creator><![CDATA[Anna Soracco]]></dc:creator>
		<pubDate>Tue, 25 Mar 2025 13:03:15 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57390</guid>

					<description><![CDATA[<p><strong><em>Editor’s note – March 26, 2025 –</em></strong><em> The screenshot was updated to reflect relevant models.</em></p>
<p><a href="https://aka.ms/aitoolkit">AI Toolkit</a> now supports <a href="https://developer.nvidia.com/nim">NVIDIA NIM mic</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/03/25/ai-toolkit-for-visual-studio-code-now-supports-nvidia-nim-microservices-for-rtx-ai-pcs/">AI Toolkit for Visual Studio Code now supports NVIDIA NIM microservices for RTX AI PCs</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<strong><em>Editor’s note – March 26, 2025 –</em></strong><em> The screenshot was updated to reflect relevant models.</em>

<a href="https://aka.ms/aitoolkit">AI Toolkit</a> now supports <a href="https://developer.nvidia.com/nim">NVIDIA NIM microservice</a>-based foundation models for inference testing in the model playground! Explore these NIMs in AI Toolkit's model catalog today!

[caption id="attachment_57393" align="alignnone" width="1024"]<img class="wp-image-57393 size-large" src="https://pub-d00f534024b04d0e8036586fc78a41fa.r2.dev/sites/3/2025/03/blog_post_update1920-1024x578.png" alt="Editor’s note – March 25, 2025 – The screenshot was updated to reflect relevant models" width="1024" height="578" /> AI Toolkit now supports NVIDIA NIM for a unified development environment[/caption]

Read more about it on the Tech Community Blog at <a href="https://aka.ms/aitk/nimsblog">aka.ms/aitk/nimsblog</a>.]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Available today: DeepSeek R1 7B &#038; 14B distilled models for Copilot+ PCs via Azure AI Foundry – further expanding AI on the edge</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/03/03/available-today-deepseek-r1-7b-14b-distilled-models-for-copilot-pcs-via-azure-ai-foundry-further-expanding-ai-on-the-edge/</link>
		
		<dc:creator><![CDATA[Vivek Pradeep]]></dc:creator>
		<pubDate>Mon, 03 Mar 2025 19:53:09 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57375</guid>

					<description><![CDATA[<p>At Microsoft, we believe the future of AI is happening now — spanning from the cloud to the edge. Our vision is bold: to build Windows as the ultimate platform for AI innovation, where intelligence isn’t just in the cloud but seamlessly woven t</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/03/03/available-today-deepseek-r1-7b-14b-distilled-models-for-copilot-pcs-via-azure-ai-foundry-further-expanding-ai-on-the-edge/">Available today: DeepSeek R1 7B &#038; 14B distilled models for Copilot+ PCs via Azure AI Foundry – further expanding AI on the edge</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[At Microsoft, we believe the future of AI is happening now — spanning from the cloud to the edge. Our vision is bold: to build Windows as the ultimate platform for AI innovation, where intelligence isn’t just in the cloud but seamlessly woven throughout the system, silicon and hardware at the edge. Building on our <a href="https://blogs.windows.com/windowsdeveloper/2025/01/29/running-distilled-deepseek-r1-models-locally-on-copilot-pcs-powered-by-windows-copilot-runtime/">recent announcement</a> of bringing NPU-optimized versions of DeepSeek-R1 1.5B distilled model directly to Copilot+ PCs, we’re taking the next step forward with the availability of DeepSeek R1 7B &amp; 14B distilled models for Copilot+ PCs via Azure AI Foundry. This milestone reinforces our commitment to delivering cutting-edge AI capabilities that are fast, efficient and built for real-world applications — helping developers, businesses and creators push the boundaries of what’s possible.

https://www.youtube.com/watch?v=GotHKdBQPw4

Availability starts with Copilot+ PCs powered by Qualcomm Snapdragon X, followed by Intel Core Ultra 200V and AMD Ryzen.

The ability to run 7B and 14B parameter reasoning models on <a href="https://support.microsoft.com/en-us/windows/all-about-neural-processing-units-npus-e77a5637-7705-4915-96c8-0c6a975f9db4">Neural Processing Units (NPUs)</a> is a significant milestone in the democratization and accessibility of artificial intelligence. This progression allows researchers, developers and enthusiasts to leverage the substantial power and functionalities of large-scale machine learning models directly from their Copilot+ PCs. These Copilot+ PCs include an NPU capable of over 40 trillion operations per second (TOPS).
<h3><strong>NPUs are purpose-built to run AI models locally on-device with exceptional efficiency </strong></h3>
NPUs like those built into Copilot+ PCs are purpose-built to run AI models with exceptional efficiency, balancing speed and power consumption. They ensure sustained AI compute with minimal impact on battery life, thermal performance and resource usage. This leaves CPUs and GPUs free to perform other tasks, allowing reasoning models to operate longer and deliver superior results — all while keeping your PC running smoothly.

Efficient inferencing has heightened significance due to a new scaling law for language models, which indicates that chain of thought reasoning during inference can improve response quality across various tasks. The longer a model can “think,” the better its quality will be. Instead of increasing parameters or training data, this approach taps into additional computational power for better outcomes. DeepSeek distilled models exemplify how even small pretrained models can shine with enhanced reasoning capabilities and when coupled with the NPUs on <a href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/">Copilot+ PCs</a>, they unlock exciting new opportunities for innovation.

Reasoning emerges in models of a certain minimum scale, and models at that scale must think using a large number of tokens to excel at complex multi-step reasoning. Although the NPU hardware aids in reducing inference costs, it is equally important to maintain a manageable memory footprint for these models on consumer PCs, say with 16GB RAM.
<h3><strong>Pushing the boundaries of what’s possible on Windows</strong></h3>
Our research investments have enabled us to push the boundaries of what’s possible on Windows even further at the system level and at a model level leading to innovations like Phi Silica. <a href="https://blogs.windows.com/windowsexperience/2024/12/06/phi-silica-small-but-mighty-on-device-slm/">With our work on Phi Silica</a> we were able to create a scalable platform for low-bit inference on NPUs, enabling powerful performance with minimal memory and bandwidth tax. Combined with the data privacy offered by local compute, this puts advanced scenarios like Retrieval Augmented Generation (RAG) and model fine-tuning at the fingertips of application developers.

We reused techniques such as <a href="https://arxiv.org/abs/2404.00456">QuaRot</a>, sliding window for fast first token responses and many other optimizations to enable the DeepSeek 1.5B release. We used Aqua, an internal automatic quantization tool, to quantize all the DeepSeek model variants to int4 weights with QuaRot, while retaining most of the accuracy. Using the same toolchain we used to optimize Phi Silica we quickly integrated all the optimizations into an efficient <a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html">ONNX QdQ</a> model with low precision weights.

Like the 1.5B model, the 7B and 14B variants use 4-bit block wise quantization for the embeddings and language model head and run these memory-access heavy operations on the CPU. The compute-heavy transformer block containing the context processing and token iteration uses int4 per-channel quantization for the weights alongside int16 activations. We already see about 8 tok/sec on the 14B model (the 1.5B model, being very small, demonstrated close to 40 tok/sec) — and further optimizations are coming in as we leverage more <a href="https://arxiv.org/abs/2402.17764">advanced techniques</a>. With all this in place, these nimble language models think longer and harder.

This durable path to innovation has made it possible for us to more quickly optimize larger variants of DeepSeek models (7B and 14B) and will continue to enable us to bring more new models to run on Windows efficiently.
<h3><strong>Get started today</strong></h3>
Developers can access all distilled variants (1.5B, 7B and 14B) of DeepSeek models and run them on Copilot+ PCs by simply downloading the <a href="https://learn.microsoft.com/en-us/windows/ai/toolkit/toolkit-getting-started?utm_source=chatgpt.com&amp;tabs=rest">AI Toolkit VS Code extension</a>. The DeepSeek model optimized in the ONNX QDQ format is available in AI Toolkit’s model catalog, pulled directly from Azure AI Foundry. You can download it locally by clicking the “Download” button. Once downloaded, experimenting with the model is as simple as opening the Playground, loading the “deepseek_r1_1_5” model and sending it prompts.
<h3><strong>Run models across Copilot+ PCs and Azure</strong></h3>
Copilot+ PCs offer local compute capabilities that are an extension of capabilities enabled by Azure, giving developers even more flexibility to train, fine-tune small language models on-device and leverage the cloud for larger intensive workloads. In addition to the ONNX model optimized for Copilot+ PC, you can also try the cloud-hosted source model in Azure Foundry by clicking on the “Try in Playground” button under “DeepSeek R1.” AI Toolkit is part of your developer workflow as you experiment with models and get them ready for deployment. With this playground, you can effortlessly test the DeepSeek models available in Azure AI Foundry for local deployment too. Through this, developers now have access to the most complete set of DeepSeek models available through the Azure AI Foundry from cloud to client.

Copilot+ PCs pair efficient compute with the near infinite compute Microsoft has to offer via its Azure services. With reasoning able to span the cloud and the edge, running in sustained loops on the PC and invoking the much larger brains in the cloud as needed — we are on to a new paradigm of continuous compute creating value for our customers. The future of AI compute just got brighter! We can’t wait to see the new innovations from our developer community taking advantage of these rich capabilities. Please keep the feedback coming!]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Discover magical AI experiences through the Microsoft Store on Windows, with the new AI Hub</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/02/18/discover-magical-ai-experiences-through-the-microsoft-store-on-windows-with-the-new-ai-hub/</link>
		
		<dc:creator><![CDATA[Steve Clarke, Editor]]></dc:creator>
		<pubDate>Tue, 18 Feb 2025 17:00:01 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57340</guid>

					<description><![CDATA[<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/02/18/discover-magical-ai-experiences-through-the-microsoft-store-on-windows-with-the-new-ai-hub/">Discover magical AI experiences through the Microsoft Store on Windows, with the new AI Hub</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[The Microsoft Store on Windows is the best place to discover AI experiences for your PC from Microsoft and our rich developer ecosystem. In 2023, we launched the AI Hub in Store to help our users find great AI-powered apps. In the last two years, we’ve seen hundreds of new AI-capable apps join the Microsoft Store community and we’ve partnered with many to showcase their content in the AI Hub. Developers like <a href="https://apps.microsoft.com/detail/XPDDXX9QW8N9D7?hl=en-US&gl=US&ocid=pdpshare">Grammarly</a> have noted that “through this partnership, we saw a meaningful lift in key metrics, including a 245% increase in page views, a 13% growth in installs and a 24% rise in new active users. The AI Hub spotlight helped us connect with more users seeking AI assistance for communication and productivity, reinforcing the value of Grammarly for Windows.”

<strong>Today, we’re excited to announce the next iteration of AI Hub<sup>1</sup>.</strong> We have reimagined it to be the destination for users to learn about the AI experiences possible on their devices. For users on Copilot+ PCs, we’ll showcase the Windows and developer ecosystem experiences that leverage the power of their PC. For users not yet on Copilot+ PCs, we’ll showcase the best apps and experiences compatible with their devices. And we are proud to do this with a brand-new product design – you’ll find beautiful new visuals and snappy flows that help make your browsing experience inspiring.
<h3>AI experiences powered by Microsoft</h3>
https://youtu.be/A63BCh-qs-E

When you’re browsing the new AI Hub on your Copilot+ PC, you’ll find rich informational content on the AI experiences powered by your PC’s <a href="https://support.microsoft.com/en-us/windows/all-about-neural-processing-units-npus-e77a5637-7705-4915-96c8-0c6a975f9db4">Neural Processing Unit</a> (NPU). The NPU plays a key role in handling tasks related to AI and machine learning. It is designed to speed up complex processes such as facial recognition, voice assistance and data analysis, delivering advanced computation with exceptional battery efficiency. The NPU’s ability to offload these tasks from the CPU and GPU allows for faster, more efficient operation of the entire system.

As you become more familiar with the capabilities of your Copilot+ PC, AI Hub is here to help! We’ve created a new Welcome Experience to help you familiarize yourself with exclusive Windows features, ready for you to use – to start, these include <a href="https://support.microsoft.com/en-us/windows/windows-studio-effects-273c1fa8-2b3f-41b1-a587-7cc7a24b62d8">Windows Studio Effects</a>, <a href="https://support.microsoft.com/en-us/windows/use-copilot-pc-features-in-paint-53857513-e36c-472d-8d4a-adbcd14b2e54">Paint Cocreator</a> and <a href="https://support.microsoft.com/en-us/windows/microsoft-photos-restyle-image-and-image-creator-responsible-ai-faq-6c352e99-d954-49c9-84cd-b7cacd018868">Photos Image Creator</a>. And when you’re ready to dive into a specific workflow, we’ll help you get started.

Your AI Hub will stay current to include details on the latest Copilot+ PC features and NPU-powered apps available. We're excited about helping customers connect to upcoming Windows features when they're generally available, including Recall, Click To Do and Improved Windows Search<sup>2</sup>.

https://youtu.be/3ptccBogS18

For users browsing AI Hub on Windows 11 PCs, you’ll find informational content available on the AI-powered apps that work on your device. Microsoft apps like <a href="https://apps.microsoft.com/detail/9nht9rb2f4hd"><strong>Copilot</strong></a>, <a href="https://apps.microsoft.com/detail/9pjgrcldlx5v"><strong>Designer</strong></a>, <a href="https://apps.microsoft.com/detail/9p1j8s7ccwwt?hl"><strong>Clipchamp</strong></a> and <a href="https://apps.microsoft.com/detail/9n0dgwh9pszf"><strong>Reading Coach</strong></a> all have AI functionalities ready for you to dive into.
<h3>Our rich AI developer ecosystem</h3>
<img class="alignnone wp-image-57371 size-large" src="https://pub-d00f534024b04d0e8036586fc78a41fa.r2.dev/sites/3/2025/02/Rich-Apps-1024x576.png" alt="Decorative image with app icons" width="1024" height="576" />

No matter what PC you’re using, we have a rich AI developer ecosystem available inside the Microsoft Store on Windows – and the AI Hub is a wonderful destination for you to browse content. Some examples include:
<ul>
 	<li>For Copilot+ PC users who are passionate about music creation, <a href="https://apps.microsoft.com/detail/9p9rb7zf49xk?ocid=storeaward24"><strong>djay Pro</strong></a> features "NeuralMix," which leverages the extra processing power of the NPU to isolate vocals or instruments when re-mixing songs. On Copilot+ PCs, NeuralMix can use an AI model that is twice as large and more complex than on other devices, resulting in better sound quality and cleaner music stem separation. djay Pro was selected as a <a href="https://blogs.windows.com/windowsdeveloper/2024/11/01/announcing-the-microsoft-store-awards-2024-winners/">2024 Microsoft Store Awards winner</a> in the category of AI apps!</li>
</ul>
<ul>
 	<li>For users on Copilot+ PCs who require a stand-out video presence, from sales professionals to streamers, <a href="https://apps.microsoft.com/detail/9pgm3qb3pdrd?launch=true&mode=full&hl"><strong>Camo Studio</strong></a> uses the NPU to power real-time visual effects that ensure you look your best. The NPU powers several machine learning workloads (such as real-time background segmentation and relighting) where latency is critical – and with incredible efficiency that ensures minimal impact to battery life.</li>
</ul>
<ul>
 	<li><a href="https://apps.microsoft.com/detail/9mv4lscdrflt?ocid=storeaward24"><strong>Gamma</strong></a><strong> </strong>stands out for its ability to offer users AI-powered presentation creation, a user-friendly interface and versatile formats. Its intuitive design, real-time collaboration features and interactive elements make it a favorite among businesses. And Gamma was also selected as a <a href="https://blogs.windows.com/windowsdeveloper/2024/11/01/announcing-the-microsoft-store-awards-2024-winners/">2024 Microsoft Store Awards winner</a> in the category of Business apps!</li>
</ul>
<ul>
 	<li>And for those who love to converse with assistants, <a href="https://apps.microsoft.com/detail/9nt1r1c2hh7j?hl"><strong>ChatGPT</strong></a> now is available as an app for all Windows devices exclusively through the Microsoft Store. Easily ask questions, get writing assistance and generate ideas – all in a seamless, user-friendly experience.</li>
</ul>
Visit AI Hub regularly to see the latest AI-enabled products from our amazing partners.
<h3>A preview of what’s ahead</h3>
Today, this experience is available for Copilot+ PCs in United States, Canada, United Kingdom and Australia. In the coming months, we will expand it to additional markets and for the broader Windows population.

To stay current on release plans, please watch the “What’s New” page in Store to see our latest release information. And, <a href="https://blogs.windows.com/windowsdeveloper/2024/12/03/raising-the-bar-updates-to-the-microsoft-store-on-windows/">as we shared in December</a>, we are keen on your feedback – please submit via Feedback Hub (WIN + F) under Microsoft Store.

AI is changing the way we do everything on our PCs – from tackling complex research projects, to finding new recipes, editing photos and everything in between. We believe AI innovation isn’t just about creating new products; it’s about enabling us to reimagine how we solve everyday problems.

Happy browsing!

<em><sup>1 </sup>AI Hub is available in select markets within Microsoft Store on Windows. </em>

<em><sup>2 </sup>Windows features will be added to the AI Hub as they become available to retail users by market. Features in the Windows Insider Program will not be included. </em>]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Running Distilled DeepSeek R1 models locally on Copilot+ PCs, powered by Windows Copilot Runtime</title>
		<link>https://blogs.windows.com/windowsdeveloper/2025/01/29/running-distilled-deepseek-r1-models-locally-on-copilot-pcs-powered-by-windows-copilot-runtime/</link>
		
		<dc:creator><![CDATA[Vivek Pradeep]]></dc:creator>
		<pubDate>Wed, 29 Jan 2025 22:11:14 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://blogs.windows.com/windowsdeveloper/?p=57331</guid>

					<description><![CDATA[<p><em><strong>Update: Feb. 3, 2025:</strong> Today, we are pleased to announce that the distilled DeepSeek R1 models optimized using ONNX are now available to use on your Snapdragon powered Copilot+ PCs. With further optimizations in place, the model i</p>
<p>The post <a href="https://blogs.windows.com/windowsdeveloper/2025/01/29/running-distilled-deepseek-r1-models-locally-on-copilot-pcs-powered-by-windows-copilot-runtime/">Running Distilled DeepSeek R1 models locally on Copilot+ PCs, powered by Windows Copilot Runtime</a> appeared first on <a href="https://blogs.windows.com/windowsdeveloper">Windows Developer Blog</a>.</p>
]]></description>
										<content:encoded><![CDATA[<em><strong>Update: Feb. 3, 2025:</strong> Today, we are pleased to announce that the distilled DeepSeek R1 models optimized using ONNX are now available to use on your Snapdragon powered Copilot+ PCs. With further optimizations in place, the model is capable of a time to first token of less than 70 ms for short prompts (&lt;64 tokens) and a throughput rate of up to ~40 tokens/s. The time to first token scales with the length of the input prompt. The throughput rate varies based on the complexity of the task specified in the prompt; responses exhibit a throughput range of ~25-40 tokens/s. Longer responses are especially likely to enjoy higher throughput rates. <a href="https://learn.microsoft.com/en-us/windows/ai/toolkit/toolkit-getting-started?utm_source=chatgpt.com&amp;tabs=rest">Get started today by downloading the AI Toolkit extension in VS Code</a>.</em>

AI is moving closer to the edge, and Copilot+ PCs are leading the way. With the <a href="https://azure.microsoft.com/en-us/blog/?p=38333">availability of cloud hosted DeepSeek R1 available on Azure AI Foundry</a>, we’re bringing NPU-optimized versions of DeepSeek-R1 directly to Copilot+ PCs, starting with Qualcomm Snapdragon X first, followed by Intel Core Ultra 200V and others. The first release, DeepSeek-R1-Distill-Qwen-1.5B (<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B">Source</a>), will be available in AIToolkit for VSCode, with the 7B (<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B">Source</a>) and 14B (<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B">Source</a>) variants arriving soon. These optimized models let developers build and deploy AI-powered applications that run efficiently on-device, taking full advantage of the powerful NPUs in Copilot+ PCs.

The <a href="https://support.microsoft.com/en-us/windows/all-about-neural-processing-units-npus-e77a5637-7705-4915-96c8-0c6a975f9db4">Neural Processing Unit (NPU)</a> on Copilot+ PCs offers a highly efficient engine for model inferencing, unlocking a paradigm where generative AI can execute not just when invoked, but enable semi-continuously running services. This empowers developers to tap into powerful reasoning engines to build proactive and sustained experiences.

<a href="https://blogs.windows.com/windowsexperience/2024/12/06/phi-silica-small-but-mighty-on-device-slm/">With our work on Phi Silica</a>, we were able to harness highly efficient inferencing – delivering very competitive time to first token and throughput rates, while minimally impacting battery life and consumption of PC resources. Running models on the NPU is about speed and efficiency. For example, as mentioned in previous posts, the Phi Silica token iterator on the NPU exhibits a 56% improvement in power consumption compared to operating on the CPU. Such efficiency enables new experiences that demand such state-of-the art models to be in the main loop of the program without draining your battery or overly heating your device. The optimized DeepSeek models for the NPU take advantage of several of the key learnings and techniques from that effort, including how we separate out the various parts of the model to drive the best tradeoffs between performance and efficiency, low bit rate quantization and mapping transformers to the NPU. Additionally, we take advantage of Windows Copilot Runtime (WCR) to scale across the diverse Windows ecosystem with <a href="https://onnxruntime.ai/">ONNX</a> QDQ format.

<strong>Get ready to play!</strong>

First things first…let’s give it a whirl.

To see DeepSeek in action on your Copilot+ PC, simply download the AI Toolkit VS Code extension. The DeepSeek model optimized in the ONNX QDQ format is available in AI Toolkit’s model catalog, pulled directly from Azure AI Foundry. You can download it locally by clicking the “Download” button. Once downloaded, experimenting with the model is as simple as opening the Playground, loading the “ deepseek_r1_1_5” model, and sending it prompts.

In addition to the ONNX model optimized for Copilot+ PC, you can also try the cloud-hosted source model in Azure Foundry by clicking on the “Try in Playground” button under “ DeepSeek R1”.

AI Toolkit is part of your developer workflow as you experiment with models and get them ready for deployment. With this playground, you can effortlessly test the DeepSeek models available in Azure AI Foundry for local deployment.

https://youtu.be/CFzH0sekxYI

<strong>Silicon Optimizations </strong>

The distilled Qwen 1.5B consists of a tokenizer, embedding layer, a context processing model, token iteration model, a language model head and a de-tokenizer. We use 4-bit block wise quantization for the embeddings and language model head and run these memory-access heavy operations on the CPU. We focus the bulk of our NPU optimization efforts on the compute-heavy transformer block containing the context processing and token iteration, wherein we employ int4 per-channel quantization for the weights alongside int16 activations.

Details of the various precisions involved are in the table below, for additional clarity on the mix.
<table style="margin-left: auto !important; margin-right: auto !important; display: block;">
<tbody>
<tr>
<td width="207"><strong>Model</strong></td>
<td width="207"><strong>Precision</strong></td>
<td width="207"><strong>Host</strong></td>
</tr>
<tr>
<td width="207">Embeddings</td>
<td width="207">w: int4 a: fp32</td>
<td width="207">CPU</td>
</tr>
<tr>
<td width="207">Context processing</td>
<td width="207">w: int4 a: int16</td>
<td width="207">NPU</td>
</tr>
<tr>
<td width="207">Token iteration</td>
<td width="207">w: int4 a: int16</td>
<td width="207">NPU</td>
</tr>
<tr>
<td width="207">Language model head</td>
<td width="207">w: int4 a: fp32</td>
<td width="207">CPU</td>
</tr>
</tbody>
</table>
While the Qwen 1.5B release from DeepSeek does have an int4 variant, it does not directly map to the NPU due to presence of dynamic input shapes and behavior – all of which needed optimizations to make compatible and extract the best efficiency. Additionally, we use the <a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html">ONNX QDQ</a> format to enable scaling across a variety of NPUs we have in the Windows ecosystem. We work out an optimal operator layout between the CPU and NPU for maximum power-efficiency and speed.

To achieve the dual goals of low memory footprint and fast inference, much like Phi Silica, we make two key changes: First, we leverage a sliding window design that unlocks super-fast time to first token and long context support despite not having dynamic tensor support in the hardware stack. Second, we use the 4-bit <a href="https://www.microsoft.com/en-us/research/publication/quarot-outlier-free-4-bit-inference-in-rotated-llms/">QuaRot</a> quantization scheme to truly take advantage of low bit processing. QuaRot employs Hadamard rotations to remove outliers in weights and activations, making the model easier to quantize. QuaRot significantly improves quantization accuracy, compared to existing methods, such as GPTQ, particularly for low granularity settings such as per-channel quantization. The combination of low-bit quantization and hardware optimizations such the sliding window design help deliver the behavior of a larger model within the memory footprint of a compact model. With these optimizations in place, the model is capable of a time to first token of 130 ms and a throughput rate of 16 tokens/s for short prompts (&lt;64 tokens).

We include examples of the original and quantized model responses below to show the minor differences between the two variants, with the latter being both fast and power-efficient:

<img class="alignnone wp-image-57336 size-large" src="https://pub-d00f534024b04d0e8036586fc78a41fa.r2.dev/sites/3/2025/01/originalleft_deepseekr1_1.5-1024x592.png" alt="Sample response from the original model" width="1024" height="592" />

<img class="alignnone wp-image-57338 size-large" src="https://pub-d00f534024b04d0e8036586fc78a41fa.r2.dev/sites/3/2025/01/quantizedright_deepseekr1_1.5-2-1024x587.png" alt="Sample response from the NPU-optimized model" width="1024" height="587" />

<em>Figure 1: Qualitative comparison. Sample responses from the original model (top) vs NPU-optimized model (bottom) for the same prompt, including the model’s reasoning capability. The model follows a similar reasoning pattern, and reaches the same answer, demonstrating that the optimized model retains the reasoning ability of the original model. </em>

With the speed and power characteristics of the NPU-optimized version of the DeepSeek R1 models users will be able to interact with these ground-breaking models entirely locally. We are excited what this capability enables for the future of the PC experience and looking forward towards innovations from our developer community.]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
